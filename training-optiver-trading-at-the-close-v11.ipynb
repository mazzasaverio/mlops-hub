{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"papermill":{"default_parameters":{},"duration":512.699896,"end_time":"2023-11-14T15:04:41.475793","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2023-11-14T14:56:08.775897","version":"2.4.0"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":57891,"databundleVersionId":7056235,"sourceType":"competition"},{"sourceId":6993423,"sourceType":"datasetVersion","datasetId":4019630},{"sourceId":150406473,"sourceType":"kernelVersion"},{"sourceId":150406565,"sourceType":"kernelVersion"},{"sourceId":150406696,"sourceType":"kernelVersion"},{"sourceId":150407056,"sourceType":"kernelVersion"},{"sourceId":150412552,"sourceType":"kernelVersion"},{"sourceId":150429530,"sourceType":"kernelVersion"},{"sourceId":151165317,"sourceType":"kernelVersion"},{"sourceId":151165495,"sourceType":"kernelVersion"}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Constants and Configuration Variables\nDEBUG = False\nTRAIN = True\nTUNING = True\nOVERWRITE_PROCESSED_DATA = True\nN_TRIALS = 10\nSTATE = 42\nN_FOLD_BREAK = 7\n\nGPU_SWITCH = \"OFF\"\nN_SPLITS = 5\nN_TEST_SPLITS = 1\nN_PURGE = 20\nN_EMBARGO = 20\n\nSAVE_MODELS = False\n\nVERSION_NB = 21\nEXPERIMENT_PURPOSE = \"optiver_trading_at_the_close\"\n\n\n#learning_rate: 0.01299 | max_depth: 13 | boosting_type: gbdt | colsample_bytree: 0.5 | \n#                importance_type: gain | min_child_samples: 20 | min_child_weight: 0.001 |\n#                            min_split_gain: 0.0 | n_estimators: 1000 | num_leaves: 205 | \n#                                        objective: mae | reg_alpha: 0.0 | reg_lambda: 0.0 |\n#                                                    subsample: 0.45 | subsample_for_bin: 200000 | \n#                                                            subsample_freq: 0 | max_bin: 254\n#                                                                    \n                                                                    \n                                                                    \nmodel_params_dict = {\n    \"LGBMR\": {\n        \"static_params\": {\n            \"device\": \"gpu\" if GPU_SWITCH == \"ON\" else \"cpu\",\n            \"objective\": \"mae\",\n            \"boosting_type\": \"gbdt\",\n            \"random_state\": STATE,\n            \"n_jobs\": 4,\n            \"verbose\": -1,\n            \"importance_type\": \"gain\",\n            \"max_bin\": 254\n        },\n        \"dynamic_params\": {\n            \"n_estimators\": {\n                \"type\": \"int\",\n                \"low\": 1000,\n                \"high\": 1000,\n            },\n            \"learning_rate\": {\n                \"type\": \"float\",\n                \"low\": 0.011,\n                \"high\": 0.01311,\n            },\n            \"max_depth\": {\"type\": \"int\", \"low\": 13, \"high\": 13},\n            \"num_leaves\": {\n                \"type\": \"int\",\n                \"low\": 200,\n                \"high\": 230,\n            },\n            \"min_child_samples\": {\n                \"type\": \"int\",\n                \"low\": 15,\n                \"high\": 30,\n            },\n            \"subsample\": {\n                \"type\": \"float\",\n                \"low\": 0.45,\n                \"high\": 0.45,\n            },\n            \"colsample_bytree\": {\n                \"type\": \"float\",\n                \"low\": 0.5,\n                \"high\": 0.5,\n            },\n        },\n    },\n}\n\n# RECORD ENSAMBLE MODEL\nmodel_paths = [\n    \"s3://mlflow-v1/kaggle_optiver_trading_at_the_close/15d253a7751b414fbc8ec5a78c81e24c/artifacts/LGBMR_0_20231118_044309/model.pkl\",\n    \"s3://mlflow-v1/kaggle_optiver_trading_at_the_close/8a881129db7c45e9bd92094cd1097297/artifacts/LGBMR_0_20231118_044743/model.pkl\",\n    \"s3://mlflow-v1/kaggle_optiver_trading_at_the_close/9e402e7a59be460897bf7cee58fc85c5/artifacts/LGBMR_0_20231118_045214/model.pkl\",\n    \"s3://mlflow-v1/kaggle_optiver_trading_at_the_close/1ffb209c2fe2420db01bb4df17d86f1d/artifacts/LGBMR_0_20231118_045642/model.pkl\",\n    \"s3://mlflow-v1/kaggle_optiver_trading_at_the_close/562d3c3b41c74c9a90e080b88eb5af3a/artifacts/LGBMR_0_20231118_050108/model.pkl\",\n]\n\n# Define the model name for registration in MLflow\n\nversion = 9\nmodel_name = f\"ensemble_model ({version})\"\nfolder_model = f\"models-v{version}\"","metadata":{"papermill":{"duration":6.524524,"end_time":"2023-11-14T14:56:18.932693","exception":false,"start_time":"2023-11-14T14:56:12.408169","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-11-19T23:34:06.176687Z","iopub.execute_input":"2023-11-19T23:34:06.177300Z","iopub.status.idle":"2023-11-19T23:34:06.214928Z","shell.execute_reply.started":"2023-11-19T23:34:06.177255Z","shell.execute_reply":"2023-11-19T23:34:06.214067Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# External general-purpose modules\nimport gc\nimport sys\nimport os\nimport itertools as itt\nfrom itertools import combinations, product\nfrom datetime import datetime\nimport numpy as np\nimport pandas as pd\nimport polars as pl\nimport joblib\nimport time\n\nfrom pathlib import Path\nimport warnings\nfrom numba import njit, prange\nimport boto3\nfrom botocore.exceptions import NoCredentialsError\nfrom dotenv import load_dotenv\n\n\n# Setting pandas options and warning filters\npd.set_option(\"display.max_columns\", None)\nwarnings.filterwarnings(\"ignore\")\nwarnings.simplefilter(action=\"ignore\", category=pd.errors.PerformanceWarning)\n\n# Load environment variables\nload_dotenv()\npath_root_project = Path.cwd()\nif path_root_project.name not in [\"working\", \"content\"]:\n    path_root_project = Path(os.getenv(\"ROOT_PATH\") or path_root_project)\n\n    directories_to_add = [\"utils\", \"feat_engineering\", \"validation\"]\n    for directory in directories_to_add:\n        sys.path.append(str(path_root_project / \"src\" / directory))\n\n\nfrom utils_training import create_model, experiments_data\nfrom utils_data import load_config, load_dataset, reduce_mem_usage, PathManager\nfrom utils_kaggle import (\n    setup_kaggle,\n    download_data,\n    get_data,\n    clean_directory_except_one,\n)\n\nfrom fe_optiver_trading_at_the_close import (\n    calculate_triplet_imbalance_numba,\n    convert_weights_to_dict,\n    global_stock_id_feats,\n    compute_rolling_averages,\n    generate_rsi,\n)\n\npm = PathManager(path_root_project)\n\nif TRAIN:\n    if pm.path_root_project.name == \"working\":\n        from kaggle_secrets import UserSecretsClient\n\n        user_secrets = UserSecretsClient()\n        aws_access_key_id = user_secrets.get_secret(\"AWS_ACCESS_KEY_ID\")\n        aws_region = user_secrets.get_secret(\"AWS_DEFAULT_REGION\")\n        aws_secret_access_key = user_secrets.get_secret(\"AWS_SECRET_ACCESS_KEY\")\n        s3_bucket_name = user_secrets.get_secret(\"S3_BUCKET\")\n\n        # Set AWS credentials in the environment variables\n        os.environ[\"AWS_ACCESS_KEY_ID\"] = aws_access_key_id\n        os.environ[\"AWS_SECRET_ACCESS_KEY\"] = aws_secret_access_key\n        os.environ[\"AWS_DEFAULT_REGION\"] = aws_region\n    else:\n        aws_access_key_id = os.getenv(\"AWS_ACCESS_KEY_ID\")\n        aws_secret_access_key = os.getenv(\"AWS_SECRET_ACCESS_KEY\")","metadata":{"papermill":{"duration":2.108409,"end_time":"2023-11-14T14:56:21.135276","exception":false,"start_time":"2023-11-14T14:56:19.026867","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-11-19T23:34:06.328621Z","iopub.execute_input":"2023-11-19T23:34:06.329199Z","iopub.status.idle":"2023-11-19T23:34:11.213592Z","shell.execute_reply.started":"2023-11-19T23:34:06.329158Z","shell.execute_reply":"2023-11-19T23:34:11.212473Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"model_prod = joblib.load(f'/kaggle/input/{folder_model}/{model_name}.pkl')","metadata":{"execution":{"iopub.status.busy":"2023-11-19T23:34:11.215399Z","iopub.execute_input":"2023-11-19T23:34:11.215883Z","iopub.status.idle":"2023-11-19T23:34:14.993253Z","shell.execute_reply.started":"2023-11-19T23:34:11.215849Z","shell.execute_reply":"2023-11-19T23:34:14.992157Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#df_feat = pd.read_csv(\"/kaggle/working/feat_impor_optiver_trading_at_the_close_23_11_18_0438.csv\").sort_values(\"t0_imp_fold_1\")","metadata":{"papermill":{"duration":0.022293,"end_time":"2023-11-14T14:56:21.250975","exception":false,"start_time":"2023-11-14T14:56:21.228682","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-11-19T23:34:14.994520Z","iopub.execute_input":"2023-11-19T23:34:14.994878Z","iopub.status.idle":"2023-11-19T23:34:14.999855Z","shell.execute_reply.started":"2023-11-19T23:34:14.994843Z","shell.execute_reply":"2023-11-19T23:34:14.998648Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"def aggregate_feature_importance(df_feat_importance):\n    df_feat_importance[\"feat_imp_mean\"] = df_feat_importance.loc[\n        :, df_feat_importance.columns != \"feat\"\n    ].mean(axis=1, skipna=True)\n\n\n\n    df_feat_importance.sort_values('feat_imp_mean', inplace = True, ascending = False)\n    df_feat_importance.reset_index(drop =  True, inplace = True)\n\n\n\n    cols  = [col for col in  df_feat_importance.columns if col not in  [\"feat\",\"feat_imp_mean\"]]\n\n\n    df_feat_importance[\"feat_imp_std\"] = df_feat_importance.loc[\n            :,cols\n        ].std(axis=1, skipna=True)\n\n    df_feat_importance['feat_imp_variability'] =  df_feat_importance['feat_imp_std'] /df_feat_importance['feat_imp_mean'] \n\n    return df_feat_importance","metadata":{"execution":{"iopub.status.busy":"2023-11-19T23:34:15.002206Z","iopub.execute_input":"2023-11-19T23:34:15.002590Z","iopub.status.idle":"2023-11-19T23:34:15.012718Z","shell.execute_reply.started":"2023-11-19T23:34:15.002555Z","shell.execute_reply":"2023-11-19T23:34:15.011664Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n\n# Conditional imports and settings based on TRAIN constant\nif TRAIN:\n    if pm.path_root_project.name == \"working\":\n        !pip install loguru mlflow optuna > /dev/null\n\n    \n\n    from utils_mlflow import (\n        get_experiments_df,\n        delete_runs_and_artifacts,\n        download_and_load_model,\n        load_models_and_create_ensemble,\n        save_and_register_model,\n        log_model_parameters,\n        get_or_create_experiment,\n        experiments_data,\n    )\n    from utils_feat_importance import log_feature_importance, aggregate_feature_importance\n    from model_validation import time_series_split\n\n    # External Libraries\n    import boto3\n    from botocore.exceptions import NoCredentialsError\n    from mlflow.exceptions import MlflowException\n    import lightgbm as lgbm\n    import mlflow\n    import optuna\n    from mlflow.tracking import MlflowClient\n    from optuna.integration.mlflow import MLflowCallback\n    from sklearn.model_selection import KFold\n    from xgboost import XGBRegressor as XGBR\n    from lightgbm import log_evaluation, early_stopping, LGBMRegressor as LGBMR\n\n\n\n    # Auto-reload modules - Specific to Jupyter Notebooks\n    %load_ext autoreload\n    %autoreload 2\n    if not DEBUG:\n        mlflow.set_tracking_uri(pm.path_experiments_dir)\n        \n    client = MlflowClient()\n    \n    # Create an S3 client\n    s3 = boto3.client(\n        \"s3\",\n        aws_access_key_id=aws_access_key_id,\n        aws_secret_access_key=aws_secret_access_key,\n    )","metadata":{"papermill":{"duration":0.035618,"end_time":"2023-11-14T14:56:21.300299","exception":false,"start_time":"2023-11-14T14:56:21.264681","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-11-19T23:34:15.014329Z","iopub.execute_input":"2023-11-19T23:34:15.015180Z","iopub.status.idle":"2023-11-19T23:34:35.364376Z","shell.execute_reply.started":"2023-11-19T23:34:15.015128Z","shell.execute_reply":"2023-11-19T23:34:35.363212Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"if SAVE_MODELS:\n     s3 = boto3.client(\n         \"s3\",\n         aws_access_key_id=aws_access_key_id,\n         aws_secret_access_key=aws_secret_access_key,\n     )\n     # Load the models and create an ensemble\n     ensemble_model = load_models_and_create_ensemble(s3, model_paths)\n\n     # Save and register the ensemble model in MLflow\n     save_and_register_model(ensemble_model, model_name)","metadata":{"papermill":{"duration":0.021792,"end_time":"2023-11-14T14:56:21.335759","exception":false,"start_time":"2023-11-14T14:56:21.313967","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-11-19T23:34:35.365834Z","iopub.execute_input":"2023-11-19T23:34:35.366446Z","iopub.status.idle":"2023-11-19T23:34:35.409779Z","shell.execute_reply.started":"2023-11-19T23:34:35.366410Z","shell.execute_reply":"2023-11-19T23:34:35.406878Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"papermill":{"duration":0.013331,"end_time":"2023-11-14T14:56:21.362906","exception":false,"start_time":"2023-11-14T14:56:21.349575","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if TRAIN:\n    if not os.path.exists(pm.path_dataset_processed) or OVERWRITE_PROCESSED_DATA:\n        df_train_raw = pd.read_csv(pm.path_data_train_raw)\n\n        if DEBUG:\n            df_train_raw = df_train_raw[df_train_raw[\"stock_id\"].isin([0, 1, 2, 3, 4])]\n\n        drop_idx = df_train_raw.loc[\n            df_train_raw[\"target\"].isna(), \"target\"\n        ].index.to_list()\n        df_train = df_train_raw.drop(drop_idx, axis=0)\n        df_train.reset_index(drop=True, inplace=True)\n    else:\n        df_train = pd.read_csv(pm.path_dataset_processed)\n        if DEBUG:\n            df_train = df_train[df_train[\"stock_id\"].isin([0, 1, 2, 3, 4])]\n\n    df_train.sort_values([\"time_id\", \"stock_id\"], inplace=True)","metadata":{"papermill":{"duration":0.02474,"end_time":"2023-11-14T14:56:21.401516","exception":false,"start_time":"2023-11-14T14:56:21.376776","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-11-19T23:34:35.411088Z","iopub.execute_input":"2023-11-19T23:34:35.411528Z","iopub.status.idle":"2023-11-19T23:34:56.624590Z","shell.execute_reply.started":"2023-11-19T23:34:35.411492Z","shell.execute_reply":"2023-11-19T23:34:56.623490Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"if TRAIN:\n    dict_global_stock_id_feats = global_stock_id_feats(df_train)","metadata":{"papermill":{"duration":0.02096,"end_time":"2023-11-14T14:56:21.435912","exception":false,"start_time":"2023-11-14T14:56:21.414952","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-11-19T23:34:56.625911Z","iopub.execute_input":"2023-11-19T23:34:56.626255Z","iopub.status.idle":"2023-11-19T23:34:58.644773Z","shell.execute_reply.started":"2023-11-19T23:34:56.626211Z","shell.execute_reply":"2023-11-19T23:34:58.643550Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"@njit(parallel=True)\ndef calculate_macd(data, short_window=12, long_window=26, signal_window=9):\n    rows, cols = data.shape\n    macd_values = np.empty((rows, cols))\n    signal_line_values = np.empty((rows, cols))\n    histogram_values = np.empty((rows, cols))\n\n    for i in prange(cols):\n        short_ema = np.zeros(rows)\n        long_ema = np.zeros(rows)\n\n        for j in range(1, rows):\n            short_ema[j] = (data[j, i] - short_ema[j - 1]) * (2 / (short_window + 1)) + short_ema[j - 1]\n            long_ema[j] = (data[j, i] - long_ema[j - 1]) * (2 / (long_window + 1)) + long_ema[j - 1]\n\n        macd_values[:, i] = short_ema - long_ema\n\n        signal_line = np.zeros(rows)\n        for j in range(1, rows):\n            signal_line[j] = (macd_values[j, i] - signal_line[j - 1]) * (2 / (signal_window + 1)) + signal_line[j - 1]\n\n        signal_line_values[:, i] = signal_line\n        histogram_values[:, i] = macd_values[:, i] - signal_line\n\n    return macd_values, signal_line_values, histogram_values\n\n\ndef generate_macd(df):\n    # Define lists of price and size-related column names\n    prices = [\"reference_price\", \"far_price\", \"near_price\", \"ask_price\", \"bid_price\", \"wap\"]\n    sizes = [\"matched_size\", \"bid_size\", \"ask_size\", \"imbalance_size\"]\n    \n    for stock_id, values in df.groupby(['stock_id'])[prices]:\n        macd_values, signal_line_values, histogram_values = calculate_macd(values.values)\n        col_macd = [f'macd_{col}' for col in values.columns]\n        col_signal = [f'macd_sig_{col}' for col in values.columns]\n        col_hist = [f'macd_hist_{col}' for col in values.columns]\n        \n        df.loc[values.index, col_macd] = macd_values\n        df.loc[values.index, col_signal] = signal_line_values\n        df.loc[values.index, col_hist] = histogram_values\n    \n    return df\n\ndef generate_rsi(df):\n    # Define lists of price and size-related column names\n    prices = [\"reference_price\", \"far_price\", \"near_price\", \"ask_price\", \"bid_price\", \"wap\"]\n    sizes = [\"matched_size\", \"bid_size\", \"ask_size\", \"imbalance_size\"]\n    \n    for stock_id, values in df.groupby(['stock_id'])[prices]:\n        columns = [f'rsi_{col}' for col in values.columns]\n        data = calculate_rsi(values.values)\n        df.loc[values.index, columns] = data\n    \n    return df\n\n\n\n@njit(parallel = True)\ndef calculate_rsi(prices, period=14):\n    rsi_values = np.zeros_like(prices)\n\n    for col in prange(prices.shape[1]):\n        price_data = prices[:, col]\n        delta = np.zeros_like(price_data)\n        delta[1:] = price_data[1:] - price_data[:-1]\n        gain = np.where(delta > 0, delta, 0)\n        loss = np.where(delta < 0, -delta, 0)\n\n        avg_gain = np.mean(gain[:period])\n        avg_loss = np.mean(loss[:period])\n        \n        if avg_loss != 0:\n            rs = avg_gain / avg_loss\n        else:\n            rs = 1e-9  # or any other appropriate default value\n            \n        rsi_values[:period, col] = 100 - (100 / (1 + rs))\n\n        for i in prange(period-1, len(price_data)-1):\n            avg_gain = (avg_gain * (period - 1) + gain[i]) / period\n            avg_loss = (avg_loss * (period - 1) + loss[i]) / period\n            if avg_loss != 0:\n                rs = avg_gain / avg_loss\n            else:\n                rs = 1e-9  # or any other appropriate default value\n            rsi_values[i+1, col] = 100 - (100 / (1 + rs))\n\n    return rsi_values\n\n","metadata":{"execution":{"iopub.status.busy":"2023-11-19T23:34:58.646081Z","iopub.execute_input":"2023-11-19T23:34:58.646435Z","iopub.status.idle":"2023-11-19T23:34:58.701012Z","shell.execute_reply.started":"2023-11-19T23:34:58.646396Z","shell.execute_reply":"2023-11-19T23:34:58.699831Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"def order_flow_imbalance(df):\n    df['ofi'] = (df['ask_price'] - df['bid_price']) * (df['ask_size'] - df['bid_size'])\n    df['ofi_normalized'] = df['ofi'] / df['volume']\n    return df\n\ndef trend_momentum_features(df):\n    # Exponential Moving Average\n    for col in ['wap', 'ask_price', 'bid_price']:\n        for window in [5, 10, 15]:\n            df[f'{col}_ema_{window}'] = df.groupby('stock_id')[col].transform(lambda x: x.ewm(span=window).mean())\n\n    # Moving Average Convergence Divergence (MACD)\n    for col in ['wap', 'ask_price', 'bid_price']:\n        fast_window, slow_window, signal = 12, 26, 9\n        df[f'{col}_macd'] = df.groupby('stock_id')[col].transform(lambda x: x.ewm(span=fast_window).mean() - x.ewm(span=slow_window).mean())\n        df[f'{col}_macd_signal'] = df.groupby('stock_id')[f'{col}_macd'].transform(lambda x: x.ewm(span=signal).mean())\n\n    return df\n\n\ndef relative_strength_index(df, periods=14):\n    delta = df['wap'].diff(1)\n    gain = (delta.where(delta > 0, 0)).groupby(df['stock_id']).transform(lambda x: x.rolling(periods).mean())\n    loss = (-delta.where(delta < 0, 0)).groupby(df['stock_id']).transform(lambda x: x.rolling(periods).mean())\n    rs = gain / loss\n    df['rsi'] = 100 - (100 / (1 + rs))\n    return df\n\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression\n\ndef calculate_trend(df):\n    # Calcola la tendenza del prezzo WAP\n    def trendline(data, order=1):\n        coeffs = np.polyfit(range(len(data)), list(data), order)\n        slope = coeffs[-2]\n        return float(slope)\n\n    df['trend'] = df.groupby(['stock_id', 'time_id'])['wap'].transform(trendline)\n    return df\n\n","metadata":{"execution":{"iopub.status.busy":"2023-11-19T23:34:58.704484Z","iopub.execute_input":"2023-11-19T23:34:58.704980Z","iopub.status.idle":"2023-11-19T23:34:58.869770Z","shell.execute_reply.started":"2023-11-19T23:34:58.704941Z","shell.execute_reply":"2023-11-19T23:34:58.868514Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\n\ndef calculate_market_pressure(df):\n    df['bid_ask_size_diff'] = df['bid_size'] - df['ask_size']\n    df['bid_ask_price_diff'] = df['bid_price'] - df['ask_price']\n    df['market_pressure'] = df['bid_ask_size_diff'] * df['bid_ask_price_diff']\n    return df\n\ndef calculate_imbalance_ratio(df):\n    df['imbalance_ratio'] = df['imbalance_size'] / df['matched_size']\n    return df\n\ndef calculate_auction_price_indicators(df):\n    df['far_price_deviation'] = df['far_price'] - df['wap']\n    df['near_price_deviation'] = df['near_price'] - df['wap']\n    return df\n\ndef calculate_auction_time_series_features(df):\n    for col in ['imbalance_size', 'matched_size', 'far_price', 'near_price']:\n        df[f'{col}_time_change'] = df.groupby(['stock_id', 'time_id'])[col].diff()\n    return df\n\n","metadata":{"execution":{"iopub.status.busy":"2023-11-19T23:34:58.870832Z","iopub.execute_input":"2023-11-19T23:34:58.871926Z","iopub.status.idle":"2023-11-19T23:34:58.921568Z","shell.execute_reply.started":"2023-11-19T23:34:58.871885Z","shell.execute_reply":"2023-11-19T23:34:58.920275Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"def imbalance_features(df):\n    # Define lists of price and size-related column names\n    prices = [\"reference_price\", \"far_price\", \"near_price\", \"ask_price\", \"bid_price\", \"wap\"]\n    sizes = [\"matched_size\", \"bid_size\", \"ask_size\", \"imbalance_size\"]\n    df[\"volume\"] = df.eval(\"ask_size + bid_size\")\n    df[\"mid_price\"] = df.eval(\"(ask_price + bid_price) / 2\")\n    df[\"liquidity_imbalance\"] = df.eval(\"(bid_size-ask_size)/(bid_size+ask_size)\")\n    df[\"matched_imbalance\"] = df.eval(\"(imbalance_size-matched_size)/(matched_size+imbalance_size)\")\n    df[\"size_imbalance\"] = df.eval(\"bid_size / ask_size\")\n\n    for c in combinations(prices, 2):\n        df[f\"{c[0]}_{c[1]}_imb\"] = df.eval(f\"({c[0]} - {c[1]})/({c[0]} + {c[1]})\")\n\n    for c in [['ask_price', 'bid_price', 'wap', 'reference_price'], sizes]:\n        triplet_feature = calculate_triplet_imbalance_numba(c, df)\n        df[triplet_feature.columns] = triplet_feature.values\n   \n    df[\"imbalance_momentum\"] = df.groupby(['stock_id'])['imbalance_size'].diff(periods=1) / df['matched_size']\n    df[\"price_spread\"] = df[\"ask_price\"] - df[\"bid_price\"]\n    df[\"spread_intensity\"] = df.groupby(['stock_id'])['price_spread'].diff()\n    df['price_pressure'] = df['imbalance_size'] * (df['ask_price'] - df['bid_price'])\n    df['market_urgency'] = df['price_spread'] * df['liquidity_imbalance']\n    df['depth_pressure'] = (df['ask_size'] - df['bid_size']) * (df['far_price'] - df['near_price'])\n    \n    \n\n        \n    # Calculate various statistical aggregation features\n    for func in [\"mean\", \"std\", \"skew\", \"kurt\"]:\n        df[f\"all_prices_{func}\"] = df[prices].agg(func, axis=1)\n        df[f\"all_sizes_{func}\"] = df[sizes].agg(func, axis=1)\n        \n\n    for col in ['matched_size', 'imbalance_size', 'reference_price', \"wap\"]:\n        for window in [1, 2, 3,6, 10,15]:\n            df[f\"{col}_shift_{window}\"] = df.groupby('stock_id')[col].shift(window)\n            df[f\"{col}_ret_{window}\"] = df.groupby('stock_id')[col].pct_change(window)\n    \n    print(\"Calculate diff features for specific columns...\")\n    for col in ['ask_price', 'bid_price', 'ask_size', 'bid_size', 'market_urgency', 'imbalance_momentum', 'size_imbalance']:\n        for window in [1, 2, 3,6, 10,15]:\n            df[f\"{col}_diff_{window}\"] = df.groupby(\"stock_id\")[col].diff(window)\n\n    df[\"price_delta\"] = df[\"ask_price\"] - df[\"bid_price\"]\n    df[\"size_delta\"] = df[\"ask_size\"] - df[\"bid_size\"]\n    df[\"auction_imbalance_ratio\"] = df[\"imbalance_buy_sell_flag\"].abs()\n    \n    df[\"volume_weighted_imbalance\"] = (df[\"imbalance_size\"] * df[\"wap\"]) / df[\"volume\"]\n\n    \n    # Bid-Ask Spread Over Time\n    df[\"bid_ask_spread\"] = df[\"ask_price\"] - df[\"bid_price\"]\n    df[\"bid_ask_spread_change\"] = df.groupby(\"stock_id\")[\"bid_ask_spread\"].diff()\n\n    # Order Book Density\n    df[\"order_book_density\"] = (df[\"bid_size\"] + df[\"ask_size\"]) / df[\"bid_ask_spread\"]\n        \n    return df.replace([np.inf, -np.inf], 0)\n\ndef other_features(df):\n    df[\"dow\"] = df[\"date_id\"] % 5  # Day of the week\n    df[\"seconds\"] = df[\"seconds_in_bucket\"] % 60  \n    for key, value in dict_global_stock_id_feats.items():\n        df[f\"global_{key}\"] = df[\"stock_id\"].map(value.to_dict())\n\n    return df\n\n\n","metadata":{"papermill":{"duration":0.049258,"end_time":"2023-11-14T14:56:21.898846","exception":false,"start_time":"2023-11-14T14:56:21.849588","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-11-19T23:34:58.923452Z","iopub.execute_input":"2023-11-19T23:34:58.923826Z","iopub.status.idle":"2023-11-19T23:34:58.979011Z","shell.execute_reply.started":"2023-11-19T23:34:58.923791Z","shell.execute_reply":"2023-11-19T23:34:58.977768Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":" def rolling_polars(df, list_cols, col_group_by, index_column):\n    for col in list_cols:\n        base_col = col\n        for function in [\"mean\", \"std\"]:\n            print(f\"rolling {function} ...\")\n            for window in [3,5,9]:\n\n                rolling_group = df.group_by_rolling(\n                    index_column=index_column,\n                    period=f\"{window}i\",  # 'i' denotes index count (integer)\n                    by=col_group_by,\n                    closed=\"left\",  # Adjust as needed\n                )\n                if function == \"mean\":\n                    df = df.join(\n                        rolling_group.agg(\n                            pl.col(base_col).mean().alias(f\"{base_col}_mean_{window}\")\n                        ),\n                        on=[\"stock_id\", \"time_id\"],\n                        how=\"left\",\n                    )\n                elif( function == \"std\") & (window > 6):\n                    df = df.join(\n                        rolling_group.agg(\n                            pl.col(base_col).std().alias(f\"{base_col}_std_{window}\")\n                        ),\n                        on=[\"stock_id\", \"time_id\"],\n                        how=\"left\",\n                    )\n    return df","metadata":{"execution":{"iopub.status.busy":"2023-11-19T23:34:58.980884Z","iopub.execute_input":"2023-11-19T23:34:58.981221Z","iopub.status.idle":"2023-11-19T23:34:59.027759Z","shell.execute_reply.started":"2023-11-19T23:34:58.981191Z","shell.execute_reply":"2023-11-19T23:34:59.026690Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"\n\ndef feat_engineering(df):\n    # Select relevant columns for feature generation\n    cols = [c for c in df.columns if c not in [\"row_id\"]]\n    df = df[cols]\n    \n    # Generate imbalance features\n    df = imbalance_features(df)\n    df = other_features(df)\n    \n    \n\n    df_pl = pl.DataFrame(df)\n    df_pl = df_pl.sort([\"stock_id\", \"time_id\"])\n\n    df_pl = df_pl.with_columns(\n        (pl.col(\"seconds_in_bucket\") / 10).cast(pl.Int32).alias(\"seconds_in_bucket\")\n    )\n    df_pl = df_pl.with_columns(\n        pl.col(\"time_id\").cast(pl.Int32).alias(\"time_id\")\n    )\n\n    df_pl = rolling_polars(df_pl, [\"wap\",\"volume\",\"price_delta\",\"size_delta\"], [\"stock_id\"], \"time_id\")\n\n    df = df_pl.to_pandas()\n\n    gc.collect()  \n    df = order_flow_imbalance(df)\n    \n    df = trend_momentum_features(df)\n    df = relative_strength_index(df, periods=14)\n    #df = calculate_trend(df)\n    \n    gc.collect()  \n    \n    print(\"Applica le funzioni per generare nuove caratteristiche...\")\n    df = calculate_market_pressure(df)\n    df = calculate_imbalance_ratio(df)\n    df = calculate_auction_price_indicators(df)\n    df = calculate_auction_time_series_features(df)\n   \n    # Calcolo delle variazioni passate di WAP\n    df['wap_change'] = df.groupby('stock_id')['wap'].pct_change()\n\n    # Calcolo del momentum di WAP\n    df['wap_momentum'] = df['wap'] - df.groupby('stock_id')['wap'].shift(1)\n\n    # Calcolo della volatilitÃ  di WAP\n    df['wap_volatility'] = df.groupby('stock_id')['wap'].rolling(window=5).std().reset_index(level=0, drop=True)\n\n    # Rapporto tra WAP e Volume\n    df['wap_volume_ratio'] = df['wap'] / df['volume']\n\n    # Imbalance Dinamico\n    df['dynamic_imbalance'] = df['imbalance_buy_sell_flag'] * df['imbalance_size']\n\n    list_cols = [i for i in df.columns if i not in [\"row_id\"]]\n    gc.collect()  \n    \n    return df[list_cols]\n    ","metadata":{"execution":{"iopub.status.busy":"2023-11-19T23:34:59.029568Z","iopub.execute_input":"2023-11-19T23:34:59.029897Z","iopub.status.idle":"2023-11-19T23:34:59.077776Z","shell.execute_reply.started":"2023-11-19T23:34:59.029868Z","shell.execute_reply":"2023-11-19T23:34:59.076603Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"if TRAIN:\n    df_train_feats = feat_engineering(df_train)\n    print(\"Build Online Train Feats Finished.\")\n\n    df_train_feats = reduce_mem_usage(df_train_feats)","metadata":{"papermill":{"duration":0.022316,"end_time":"2023-11-14T14:56:21.934972","exception":false,"start_time":"2023-11-14T14:56:21.912656","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-11-19T23:34:59.079104Z","iopub.execute_input":"2023-11-19T23:34:59.079494Z","iopub.status.idle":"2023-11-19T23:39:09.032774Z","shell.execute_reply.started":"2023-11-19T23:34:59.079456Z","shell.execute_reply":"2023-11-19T23:39:09.030891Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"Calculate diff features for specific columns...\nrolling mean ...\nrolling std ...\nrolling mean ...\nrolling std ...\nrolling mean ...\nrolling std ...\nrolling mean ...\nrolling std ...\nApplica le funzioni per generare nuove caratteristiche...\nBuild Online Train Feats Finished.\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if TRAIN:\n    col_split = \"time_id\"\n    df_train_feats.sort_values([col_split], inplace=True)\n\n    df_train_feats[\"factorized\"] = pd.factorize(df_train_feats[col_split])[0]\n\n    df_train_feats.sort_values([\"time_id\", \"stock_id\"], inplace=True)\n    list_cols_drop = [\"date_id\", \"time_id\"]\n\n    df_train_feats.reset_index(drop=True, inplace=True)\n    df_train_feats.drop(list_cols_drop, axis=1, inplace=True)","metadata":{"papermill":{"duration":0.023188,"end_time":"2023-11-14T14:56:21.971602","exception":false,"start_time":"2023-11-14T14:56:21.948414","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-11-19T23:39:09.035285Z","iopub.execute_input":"2023-11-19T23:39:09.035683Z","iopub.status.idle":"2023-11-19T23:39:31.117623Z","shell.execute_reply.started":"2023-11-19T23:39:09.035647Z","shell.execute_reply":"2023-11-19T23:39:31.116404Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"papermill":{"duration":0.013082,"end_time":"2023-11-14T14:56:22.051720","exception":false,"start_time":"2023-11-14T14:56:22.038638","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if TRAIN:\n    experiment_name = f\"{EXPERIMENT_PURPOSE}_v{VERSION_NB}\"\n    name_folder_models = f\"models_v{VERSION_NB}\"\n\n    experiment_date_str = datetime.now().strftime(\"%y_%m_%d_%H%M\")\n\n    if DEBUG:\n        experiment_name = f\"{experiment_name}_debug\"\n        name_folder_models = f\"{name_folder_models}_debug\"\n        path_artifact_location = \".\"\n    else:\n        path_artifact_location = pm.path_artifact_location\n\n    experiment_id = get_or_create_experiment(\n        client, experiment_name, artifact_location=path_artifact_location\n    )\n\n    nbrnd_erly_stp = 130\n    cv_mthd = \"KF\"\n\n    mlflow_callback = MLflowCallback(\n        tracking_uri=mlflow.get_tracking_uri(), metric_name=\"mae\"\n    )\n\n    all_cv = {\"KF\": KFold(n_splits=5, shuffle=True, random_state=STATE)}\n    cv = all_cv[cv_mthd]\n\n    dict_models = {\"LGBMR\": LGBMR}\n\n    log_model = True\n\n    args = {\n        \"cv_mthd\": cv_mthd,\n        \"experiment_purpose\": EXPERIMENT_PURPOSE,\n        \"experiment_name\": experiment_name,\n        \"dict_models\": dict_models,\n        \"model_params_dict\": model_params_dict,\n        \"n_splits\": N_SPLITS,\n        \"n_test_splits\": N_TEST_SPLITS,\n        \"n_purge\": N_PURGE,\n        \"n_embargo\": N_EMBARGO,\n        \"experiment_date_str\": experiment_date_str,\n        \"path_artifact_location\": pm.path_artifact_location,\n        \"target_col\": \"target\",\n    }","metadata":{"papermill":{"duration":0.032117,"end_time":"2023-11-14T14:56:22.097275","exception":false,"start_time":"2023-11-14T14:56:22.065158","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-11-19T23:39:31.122642Z","iopub.execute_input":"2023-11-19T23:39:31.123025Z","iopub.status.idle":"2023-11-19T23:39:32.999745Z","shell.execute_reply.started":"2023-11-19T23:39:31.122991Z","shell.execute_reply":"2023-11-19T23:39:32.998427Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"Created new experiment with ID 802089213934157600.\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"papermill":{"duration":0.013498,"end_time":"2023-11-14T14:56:22.125143","exception":false,"start_time":"2023-11-14T14:56:22.111645","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def run_mlflow_experiment(df_train, args, trial=None):\n    cv_mthd = args[\"cv_mthd\"]\n    experiment_purpose = args[\"experiment_purpose\"]\n    experiment_name = args[\"experiment_name\"]\n    dict_models = args[\"dict_models\"]\n    model_params_dict = args[\"model_params_dict\"]\n\n    n_splits = args[\"n_splits\"]\n    n_test_splits = args[\"n_test_splits\"]\n    n_purge = args[\"n_purge\"]\n    n_embargo = args[\"n_embargo\"]\n\n    experiment_date_str = args[\"experiment_date_str\"]\n    path_artifact_location = args[\"path_artifact_location\"]\n    target_col = args[\"target_col\"]\n\n    if trial == None:\n        trial = optuna.trial.FixedTrial(\n            {\n                \"n_estimators\": 500,\n                \"learning_rate\": 0.005,\n                \"max_depth\": 10,\n                \"num_leaves\": 20,\n                \"min_child_samples\": 10,\n                \"subsample\": 0.7,\n                \"colsample_bytree\": 1.0,\n                \"min_split_gain\": 0.0,\n                \"reg_alpha\": 0.0,\n                \"reg_lambda\": 0.0,\n                \"device\": \"gpu\" if GPU_SWITCH == \"ON\" else \"cpu\",\n            }\n        )\n\n    run_time_start_trial = datetime.now().strftime(\"%y_%m_%d_%H%M%S\")\n\n    with mlflow.start_run(\n        run_name=run_time_start_trial, experiment_id=experiment_id\n    ) as run:\n        score_list = []\n\n        # mlflow.set_tag(\"cv_mthd\", cv_mthd)\n        mlflow.set_tag(\"n_splits\", n_splits)\n        mlflow.set_tag(\"n_test_splits\", n_test_splits)\n        mlflow.set_tag(\"n_purge\", n_purge)\n        mlflow.set_tag(\"n_embargo\", n_embargo)\n\n        for model_name, model_class in dict_models.items():\n            if TUNING:\n                \n                model = create_model(\n                    trial,\n                    model_class,\n                    model_params_dict[model_name][\"static_params\"],\n                    model_params_dict[model_name][\"dynamic_params\"],\n                )\n            \n            else:\n                params = model_prod.get_params()\n                params[\"device\"] = \"gpu\" if GPU_SWITCH == \"ON\" else \"cpu\"\n\n                model = model_class(**params)\n\n            priority_params = [\"learning_rate\", \"max_depth\"]\n            excluded_params = [\n                \"device\",\n                \"class_weight\",\n                \"random_state\",\n                \"silent\",\n                \"verbose\",\n                \"n_jobs\",\n            ]\n\n            ordered_params = log_model_parameters(\n                model, priority_params, excluded_params, verbose=True\n            )\n\n            mlflow.log_params(ordered_params)\n\n            for fold_n, (train_indices, test_indices) in enumerate(\n                time_series_split(\n                    df_train,\n                    n_splits=n_splits,\n                    n_test_splits=n_test_splits,\n                    n_purge=n_purge,\n                    n_embargo=n_embargo,\n                )\n            ):\n                \n                with mlflow.start_run(\n                    run_name=f\"fold_{fold_n+1}\",\n                    nested=True,\n                    experiment_id=experiment_id,\n                ) as nested_run:\n                    mlflow.set_tag(\"n_trial\", str(trial.number))\n\n                    mask_train = df_train[\"factorized\"].isin(train_indices)\n                    mask_test = df_train[\"factorized\"].isin(test_indices)\n\n                    y_train = df_train.loc[mask_train, target_col]\n                    y_val = df_train.loc[mask_test, target_col]\n                    X_train = df_train.loc[mask_train].drop(\n                        [target_col, \"factorized\"], axis=1\n                    )\n                    X_val = df_train.loc[mask_test].drop(\n                        [target_col, \"factorized\"], axis=1\n                    )\n                    \n                    print(f\"X_train shape: {X_train.shape}\")\n\n                    mlflow.log_param(\"train_rows\", X_train.shape[0])\n                    mlflow.log_param(\"train_cols\", X_train.shape[1])\n\n                    model.fit(\n                        X_train,\n                        y_train,\n                        eval_set=[(X_val, y_val)],\n                        eval_metric=\"mae\",\n                        callbacks=[\n                            lgbm.callback.early_stopping(stopping_rounds=100),\n                            lgbm.callback.log_evaluation(period=100),\n                        ],\n                    )\n                    \n\n                    log_feature_importance(\n                        trial.number,\n                        model,\n                        X_train,\n                        fold_n,\n                        experiment_purpose,\n                        experiment_date_str,\n                    )\n                    \n                    del mask_train, mask_test, X_train, y_train\n\n                    fold_score = model.best_score_[\"valid_0\"][\"l1\"]\n\n                    score_list.append(fold_score)\n\n                    mlflow.log_metric(\"fold_score\", round(fold_score, 6))\n                    mlflow.log_param(\"fold_number\", fold_n + 1)\n                    mlflow.log_param(\"model_name\", model_name)\n\n                    mlflow.log_params(ordered_params)\n\n                    current_time_str = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n                    model_log_name = f\"{model_name}_{trial.number}_{current_time_str}\"\n\n                    mlflow.sklearn.log_model(model, model_log_name)\n\n                    mlflow.log_param(\"run_time\", current_time_str)\n\n                    nested_run_id = nested_run.info.run_id\n                    model_path = f\"{path_artifact_location}/{run.info.experiment_id}/{nested_run_id}/artifacts/{model_log_name}/model.pkl\"\n                    mlflow.log_param(\"model_path\", model_path)\n\n                avg_score = sum(score_list) / len(score_list)\n                median_score = np.median(score_list)\n                mlflow.log_metric(\"avg score\", round(avg_score, 6))\n                mlflow.log_metric(\"median score\", round(median_score, 6))\n                gc.collect()  \n                if fold_n >= N_FOLD_BREAK:\n                    break\n\n        return avg_score\n\n\ndef objective(trial, df_train):\n    avg_score = run_mlflow_experiment(df_train, args, trial)\n    return avg_score\n\n\n# Run the Optuna study\nif TRAIN:\n    study = optuna.create_study(\n        direction=\"minimize\",\n        study_name=\"Your Study Name\",\n        load_if_exists=True,\n    )\n    study.optimize(lambda trial: objective(trial, df_train_feats), n_trials=N_TRIALS)","metadata":{"papermill":{"duration":0.045154,"end_time":"2023-11-14T14:56:22.184731","exception":false,"start_time":"2023-11-14T14:56:22.139577","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-11-19T23:39:33.001596Z","iopub.execute_input":"2023-11-19T23:39:33.001937Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":"[I 2023-11-19 23:39:33,057] A new study created in memory with name: Your Study Name\n","output_type":"stream"},{"name":"stdout","text":"\nlearning_rate: 0.01171 | max_depth: 13 | boosting_type: gbdt | colsample_bytree: 0.5 | importance_type: gain | min_child_samples: 15 | min_child_weight: 0.001 | min_split_gain: 0.0 | n_estimators: 1000 | num_leaves: 212 | objective: mae | reg_alpha: 0.0 | reg_lambda: 0.0 | subsample: 0.45 | subsample_for_bin: 200000 | subsample_freq: 0 | max_bin: 254\n\nX_train shape: (4175748, 209)\nTraining until validation scores don't improve for 100 rounds\n[100]\tvalid_0's l1: 5.9697\n[200]\tvalid_0's l1: 5.95222\n[300]\tvalid_0's l1: 5.94286\n[400]\tvalid_0's l1: 5.93771\n[500]\tvalid_0's l1: 5.9338\n[600]\tvalid_0's l1: 5.93126\n[700]\tvalid_0's l1: 5.92972\n[800]\tvalid_0's l1: 5.92855\n[900]\tvalid_0's l1: 5.92802\n[1000]\tvalid_0's l1: 5.92755\nDid not meet early stopping. Best iteration is:\n[996]\tvalid_0's l1: 5.92754\nX_train shape: (4172120, 209)\nTraining until validation scores don't improve for 100 rounds\n[100]\tvalid_0's l1: 6.3399\n[200]\tvalid_0's l1: 6.32173\n[300]\tvalid_0's l1: 6.31143\n[400]\tvalid_0's l1: 6.30533\n[500]\tvalid_0's l1: 6.30074\n[600]\tvalid_0's l1: 6.29791\n[700]\tvalid_0's l1: 6.2961\n[800]\tvalid_0's l1: 6.2944\n[900]\tvalid_0's l1: 6.29358\n[1000]\tvalid_0's l1: 6.29294\nDid not meet early stopping. Best iteration is:\n[1000]\tvalid_0's l1: 6.29294\nX_train shape: (4177023, 209)\nTraining until validation scores don't improve for 100 rounds\n[100]\tvalid_0's l1: 6.41845\n[200]\tvalid_0's l1: 6.39891\n[300]\tvalid_0's l1: 6.38812\n[400]\tvalid_0's l1: 6.38173\n[500]\tvalid_0's l1: 6.37772\n[600]\tvalid_0's l1: 6.37491\n[700]\tvalid_0's l1: 6.37298\n[800]\tvalid_0's l1: 6.3716\n[900]\tvalid_0's l1: 6.37052\n[1000]\tvalid_0's l1: 6.36962\nDid not meet early stopping. Best iteration is:\n[996]\tvalid_0's l1: 6.3696\nX_train shape: (4186996, 209)\nTraining until validation scores don't improve for 100 rounds\n[100]\tvalid_0's l1: 7.08979\n[200]\tvalid_0's l1: 7.05869\n[300]\tvalid_0's l1: 7.04417\n[400]\tvalid_0's l1: 7.0354\n[500]\tvalid_0's l1: 7.02986\n[600]\tvalid_0's l1: 7.02571\n[700]\tvalid_0's l1: 7.02302\n[800]\tvalid_0's l1: 7.02102\n[900]\tvalid_0's l1: 7.01966\n[1000]\tvalid_0's l1: 7.01848\nDid not meet early stopping. Best iteration is:\n[1000]\tvalid_0's l1: 7.01848\nX_train shape: (4207881, 209)\nTraining until validation scores don't improve for 100 rounds\n[100]\tvalid_0's l1: 5.66566\n[200]\tvalid_0's l1: 5.64202\n[300]\tvalid_0's l1: 5.63068\n[400]\tvalid_0's l1: 5.62404\n[500]\tvalid_0's l1: 5.6201\n[600]\tvalid_0's l1: 5.61747\n[700]\tvalid_0's l1: 5.61587\n[800]\tvalid_0's l1: 5.61464\n[900]\tvalid_0's l1: 5.61376\n[1000]\tvalid_0's l1: 5.61332\nDid not meet early stopping. Best iteration is:\n[1000]\tvalid_0's l1: 5.61332\n","output_type":"stream"},{"name":"stderr","text":"[I 2023-11-20 02:51:24,583] Trial 0 finished with value: 6.244375806549036 and parameters: {'n_estimators': 1000, 'learning_rate': 0.011707145722188879, 'max_depth': 13, 'num_leaves': 212, 'min_child_samples': 15, 'subsample': 0.45, 'colsample_bytree': 0.5}. Best is trial 0 with value: 6.244375806549036.\n","output_type":"stream"},{"name":"stdout","text":"\nlearning_rate: 0.01135 | max_depth: 13 | boosting_type: gbdt | colsample_bytree: 0.5 | importance_type: gain | min_child_samples: 23 | min_child_weight: 0.001 | min_split_gain: 0.0 | n_estimators: 1000 | num_leaves: 208 | objective: mae | reg_alpha: 0.0 | reg_lambda: 0.0 | subsample: 0.45 | subsample_for_bin: 200000 | subsample_freq: 0 | max_bin: 254\n\nX_train shape: (4175748, 209)\nTraining until validation scores don't improve for 100 rounds\n[100]\tvalid_0's l1: 5.971\n[200]\tvalid_0's l1: 5.95266\n[300]\tvalid_0's l1: 5.94377\n[400]\tvalid_0's l1: 5.93831\n[500]\tvalid_0's l1: 5.93459\n[600]\tvalid_0's l1: 5.93166\n[700]\tvalid_0's l1: 5.92979\n[800]\tvalid_0's l1: 5.92896\n[900]\tvalid_0's l1: 5.92803\n[1000]\tvalid_0's l1: 5.92727\nDid not meet early stopping. Best iteration is:\n[998]\tvalid_0's l1: 5.92727\nX_train shape: (4172120, 209)\nTraining until validation scores don't improve for 100 rounds\n[100]\tvalid_0's l1: 6.34121\n[200]\tvalid_0's l1: 6.32245\n[300]\tvalid_0's l1: 6.3128\n[400]\tvalid_0's l1: 6.30668\n[500]\tvalid_0's l1: 6.30223\n[600]\tvalid_0's l1: 6.29902\n[700]\tvalid_0's l1: 6.29704\n[800]\tvalid_0's l1: 6.29586\n[900]\tvalid_0's l1: 6.29475\n[1000]\tvalid_0's l1: 6.29385\nDid not meet early stopping. Best iteration is:\n[1000]\tvalid_0's l1: 6.29385\nX_train shape: (4177023, 209)\nTraining until validation scores don't improve for 100 rounds\n[100]\tvalid_0's l1: 6.4199\n[200]\tvalid_0's l1: 6.39959\n[300]\tvalid_0's l1: 6.38975\n[400]\tvalid_0's l1: 6.38351\n[500]\tvalid_0's l1: 6.37912\n[600]\tvalid_0's l1: 6.3757\n[700]\tvalid_0's l1: 6.37313\n[800]\tvalid_0's l1: 6.37158\n[900]\tvalid_0's l1: 6.37047\n[1000]\tvalid_0's l1: 6.36949\nDid not meet early stopping. Best iteration is:\n[1000]\tvalid_0's l1: 6.36949\nX_train shape: (4186996, 209)\nTraining until validation scores don't improve for 100 rounds\n[100]\tvalid_0's l1: 7.09184\n[200]\tvalid_0's l1: 7.05976\n[300]\tvalid_0's l1: 7.0456\n[400]\tvalid_0's l1: 7.03661\n[500]\tvalid_0's l1: 7.03087\n[600]\tvalid_0's l1: 7.02664\n[700]\tvalid_0's l1: 7.0237\n[800]\tvalid_0's l1: 7.02158\n[900]\tvalid_0's l1: 7.02013\n[1000]\tvalid_0's l1: 7.01911\nDid not meet early stopping. Best iteration is:\n[999]\tvalid_0's l1: 7.01911\nX_train shape: (4207881, 209)\nTraining until validation scores don't improve for 100 rounds\n[100]\tvalid_0's l1: 5.66697\n[200]\tvalid_0's l1: 5.64272\n[300]\tvalid_0's l1: 5.63166\n[400]\tvalid_0's l1: 5.62465\n[500]\tvalid_0's l1: 5.62079\n[600]\tvalid_0's l1: 5.61815\n[700]\tvalid_0's l1: 5.61583\n[800]\tvalid_0's l1: 5.61458\n[900]\tvalid_0's l1: 5.61367\n[1000]\tvalid_0's l1: 5.61277\nDid not meet early stopping. Best iteration is:\n[996]\tvalid_0's l1: 5.61276\n","output_type":"stream"},{"name":"stderr","text":"[I 2023-11-20 06:04:35,835] Trial 1 finished with value: 6.244496886906506 and parameters: {'n_estimators': 1000, 'learning_rate': 0.011351818800013512, 'max_depth': 13, 'num_leaves': 208, 'min_child_samples': 23, 'subsample': 0.45, 'colsample_bytree': 0.5}. Best is trial 0 with value: 6.244375806549036.\n","output_type":"stream"},{"name":"stdout","text":"\nlearning_rate: 0.01299 | max_depth: 13 | boosting_type: gbdt | colsample_bytree: 0.5 | importance_type: gain | min_child_samples: 20 | min_child_weight: 0.001 | min_split_gain: 0.0 | n_estimators: 1000 | num_leaves: 205 | objective: mae | reg_alpha: 0.0 | reg_lambda: 0.0 | subsample: 0.45 | subsample_for_bin: 200000 | subsample_freq: 0 | max_bin: 254\n\nX_train shape: (4175748, 209)\nTraining until validation scores don't improve for 100 rounds\n[100]\tvalid_0's l1: 5.96664\n[200]\tvalid_0's l1: 5.94998\n[300]\tvalid_0's l1: 5.94101\n[400]\tvalid_0's l1: 5.93553\n[500]\tvalid_0's l1: 5.93231\n","output_type":"stream"}]},{"cell_type":"code","source":"if TRAIN:\n    df_exp = experiments_data(\n        client, list_experiment_id=None, save_df=None, list_columns=None\n    )\n    list_base_cols = [\n        \"run_time\",\n        \"experiment_id\",\n        \"n_trial\",\n        \"run_id\",\n        \"model_name\",\n        \"fold_number\",\n        \"fold_score\",\n    ]\n    list_dynamic_params = list(model_params_dict[\"LGBMR\"][\"dynamic_params\"].keys())\n\n    df_exp[\"run_time\"] = pd.to_datetime(\n        df_exp[\"run_time\"], format=\"%Y%m%d_%H%M%S\", errors=\"coerce\"\n    )\n\n    for col in df_exp.columns:\n        df_exp[col] = pd.to_numeric(df_exp[col], errors=\"ignore\")\n\n    for col in df_exp.select_dtypes(include=[\"float\", \"int\"]):\n        df_exp[col] = df_exp[col].round(5)\n\n    list_cols_exp = [\"run_time\"] + list_base_cols + list_dynamic_params + [\"model_path\"]\n\n    experiment_id\n    df_exp = df_exp[df_exp[\"experiment_id\"] != 0]\n\n    df_exp = df_exp[list_cols_exp]","metadata":{"_cell_guid":"51030a77-aba3-469f-9c08-7963bc8a09d2","_uuid":"6ff7d358-62f0-4ac9-b028-d1c920bb4eaf","collapsed":false,"jupyter":{"outputs_hidden":false},"papermill":{"duration":0.026377,"end_time":"2023-11-14T14:56:22.224975","exception":false,"start_time":"2023-11-14T14:56:22.198598","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"papermill":{"duration":0.012914,"end_time":"2023-11-14T14:56:22.278091","exception":false,"start_time":"2023-11-14T14:56:22.265177","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"papermill":{"duration":0.013003,"end_time":"2023-11-14T14:56:22.304410","exception":false,"start_time":"2023-11-14T14:56:22.291407","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"_cell_guid":"d4bfb979-2758-4fe9-8947-399b1c3a574c","_uuid":"bef1a199-0988-4743-813e-99d4308fc9bb","collapsed":false,"jupyter":{"outputs_hidden":false},"papermill":{"duration":0.042912,"end_time":"2023-11-14T14:56:22.361956","exception":false,"start_time":"2023-11-14T14:56:22.319044","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train = pd.read_csv(\"/kaggle/input/optiver-trading-at-the-close/train.csv\")","metadata":{"papermill":{"duration":19.451534,"end_time":"2023-11-14T14:56:41.827800","exception":false,"start_time":"2023-11-14T14:56:22.376266","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dict_global_stock_id_feats = global_stock_id_feats(df_train)","metadata":{"papermill":{"duration":1.784748,"end_time":"2023-11-14T14:56:43.626460","exception":false,"start_time":"2023-11-14T14:56:41.841712","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"_cell_guid":"d30f3862-dca2-4242-9db0-ab9750118622","_uuid":"efd9073c-aaef-4135-bf7c-1e892e9aa831","collapsed":false,"jupyter":{"outputs_hidden":false},"papermill":{"duration":476.139875,"end_time":"2023-11-14T15:04:39.780027","exception":false,"start_time":"2023-11-14T14:56:43.640152","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def zero_sum(prices, volumes):\n    std_error = np.sqrt(volumes)\n    step = np.sum(prices) / np.sum(std_error)\n    out = prices - std_error * step\n    return out\n\nimport optiver2023\nenv = optiver2023.make_env()\niter_test = env.iter_test()\ncounter = 0\ny_min, y_max = -64, 64\nqps, predictions = [], []\ncache = pd.DataFrame()\n\nfor (test, revealed_targets, sample_prediction) in iter_test:\n    now_time = time.time()\n    cache = pd.concat([cache, test], ignore_index=True, axis=0)\n    if counter > 0:\n        cache = cache.groupby(['stock_id']).tail(21).sort_values(by=['date_id', 'seconds_in_bucket', 'stock_id']).reset_index(drop=True)\n\n    feat = feat_engineering(cache)[-len(test):]\n\n    # added after new API, reference: https://www.kaggle.com/competitions/optiver-trading-at-the-close/discussion/455690#2526672\n    if test.currently_scored.iloc[0]== False:\n        sample_prediction['target'] = 0\n        env.predict(sample_prediction)\n        counter += 1\n        qps.append(time.time() - now_time)\n        if counter % 10 == 0:\n            print(counter, 'qps:', np.mean(qps))\n        continue\n\n    #feat = feat.drop(columns = [\"currently_scored\"])    \n    # end of new codes for new API\n\n    # Generate predictions for each model and calculate the weighted average\n\n    list_cols_drop = [\"date_id\"]\n    feat.drop(list_cols_drop, axis=1, inplace=True)\n\n    list_features = model_prod.models[0].feature_name_\n    feat = feat[list_features]\n    lgb_predictions = model_prod.predict(feat, \"mean\")\n\n\n    lgb_predictions = zero_sum(lgb_predictions, test['bid_size'] + test['ask_size'])\n    clipped_predictions = np.clip(lgb_predictions, y_min, y_max)\n    sample_prediction['target'] = clipped_predictions\n    env.predict(sample_prediction)\n    counter += 1\n    qps.append(time.time() - now_time)\n    if counter % 10 == 0:\n        print(counter, 'qps:', np.mean(qps))\n\ntime_cost = 1.146 * np.mean(qps)\nprint(f\"The code will take approximately {np.round(time_cost, 4)} hours to reason about\")","metadata":{"papermill":{"duration":0.013508,"end_time":"2023-11-14T15:04:39.808301","exception":false,"start_time":"2023-11-14T15:04:39.794793","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# clean_directory_except_one('/kaggle/working/','submission.csv')","metadata":{"papermill":{"duration":0.013731,"end_time":"2023-11-14T15:04:39.835856","exception":false,"start_time":"2023-11-14T15:04:39.822125","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}