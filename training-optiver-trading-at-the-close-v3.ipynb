{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Constants\nTRAIN = True\nOVERWRITE = False\nDEBUG = True\n\nmodels_dir = \"models_5\"\n\nN_TRIALS = 3\n\nVERSION_NB = 4\n\nstate = 42\n\ndownload_kaggle_data = False\n\n# External general-purpose modules\nimport os\nimport gc\n\nimport shutil\nimport warnings\nfrom datetime import datetime\nimport glob\nfrom itertools import combinations\nfrom warnings import simplefilter\n\nimport numpy as np\nimport pandas as pd\nimport polars as pl\nfrom dotenv import load_dotenv\nfrom joblib import dump\nimport joblib\nimport os\n\n# Setting pandas options and warning filters\npd.set_option(\"display.max_columns\", None)\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\nwarnings.filterwarnings(\"ignore\")\nsimplefilter(action=\"ignore\", category=pd.errors.PerformanceWarning)\n\n# Load environment variables\nload_dotenv()","metadata":{"papermill":{"duration":0.660513,"end_time":"2023-10-29T17:16:44.604311","exception":false,"start_time":"2023-10-29T17:16:43.943798","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-11-08T13:11:26.819214Z","iopub.execute_input":"2023-11-08T13:11:26.819620Z","iopub.status.idle":"2023-11-08T13:11:26.833737Z","shell.execute_reply.started":"2023-11-08T13:11:26.819589Z","shell.execute_reply":"2023-11-08T13:11:26.832466Z"},"trusted":true},"execution_count":2,"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"False"},"metadata":{}}]},{"cell_type":"code","source":"\npath_project_dir = os.getcwd()\nif path_project_dir not in [\"/kaggle/working\", \"/content\"]:\n    path_project_dir = os.getenv(\"ROOT_PATH\")\n\n# Imports and setup for training\nfrom numba import njit, prange\nif TRAIN:\n    \n\n    import itertools as itt\n    # Install packages and import logging libraries\n    if path_project_dir == '/kaggle/working':\n        !pip install loguru mlflow optuna > /dev/null\n        \n\n    from utils import log_feature_importance, create_model, log_training_details, aggregate_feature_importance,  get_data, clean_directory_except_one\n    \n    from loguru import logger\n    import mlflow\n    import optuna\n    from optuna.integration.mlflow import MLflowCallback\n    from mlflow.tracking import MlflowClient\n    import zipfile\n    \n    from tqdm import tqdm\n\n    # Import machine learning libraries\n    import lightgbm as lgbm\n    \n    from lightgbm import log_evaluation, early_stopping, LGBMRegressor as LGBMR\n    from sklearn.model_selection import KFold\n    from xgboost import XGBRegressor as XGBR\n\n    # Set logging\n    logger.add(\"logs.log\", format=\"{time:YYYY-MM-DD HH:mm} | {level} | {message}\")\n    optuna.logging.set_verbosity(optuna.logging.WARNING)\n    warnings.filterwarnings(\"ignore\", category=optuna.exceptions.ExperimentalWarning)\n    \n    # Auto-reload modules\n    %load_ext autoreload\n    %autoreload 2\n\n    # Initialize MLflow callback\n    mlflow_callback = MLflowCallback(\n        tracking_uri=mlflow.get_tracking_uri(), metric_name=\"mae\"\n    )","metadata":{"_cell_guid":"3bdaa15b-0b9e-4b3b-9c47-58e6b9b18b5e","_uuid":"206043b9-f1d5-4691-b03a-935f2177086d","collapsed":false,"jupyter":{"outputs_hidden":false},"papermill":{"duration":0.032481,"end_time":"2023-10-29T17:16:44.649372","exception":false,"start_time":"2023-10-29T17:16:44.616891","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-11-08T13:11:31.363391Z","iopub.execute_input":"2023-11-08T13:11:31.363797Z","iopub.status.idle":"2023-11-08T13:11:57.922501Z","shell.execute_reply.started":"2023-11-08T13:11:31.363767Z","shell.execute_reply":"2023-11-08T13:11:57.921175Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"if path_project_dir == \"/kaggle/working\":\n    path_data_project_dir = \"/kaggle/input/optiver-trading-at-the-close\"\n    path_experiments_storage = os.path.join(path_project_dir, \"experiments_storage\")\n\n    path_dataset_train_raw = \"/kaggle/input/optiver-trading-at-the-close/train.csv\"\n    path_dataset_test_raw = (\n        \"/kaggle/input/optiver-trading-at-the-close/example_test_files/test.csv\"\n    )\n\n    path_dataset_processed = \"/kaggle/working/processed_data\"\n    path_dataset_train = os.path.join(path_dataset_processed, \"train.csv\")\n    path_dataset_test = os.path.join(path_dataset_processed, \"test.csv\")\n\nelse:\n    name_folder_data_project = \"kaggle_optiver_trading_at_the_close\"\n\n    path_data_dir = os.path.join(path_project_dir, \"data\")\n    path_dataset_train_raw = os.path.join(\n        path_data_dir, \"kaggle_optiver_trading_at_the_close/raw\", \"train.csv\"\n    )\n    path_dataset_processed = os.path.join(\n        path_data_dir, \"kaggle_optiver_trading_at_the_close/processed\"\n    )\n\n    path_data_project_dir = os.path.join(path_data_dir, name_folder_data_project)\n\n    path_config_dir = os.path.join(path_project_dir, \"config\")\n    path_config_train = os.path.join(path_config_dir, \"train_config.yaml\")\n\n    path_experiments_storage = os.path.join(\n        path_data_project_dir, \"experiments_storage\"\n    )\n\n    if download_kaggle_data:\n        dataset_name = \"ravi20076/optiver-memoryreduceddatasets\"\n        kaggle_json_path = os.path.join(path_project_dir, \"kaggle.json\")\n        get_data(\n            kaggle_json_path,\n            path_data_project_dir,\n            dataset_name=dataset_name,\n            specific_file=None,\n        )\n\n    file_name_df_train = \"train.csv\"\n    file_name_df_test = \"test.csv\"\n\n    path_dataset_train = os.path.join(path_data_project_dir, file_name_df_train)\n    path_dataset_test = os.path.join(path_data_project_dir, file_name_df_test)\n\nif TRAIN:\n    mlflow.set_tracking_uri(path_experiments_storage)\n    client = MlflowClient()","metadata":{"_cell_guid":"9765420b-f8ab-46ba-8b4c-e4487eb04b76","_uuid":"c4174134-1441-41f8-adbf-f03ad3f7d7dc","collapsed":false,"jupyter":{"outputs_hidden":false},"papermill":{"duration":0.024352,"end_time":"2023-10-29T17:16:44.685842","exception":false,"start_time":"2023-10-29T17:16:44.66149","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-11-08T13:11:57.925719Z","iopub.execute_input":"2023-11-08T13:11:57.926471Z","iopub.status.idle":"2023-11-08T13:11:57.988036Z","shell.execute_reply.started":"2023-11-08T13:11:57.926427Z","shell.execute_reply":"2023-11-08T13:11:57.986534Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"\n@njit(parallel=True)\ndef compute_triplet_imbalance(df_values, comb_indices):\n    num_rows = df_values.shape[0]\n    num_combinations = len(comb_indices)\n    imbalance_features = np.empty((num_rows, num_combinations))\n\n    for i in prange(num_combinations):\n        a, b, c = comb_indices[i]\n        for j in range(num_rows):\n            max_val = max(df_values[j, a], df_values[j, b], df_values[j, c])\n            min_val = min(df_values[j, a], df_values[j, b], df_values[j, c])\n            mid_val = df_values[j, a] + df_values[j, b] + df_values[j, c] - min_val - max_val\n            if mid_val == min_val:  # Prevent division by zero\n                imbalance_features[j, i] = np.nan\n            else:\n                imbalance_features[j, i] = (max_val - mid_val) / (mid_val - min_val)\n\n    return imbalance_features\n\n\ndef calculate_triplet_imbalance_numba(price, df):\n    # Convert DataFrame to numpy array for Numba compatibility\n    df_values = df[price].values\n    comb_indices = [(price.index(a), price.index(b), price.index(c)) for a, b, c in combinations(price, 3)]\n\n    # Calculate the triplet imbalance\n    features_array = compute_triplet_imbalance(df_values, comb_indices)\n\n    # Create a DataFrame from the results\n    columns = [f\"{a}_{b}_{c}_imb2\" for a, b, c in combinations(price, 3)]\n    features = pd.DataFrame(features_array, columns=columns)\n\n    return features","metadata":{"execution":{"iopub.status.busy":"2023-11-08T13:11:57.989375Z","iopub.execute_input":"2023-11-08T13:11:57.989749Z","iopub.status.idle":"2023-11-08T13:11:58.149183Z","shell.execute_reply.started":"2023-11-08T13:11:57.989718Z","shell.execute_reply":"2023-11-08T13:11:58.147597Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"def reduce_mem_usage(df, verbose=0):\n    \"\"\"\n    Iterate through all numeric columns of a dataframe and modify the data type\n    to reduce memory usage.\n    \"\"\"\n\n    start_mem = df.memory_usage().sum() / 1024**2\n\n    for col in df.columns:\n        col_type = df[col].dtype\n\n        if (col_type != object) and (col != \"target\"):\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == \"int\":\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)\n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float32)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float32)\n\n    if verbose:\n        logger.info(f\"Memory usage of dataframe is {start_mem:.2f} MB\")\n        end_mem = df.memory_usage().sum() / 1024**2\n        logger.info(f\"Memory usage after optimization is: {end_mem:.2f} MB\")\n        decrease = 100 * (start_mem - end_mem) / start_mem\n        logger.info(f\"Decreased by {decrease:.2f}%\")\n\n    return df\n","metadata":{"execution":{"iopub.status.busy":"2023-11-08T13:11:58.154157Z","iopub.execute_input":"2023-11-08T13:11:58.154769Z","iopub.status.idle":"2023-11-08T13:11:58.219926Z","shell.execute_reply.started":"2023-11-08T13:11:58.154726Z","shell.execute_reply":"2023-11-08T13:11:58.218679Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"# generate imbalance features\ndef imbalance_features(df):\n    prices = [\"reference_price\", \"far_price\", \"near_price\", \"ask_price\", \"bid_price\", \"wap\"]\n    sizes = [\"matched_size\", \"bid_size\", \"ask_size\", \"imbalance_size\"]\n\n    # V1\n    df[\"volume\"] = df.eval(\"ask_size + bid_size\")\n    df[\"mid_price\"] = df.eval(\"(ask_price + bid_price) / 2\")\n    df[\"liquidity_imbalance\"] = df.eval(\"(bid_size-ask_size)/(bid_size+ask_size)\")\n    df[\"matched_imbalance\"] = df.eval(\"(imbalance_size-matched_size)/(matched_size+imbalance_size)\")\n    df[\"size_imbalance\"] = df.eval(\"bid_size / ask_size\")\n    \n    for c in combinations(prices, 2):\n        df[f\"{c[0]}_{c[1]}_imb\"] = df.eval(f\"({c[0]} - {c[1]})/({c[0]} + {c[1]})\")\n\n    for c in [['ask_price', 'bid_price', 'wap', 'reference_price'], sizes]:\n        triplet_feature = calculate_triplet_imbalance_numba(c, df)\n        df[triplet_feature.columns] = triplet_feature.values\n        \n    # V2\n    df[\"stock_weights\"] = df[\"stock_id\"].map(weights)\n    df[\"weighted_wap\"] = df[\"stock_weights\"] * df[\"wap\"]\n    df['wap_momentum'] = df.groupby('stock_id')['weighted_wap'].pct_change(periods=6)\n    df[\"imbalance_momentum\"] = df.groupby(['stock_id'])['imbalance_size'].diff(periods=1) / df['matched_size']\n    df[\"price_spread\"] = df[\"ask_price\"] - df[\"bid_price\"]\n    df[\"spread_intensity\"] = df.groupby(['stock_id'])['price_spread'].diff()\n    df['price_pressure'] = df['imbalance_size'] * (df['ask_price'] - df['bid_price'])\n    df['market_urgency'] = df['price_spread'] * df['liquidity_imbalance']\n    df['depth_pressure'] = (df['ask_size'] - df['bid_size']) * (df['far_price'] - df['near_price'])\n    df['spread_depth_ratio'] = (df['ask_price'] - df['bid_price']) / (df['bid_size'] + df['ask_size'])\n    df['mid_price_movement'] = df['mid_price'].diff(periods=5).apply(lambda x: 1 if x > 0 else (-1 if x < 0 else 0))\n    df['micro_price'] = ((df['bid_price'] * df['ask_size']) + (df['ask_price'] * df['bid_size'])) / (df['bid_size'] + df['ask_size'])\n    df['relative_spread'] = (df['ask_price'] - df['bid_price']) / df['wap']\n    \n    for func in [\"mean\", \"std\", \"skew\", \"kurt\"]:\n        df[f\"all_prices_{func}\"] = df[prices].agg(func, axis=1)\n        df[f\"all_sizes_{func}\"] = df[sizes].agg(func, axis=1)\n        \n    # V3\n    for col in ['matched_size', 'imbalance_size', 'reference_price', 'imbalance_buy_sell_flag']:\n        for window in [1, 2, 3, 5, 10]:\n            df[f\"{col}_shift_{window}\"] = df.groupby('stock_id')[col].shift(window)\n            df[f\"{col}_ret_{window}\"] = df.groupby('stock_id')[col].pct_change(window)\n            \n    for col in ['ask_price', 'bid_price', 'ask_size', 'bid_size',\n                'wap', 'near_price', 'far_price']:\n        for window in [1, 2, 3, 5, 10]:\n            df[f\"{col}_diff_{window}\"] = df.groupby(\"stock_id\")[col].diff(window)\n\n    return df.replace([np.inf, -np.inf], 0)\n\n# generate time & stock features\ndef other_features(df):\n    df[\"dow\"] = df[\"date_id\"] % 5\n    df[\"dom\"] = df[\"date_id\"] % 20\n    df[\"seconds\"] = df[\"seconds_in_bucket\"] % 60\n    df[\"minute\"] = df[\"seconds_in_bucket\"] // 60\n\n    for key, value in global_stock_id_feats.items():\n        df[f\"global_{key}\"] = df[\"stock_id\"].map(value.to_dict())\n\n    return df\n\n# generate all features\ndef feat_engineering(df):\n    cols = [c for c in df.columns if c not in [\"row_id\", \"time_id\", \"target\"]]\n    df = df[cols]\n    df = imbalance_features(df)\n    df = other_features(df)\n    gc.collect()\n    \n    feature_name = [i for i in df.columns if i not in [\"row_id\", \"target\", \"time_id\", \"date_id\"]]\n    \n    return df[feature_name]","metadata":{"execution":{"iopub.status.busy":"2023-11-08T13:11:58.221847Z","iopub.execute_input":"2023-11-08T13:11:58.222276Z","iopub.status.idle":"2023-11-08T13:11:58.298372Z","shell.execute_reply.started":"2023-11-08T13:11:58.222228Z","shell.execute_reply":"2023-11-08T13:11:58.297396Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"weights = [\n    0.004, 0.001, 0.002, 0.006, 0.004, 0.004, 0.002, 0.006, 0.006, 0.002, 0.002, 0.008,\n    0.006, 0.002, 0.008, 0.006, 0.002, 0.006, 0.004, 0.002, 0.004, 0.001, 0.006, 0.004,\n    0.002, 0.002, 0.004, 0.002, 0.004, 0.004, 0.001, 0.001, 0.002, 0.002, 0.006, 0.004,\n    0.004, 0.004, 0.006, 0.002, 0.002, 0.04 , 0.002, 0.002, 0.004, 0.04 , 0.002, 0.001,\n    0.006, 0.004, 0.004, 0.006, 0.001, 0.004, 0.004, 0.002, 0.006, 0.004, 0.006, 0.004,\n    0.006, 0.004, 0.002, 0.001, 0.002, 0.004, 0.002, 0.008, 0.004, 0.004, 0.002, 0.004,\n    0.006, 0.002, 0.004, 0.004, 0.002, 0.004, 0.004, 0.004, 0.001, 0.002, 0.002, 0.008,\n    0.02 , 0.004, 0.006, 0.002, 0.02 , 0.002, 0.002, 0.006, 0.004, 0.002, 0.001, 0.02,\n    0.006, 0.001, 0.002, 0.004, 0.001, 0.002, 0.006, 0.006, 0.004, 0.006, 0.001, 0.002,\n    0.004, 0.006, 0.006, 0.001, 0.04 , 0.006, 0.002, 0.004, 0.002, 0.002, 0.006, 0.002,\n    0.002, 0.004, 0.006, 0.006, 0.002, 0.002, 0.008, 0.006, 0.004, 0.002, 0.006, 0.002,\n    0.004, 0.006, 0.002, 0.004, 0.001, 0.004, 0.002, 0.004, 0.008, 0.006, 0.008, 0.002,\n    0.004, 0.002, 0.001, 0.004, 0.004, 0.004, 0.006, 0.008, 0.004, 0.001, 0.001, 0.002,\n    0.006, 0.004, 0.001, 0.002, 0.006, 0.004, 0.006, 0.008, 0.002, 0.002, 0.004, 0.002,\n    0.04 , 0.002, 0.002, 0.004, 0.002, 0.002, 0.006, 0.02 , 0.004, 0.002, 0.006, 0.02,\n    0.001, 0.002, 0.006, 0.004, 0.006, 0.004, 0.004, 0.004, 0.004, 0.002, 0.004, 0.04,\n    0.002, 0.008, 0.002, 0.004, 0.001, 0.004, 0.006, 0.004,\n]\n\nweights = {int(k):v for k,v in enumerate(weights)}","metadata":{"execution":{"iopub.status.busy":"2023-11-08T13:11:58.299960Z","iopub.execute_input":"2023-11-08T13:11:58.300391Z","iopub.status.idle":"2023-11-08T13:11:58.365707Z","shell.execute_reply.started":"2023-11-08T13:11:58.300358Z","shell.execute_reply":"2023-11-08T13:11:58.364348Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"if TRAIN:\n    if not os.path.exists(path_dataset_processed):\n        os.makedirs(path_dataset_processed)\n\n    if not os.path.exists(path_dataset_train) or OVERWRITE:\n        df_train_raw = pd.read_csv(path_dataset_train_raw)\n        \n    else:\n        df_train_raw = pd.read_csv(path_dataset_train)\n\n    if DEBUG:\n        df_train_raw = df_train_raw[df_train_raw[\"stock_id\"].isin([0, 1, 2])]\n        \n    drop_idx = df_train_raw.loc[df_train_raw[\"target\"].isna(), \"target\"].index.to_list()\n    df_train= df_train_raw.drop(drop_idx, axis=0)\n    df_train.reset_index(drop=True, inplace=True)\n    \n    #df_train_raw = df_train_raw.drop([\"time_id\",\"row_id\"], axis = 1)","metadata":{"execution":{"iopub.status.busy":"2023-11-08T13:11:58.367762Z","iopub.execute_input":"2023-11-08T13:11:58.368486Z","iopub.status.idle":"2023-11-08T13:12:21.681878Z","shell.execute_reply.started":"2023-11-08T13:11:58.368449Z","shell.execute_reply":"2023-11-08T13:12:21.680683Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if TRAIN:\n    global_stock_id_feats = {\n        \"median_size\": df_train.groupby(\"stock_id\")[\"bid_size\"].median() + df_train.groupby(\"stock_id\")[\"ask_size\"].median(),\n        \"std_size\": df_train.groupby(\"stock_id\")[\"bid_size\"].std() + df_train.groupby(\"stock_id\")[\"ask_size\"].std(),\n        \"ptp_size\": df_train.groupby(\"stock_id\")[\"bid_size\"].max() - df_train.groupby(\"stock_id\")[\"bid_size\"].min(),\n        \"median_price\": df_train.groupby(\"stock_id\")[\"bid_price\"].median() + df_train.groupby(\"stock_id\")[\"ask_price\"].median(),\n        \"std_price\": df_train.groupby(\"stock_id\")[\"bid_price\"].std() + df_train.groupby(\"stock_id\")[\"ask_price\"].std(),\n        \"ptp_price\": df_train.groupby(\"stock_id\")[\"bid_price\"].max() - df_train.groupby(\"stock_id\")[\"ask_price\"].min(),\n    }\n   \n\n    df_train_feats = feat_engineering(df_train)\n    print(\"Build Online Train Feats Finished.\")\n\n    df_train_feats = reduce_mem_usage(df_train_feats)","metadata":{"execution":{"iopub.status.busy":"2023-11-08T13:12:21.683338Z","iopub.execute_input":"2023-11-08T13:12:21.683878Z","iopub.status.idle":"2023-11-08T13:12:25.649914Z","shell.execute_reply.started":"2023-11-08T13:12:21.683847Z","shell.execute_reply":"2023-11-08T13:12:25.648546Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"Build Online Train Feats Finished.\n","output_type":"stream"}]},{"cell_type":"code","source":"def feat_engineering(df_train):\n    weights = [\n    0.004, 0.001, 0.002, 0.006, 0.004, 0.004, 0.002, 0.006, 0.006, 0.002, 0.002, 0.008,\n    0.006, 0.002, 0.008, 0.006, 0.002, 0.006, 0.004, 0.002, 0.004, 0.001, 0.006, 0.004,\n    0.002, 0.002, 0.004, 0.002, 0.004, 0.004, 0.001, 0.001, 0.002, 0.002, 0.006, 0.004,\n    0.004, 0.004, 0.006, 0.002, 0.002, 0.04 , 0.002, 0.002, 0.004, 0.04 , 0.002, 0.001,\n    0.006, 0.004, 0.004, 0.006, 0.001, 0.004, 0.004, 0.002, 0.006, 0.004, 0.006, 0.004,\n    0.006, 0.004, 0.002, 0.001, 0.002, 0.004, 0.002, 0.008, 0.004, 0.004, 0.002, 0.004,\n    0.006, 0.002, 0.004, 0.004, 0.002, 0.004, 0.004, 0.004, 0.001, 0.002, 0.002, 0.008,\n    0.02 , 0.004, 0.006, 0.002, 0.02 , 0.002, 0.002, 0.006, 0.004, 0.002, 0.001, 0.02,\n    0.006, 0.001, 0.002, 0.004, 0.001, 0.002, 0.006, 0.006, 0.004, 0.006, 0.001, 0.002,\n    0.004, 0.006, 0.006, 0.001, 0.04 , 0.006, 0.002, 0.004, 0.002, 0.002, 0.006, 0.002,\n    0.002, 0.004, 0.006, 0.006, 0.002, 0.002, 0.008, 0.006, 0.004, 0.002, 0.006, 0.002,\n    0.004, 0.006, 0.002, 0.004, 0.001, 0.004, 0.002, 0.004, 0.008, 0.006, 0.008, 0.002,\n    0.004, 0.002, 0.001, 0.004, 0.004, 0.004, 0.006, 0.008, 0.004, 0.001, 0.001, 0.002,\n    0.006, 0.004, 0.001, 0.002, 0.006, 0.004, 0.006, 0.008, 0.002, 0.002, 0.004, 0.002,\n    0.04 , 0.002, 0.002, 0.004, 0.002, 0.002, 0.006, 0.02 , 0.004, 0.002, 0.006, 0.02,\n    0.001, 0.002, 0.006, 0.004, 0.006, 0.004, 0.004, 0.004, 0.004, 0.002, 0.004, 0.04,\n    0.002, 0.008, 0.002, 0.004, 0.001, 0.004, 0.006, 0.004,\n    ]\n    df = df_train.copy()\n    del df_train\n    weights = {int(k):v for k,v in enumerate(weights)}\n    df[\"stock_weights\"] = df[\"stock_id\"].map(weights)\n\n    df[\"volume\"] = df.eval(\"ask_size + bid_size\")\n    df[\"liquidity_imbalance\"] = df.eval(\"(bid_size-ask_size)/(bid_size+ask_size)\")\n    df[\"matched_imbalance\"] = df.eval(\"(imbalance_size-matched_size)/(matched_size+imbalance_size)\")\n    df[\"size_imbalance\"] = df.eval(\"bid_size / ask_size\")\n    \n    prices = [\"reference_price\", \"far_price\", \"near_price\", \"ask_price\", \"bid_price\", \"wap\"]\n    sizes = [\"matched_size\", \"bid_size\", \"ask_size\", \"imbalance_size\"]\n    \n    for c in combinations(prices, 2):\n        df[f\"{c[0]}_{c[1]}_imb\"] = df.eval(f\"({c[0]} - {c[1]})/({c[0]} + {c[1]})\")\n\n    for c in [['ask_price', 'bid_price', 'wap', 'reference_price'], sizes]:\n        triplet_feature = calculate_triplet_imbalance_numba(c, df)\n        df[triplet_feature.columns] = triplet_feature.values\n        \n\n    df[\"weighted_wap\"] = df[\"stock_weights\"] * df[\"wap\"]\n    df['wap_momentum'] = df.groupby('stock_id')['weighted_wap'].pct_change(periods=6)\n    df[\"imbalance_momentum\"] = df.groupby(['stock_id'])['imbalance_size'].diff(periods=1) / df['matched_size']\n    df[\"price_spread\"] = df[\"ask_price\"] - df[\"bid_price\"]\n    df[\"spread_intensity\"] = df.groupby(['stock_id'])['price_spread'].diff()\n    df['market_urgency'] = df['price_spread'] * df['liquidity_imbalance']\n    df['depth_pressure'] = (df['ask_size'] - df['bid_size']) * (df['far_price'] - df['near_price'])\n    df['spread_depth_ratio'] = (df['ask_price'] - df['bid_price']) / (df['bid_size'] + df['ask_size'])\n\n\n    \n    for func in [\"mean\", \"std\", \"skew\"]:\n        df[f\"all_prices_{func}\"] = df[prices].agg(func, axis=1)\n        df[f\"all_sizes_{func}\"] = df[sizes].agg(func, axis=1)\n        \n    df = df.replace([np.inf, -np.inf], 0)\n    \n\n    df = pl.DataFrame(df)\n    df = df.sort([\"stock_id\", \"date_id\", \"seconds_in_bucket\"])\n\n    df = df.with_columns(\n        (pl.col(\"seconds_in_bucket\") / 10).cast(pl.Int32).alias(\"seconds_in_bucket\")\n    )\n\n    list_cols = list(df.columns)\n    list_cols_ma = [col for col in list_cols if col not in ['stock_id', 'date_id', 'seconds_in_bucket', \n       'imbalance_buy_sell_flag', 'reference_price', 'matched_size',\n       'far_price', 'near_price', 'bid_price', 'bid_size', 'ask_price',\n       'ask_size', 'wap', 'target', 'time_id', 'row_id','stock_weights','near_price_ask_price_imb'] ]\n    \n    \n    list_cols = ['all_prices_mean',\n         'all_prices_mean_mean_15',\n         'all_prices_mean_mean_3',\n         'all_prices_mean_mean_8',\n         'all_prices_mean_std_8',\n         'all_prices_skew_mean_10',\n         'all_prices_skew_mean_15',\n         'all_prices_skew_std_10',\n         'all_prices_skew_std_8',\n         'all_prices_std_std_8',\n         'all_sizes_mean_mean_15',\n         'all_sizes_mean_std_10',\n         'all_sizes_mean_std_15',\n         'all_sizes_std',\n         'all_sizes_std_std_15',\n         'ask_price_bid_price_imb_std_10',\n         'ask_price_bid_price_imb_std_15',\n         'ask_price_bid_price_reference_price_imb2',\n         'ask_price_bid_price_reference_price_imb2_std_10',\n         'ask_price_bid_price_reference_price_imb2_std_15',\n         'ask_price_bid_price_reference_price_imb2_std_8',\n         'ask_price_bid_price_wap_imb2',\n         'ask_price_wap_imb',\n         'ask_price_wap_imb_mean_8',\n         'ask_price_wap_imb_std_10',\n         'ask_price_wap_imb_std_8',\n         'ask_price_wap_reference_price_imb2',\n         'ask_price_wap_reference_price_imb2_mean_15',\n         'bid_price_wap_imb',\n         'bid_price_wap_imb_std_10',\n         'bid_price_wap_imb_std_8',\n         'bid_price_wap_reference_price_imb2',\n         'bid_price_wap_reference_price_imb2_std_15',\n         'bid_price_wap_reference_price_imb2_std_8',\n         'bid_size_ask_size_imbalance_size_imb2_mean_8',\n         'far_price_near_price_imb_mean_3',\n         'far_price_near_price_imb_std_8',\n         'imbalance_momentum_mean_10',\n         'imbalance_momentum_mean_15',\n         'imbalance_momentum_mean_8',\n         'imbalance_momentum_std_15',\n         'imbalance_size',\n         'imbalance_size_mean_15',\n         'imbalance_size_std_10',\n         'imbalance_size_std_8',\n         'market_urgency',\n         'market_urgency_mean_8',\n         'matched_size_ask_size_imbalance_size_imb2_std_15',\n         'matched_size_bid_size_ask_size_imb2',\n         'matched_size_bid_size_ask_size_imb2_std_10',\n         'matched_size_bid_size_imbalance_size_imb2_std_8',\n         'price_spread_std_15',\n         'reference_price_ask_price_imb',\n         'reference_price_ask_price_imb_mean_10',\n         'reference_price_ask_price_imb_mean_15',\n         'reference_price_ask_price_imb_std_15',\n         'reference_price_bid_price_imb',\n         'reference_price_bid_price_imb_mean_15',\n         'reference_price_bid_price_imb_std_8',\n         'reference_price_wap_imb',\n         'reference_price_wap_imb_mean_10',\n         'reference_price_wap_imb_mean_15',\n         'reference_price_wap_imb_mean_8',\n         'reference_price_wap_imb_std_8',\n         'seconds_in_bucket',\n         'size_imbalance',\n         'spread_depth_ratio',\n         'spread_depth_ratio_mean_15',\n         'spread_depth_ratio_std_10',\n         'spread_depth_ratio_std_15',\n         'spread_depth_ratio_std_8',\n         'spread_intensity_std_10',\n         'spread_intensity_std_15',\n         'volume_mean_15',\n         'wap_momentum',\n         'wap_momentum_mean_10',\n         'wap_momentum_mean_15',\n         'wap_momentum_mean_3',\n         'wap_momentum_mean_6',\n         'wap_momentum_mean_8',\n         'weighted_wap',\n         'weighted_wap_mean_15',\n         'weighted_wap_mean_3',\n         'weighted_wap_mean_8',\n         'weighted_wap_std_15']\n\n    def rolling_polars(df, list_cols, col_group_by, index_column):\n        for col in list_cols:\n            if TRAIN:\n                print(col)\n            function = col.split(\"_\")[-2]\n            if function in [\"mean\",\"std\"]:\n                base_col = \"_\".join(col.split(\"_\")[:-2])\n\n                window = col.split(\"_\")[-1]\n\n\n                rolling_group = df.group_by_rolling(\n                        index_column=index_column,\n                        period=f\"{window}i\",  # 'i' denotes index count (integer)\n                        by=col_group_by,\n                        closed=\"left\",  # Adjust as needed\n                    )\n                if function == \"mean\":\n                    df = df.join(\n                        rolling_group.agg(pl.col(base_col).mean().alias(f\"{base_col}_mean_{window}\")),\n                        on=[\"stock_id\", \"date_id\", \"seconds_in_bucket\"],\n                        how=\"left\",\n                    )\n                elif function == \"std\":\n                    df = df.join(\n                        rolling_group.agg(pl.col(base_col).std().alias(f\"{base_col}_std_{window}\")),\n                        on=[\"stock_id\", \"date_id\", \"seconds_in_bucket\"],\n                        how=\"left\",\n                    )\n        return df\n        \n    df = rolling_polars(df, list_cols, [\"stock_id\", \"date_id\"], \"seconds_in_bucket\")\n   \n\n\n    df = df.to_pandas()\n    \n    if TRAIN:\n        return df[list_cols + [\"date_id\",\"stock_id\",\"target\"]]\n    else:\n        return df[list_cols + [\"date_id\",\"stock_id\"]]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def aggregate_feature_importance(list_files_feat_importance):\n    list_of_dfs = []\n    for file_path in list_files_feat_importance:\n        feature_importance_df = pd.read_csv(file_path)\n\n        folds = [col for col in feature_importance_df.columns if \"imp_fold\" in col]\n\n        # Normalize by dividing each score by the sum of scores within its respective fold\n        for fold in folds:\n            fold_sum = feature_importance_df[fold].sum()\n            feature_importance_df[fold] = feature_importance_df[fold] / fold_sum\n\n        list_of_dfs.append(feature_importance_df)\n\n    aggregated_df = pd.concat(list_of_dfs, ignore_index=True)\n\n    df_median_importance = aggregated_df.groupby(\"feat\").median().reset_index()\n\n    df_median_importance[\"feat_imp_overall_mean\"] = df_median_importance.loc[\n        :, df_median_importance.columns != \"feat\"\n    ].median(axis=1, skipna=True)\n    cols = [\"feat\", \"feat_imp_overall_mean\"] + [\n        col\n        for col in df_median_importance.columns\n        if col not in [\"feat_imp_overall_mean\", \"feat\"]\n    ]\n    df_median_importance = df_median_importance[cols]\n\n    df_median_importance.sort_values(\n        \"feat_imp_overall_mean\", ascending=False, inplace=True\n    )\n    return df_median_importance","metadata":{"execution":{"iopub.status.busy":"2023-11-08T13:12:25.651592Z","iopub.execute_input":"2023-11-08T13:12:25.651977Z","iopub.status.idle":"2023-11-08T13:12:25.709740Z","shell.execute_reply.started":"2023-11-08T13:12:25.651945Z","shell.execute_reply":"2023-11-08T13:12:25.708509Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"def time_series_split(X, n_splits, n_test_splits, embargo_td=2):\n    factorized_indices = np.unique(X[\"factorized\"])\n\n    # Compute the fold boundaries\n    fold_bounds = [\n        (fold[0], fold[-1] + 1) for fold in np.array_split(factorized_indices, n_splits)\n    ]\n\n    # Create the list of all tests test_fold_bounds that will become the test sets\n    selected_fold_bounds = list(itt.combinations(fold_bounds, n_test_splits))\n\n    # Reverse to start the testing from the most recent part of the dataset\n    selected_fold_bounds.reverse()\n\n    for fold_bound_list in selected_fold_bounds:\n        test_factorized_indices = np.empty(0)\n        test_fold_bounds = []\n\n        for fold_start, fold_end in fold_bound_list:\n            # Records the boundaries of the current test split\n            if not test_fold_bounds or fold_start != test_fold_bounds[-1][-1]:\n                test_fold_bounds.append((fold_start, fold_end))\n            elif fold_start == test_fold_bounds[-1][-1]:\n                test_fold_bounds[-1] = (test_fold_bounds[-1][0], fold_end)\n\n            test_factorized_indices = np.union1d(\n                test_factorized_indices, factorized_indices[fold_start:fold_end]\n            ).astype(int)\n\n        # Compute the train set indices\n        train_indices = np.setdiff1d(factorized_indices, test_factorized_indices)\n\n        # Purge and embargo can be added here if needed\n        # ...\n\n        yield train_indices, test_factorized_indices\n\n\n# # Example usage:\n","metadata":{"execution":{"iopub.status.busy":"2023-11-08T13:12:25.713686Z","iopub.execute_input":"2023-11-08T13:12:25.714152Z","iopub.status.idle":"2023-11-08T13:12:25.974074Z","shell.execute_reply.started":"2023-11-08T13:12:25.714114Z","shell.execute_reply":"2023-11-08T13:12:25.973096Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"if TRAIN:\n    col_split = \"date_id\"\n    df_train.sort_values([col_split], inplace=True)\n    df_train.reset_index(drop=True, inplace=True)\n    df_train[\"factorized\"] = pd.factorize(df_train[col_split])[0]\n    \n    list_cols_drop = [\"date_id\"]\n    df_train.drop(list_cols_drop, axis=1, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2023-11-08T13:12:25.975436Z","iopub.execute_input":"2023-11-08T13:12:25.976435Z","iopub.status.idle":"2023-11-08T13:12:26.057874Z","shell.execute_reply.started":"2023-11-08T13:12:25.976396Z","shell.execute_reply":"2023-11-08T13:12:26.055962Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if DEBUG:\n    n_estimators_min =n_estimators_max= 50\nelse:\n    n_estimators_min = 500\n    n_estimators_max = 500\n    ","metadata":{"execution":{"iopub.status.busy":"2023-11-08T13:12:54.408684Z","iopub.execute_input":"2023-11-08T13:12:54.409148Z","iopub.status.idle":"2023-11-08T13:12:54.464928Z","shell.execute_reply.started":"2023-11-08T13:12:54.409113Z","shell.execute_reply":"2023-11-08T13:12:54.463449Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"gpu_switch = \"OFF\"\nn_splits = 5\nn_test_split = 1\nembargo_td = 100\n\nn_repeats = 1\nnbrnd_erly_stp = 130\n\ncv_mthd = \"KF\"\n\n# Cross-Validation Setup\nif TRAIN:\n    all_cv = {\"KF\": KFold(n_splits=n_splits, shuffle=True, random_state=state)}\n    cv = all_cv[cv_mthd]\n\n    model_params_dict = {\n        \"LGBMR\": {\n            \"static_params\": {\n                \"device\": \"gpu\" if gpu_switch == \"ON\" else \"cpu\",\n                \"objective\": \"mae\",\n                \"boosting_type\": \"gbdt\",\n                \"random_state\": state,\n                \"n_jobs\" : 4,\n                \"verbose\": -1,\n                \"importance_type\" : \"gain\",\n            },\n            \"dynamic_params\": {\n                \"n_estimators\": {\n                    \"type\": \"int\",\n                    \"low\": n_estimators_min,\n                    \"high\": n_estimators_max,\n                },\n                \"learning_rate\": {\n                    \"type\": \"float\",\n                    \"low\": 0.005,\n                    \"high\": 0.06,\n                },\n                \"max_depth\": {\"type\": \"int\", \"low\": 10, \"high\": 90},\n                \"num_leaves\": {\n                    \"type\": \"int\",\n                    \"low\": 20,\n                    \"high\": 90,\n                },\n                \"min_child_samples\": {\n                    \"type\": \"int\",\n                    \"low\": 10,\n                    \"high\": 70,\n                },\n                \"subsample\": {\n                    \"type\": \"float\",\n                    \"low\": 0.7,\n                    \"high\": 1,\n                },\n                \"colsample_bytree\": {\n                    \"type\": \"float\",\n                    \"low\": 1,\n                    \"high\": 1,\n                },\n                \"min_split_gain\": {\n                    \"type\": \"float\",\n                    \"low\": 0,\n                    \"high\": 2,\n                },\n                \"reg_alpha\": {\n                    \"type\": \"float\",\n                    \"low\": 0,\n                    \"high\": 3,\n                },\n                \"reg_lambda\": {\n                    \"type\": \"float\",\n                    \"low\": 0,\n                    \"high\": 3,\n                },\n            },\n        },\n    }\n\n    dict_models = {\"LGBMR\": LGBMR}\n\n    log_model = True\n\n    experiment_date_str = datetime.now().strftime(\"%Y%m%d_%H_%M_%S\")\n    experiment_purpose = \"optiver_trading_at_the_close\"\n    experiment_name = f\"{experiment_purpose}_{experiment_date_str}\"\n\n    mlflow.set_experiment(experiment_name)\n","metadata":{"execution":{"iopub.status.busy":"2023-11-08T13:12:55.778297Z","iopub.execute_input":"2023-11-08T13:12:55.778721Z","iopub.status.idle":"2023-11-08T13:12:55.884843Z","shell.execute_reply.started":"2023-11-08T13:12:55.778690Z","shell.execute_reply":"2023-11-08T13:12:55.883471Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stderr","text":"2023/11/08 13:12:55 INFO mlflow.tracking.fluent: Experiment with name 'optiver_trading_at_the_close_20231108_13_12_55' does not exist. Creating a new experiment.\n","output_type":"stream"}]},{"cell_type":"code","source":"# Run the Optuna study\nif TRAIN:\n    study = optuna.create_study(\n        direction=\"minimize\",\n        study_name=\"Your Study Name\",\n        load_if_exists=True,\n    )\n    study.optimize(lambda trial: objective(trial, df_train), n_trials=N_TRIALS)","metadata":{"execution":{"iopub.status.busy":"2023-11-07T22:46:11.815443Z","iopub.execute_input":"2023-11-07T22:46:11.816075Z","iopub.status.idle":"2023-11-07T23:23:53.159817Z","shell.execute_reply.started":"2023-11-07T22:46:11.816020Z","shell.execute_reply":"2023-11-07T23:23:53.158564Z"},"trusted":true},"execution_count":59,"outputs":[{"name":"stderr","text":"\u001b[32m2023-11-07 22:46:11.900\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mutils\u001b[0m:\u001b[36mlog_training_details\u001b[0m:\u001b[36m296\u001b[0m - \u001b[1m\u001b[34mTraining model: LGBMR\u001b[0m\u001b[0m\n\u001b[32m2023-11-07 22:46:11.902\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mutils\u001b[0m:\u001b[36mlog_training_details\u001b[0m:\u001b[36m300\u001b[0m - \u001b[1m\u001b[32mTrial 0    | n_estimators: 500 | learning_rate: 0.035956902678852834 | max_depth: 80 | num_leaves: 66 | min_child_samples: 44 | subsample: 0.8854528798976345 | colsample_bytree: 1.0 | min_split_gain: 0.9057767926566502 | reg_alpha: 2.5767046116590095 | reg_lambda: 1.2408695092715232\u001b[0m\u001b[0m\n\u001b[32m2023-11-07 22:46:11.903\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mutils\u001b[0m:\u001b[36mlog_training_details\u001b[0m:\u001b[36m308\u001b[0m - \u001b[1mFold  |  MAE                 \u001b[0m\n\u001b[32m2023-11-07 22:46:11.905\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mutils\u001b[0m:\u001b[36mlog_training_details\u001b[0m:\u001b[36m309\u001b[0m - \u001b[1m----- |  --------------------\u001b[0m\n","output_type":"stream"},{"name":"stdout","text":"trial: 0\nTraining until validation scores don't improve for 100 rounds\n[100]\tvalid_0's l1: 5.95833\n[200]\tvalid_0's l1: 5.95057\n[300]\tvalid_0's l1: 5.94813\n[400]\tvalid_0's l1: 5.94691\n","output_type":"stream"},{"name":"stderr","text":"\u001b[32m2023-11-07 22:57:03.325\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mobjective\u001b[0m:\u001b[36m64\u001b[0m - \u001b[1m1     |  5.946612072702436   \u001b[0m\n\u001b[32m2023-11-07 22:57:03.337\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mobjective\u001b[0m:\u001b[36m97\u001b[0m - \u001b[31m\u001b[1mAn exception occurred: 'str' object has no attribute 'items'\u001b[0m\n","output_type":"stream"},{"name":"stdout","text":"[500]\tvalid_0's l1: 5.94666\nDid not meet early stopping. Best iteration is:\n[487]\tvalid_0's l1: 5.94661\ndefaultdict(<class 'collections.OrderedDict'>, {'valid_0': OrderedDict([('l1', 5.946612072702436)])})\n","output_type":"stream"},{"name":"stderr","text":"\u001b[32m2023-11-07 22:57:03.415\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mutils\u001b[0m:\u001b[36mlog_training_details\u001b[0m:\u001b[36m296\u001b[0m - \u001b[1m\u001b[34mTraining model: LGBMR\u001b[0m\u001b[0m\n\u001b[32m2023-11-07 22:57:03.417\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mutils\u001b[0m:\u001b[36mlog_training_details\u001b[0m:\u001b[36m300\u001b[0m - \u001b[1m\u001b[32mTrial 1    | n_estimators: 500 | learning_rate: 0.03301430139806651 | max_depth: 80 | num_leaves: 87 | min_child_samples: 16 | subsample: 0.7651522822153488 | colsample_bytree: 1.0 | min_split_gain: 0.3401941575339298 | reg_alpha: 0.8288560143090971 | reg_lambda: 2.648713148921358\u001b[0m\u001b[0m\n\u001b[32m2023-11-07 22:57:03.418\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mutils\u001b[0m:\u001b[36mlog_training_details\u001b[0m:\u001b[36m308\u001b[0m - \u001b[1mFold  |  MAE                 \u001b[0m\n\u001b[32m2023-11-07 22:57:03.421\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mutils\u001b[0m:\u001b[36mlog_training_details\u001b[0m:\u001b[36m309\u001b[0m - \u001b[1m----- |  --------------------\u001b[0m\n","output_type":"stream"},{"name":"stdout","text":"trial: 1\nTraining until validation scores don't improve for 100 rounds\n[100]\tvalid_0's l1: 5.95756\n[200]\tvalid_0's l1: 5.94924\n[300]\tvalid_0's l1: 5.9476\n[400]\tvalid_0's l1: 5.94655\n[500]\tvalid_0's l1: 5.94614\nDid not meet early stopping. Best iteration is:\n[485]\tvalid_0's l1: 5.94611\n","output_type":"stream"},{"name":"stderr","text":"\u001b[32m2023-11-07 23:08:21.908\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mobjective\u001b[0m:\u001b[36m64\u001b[0m - \u001b[1m1     |  5.946109049767642   \u001b[0m\n\u001b[32m2023-11-07 23:08:21.920\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mobjective\u001b[0m:\u001b[36m97\u001b[0m - \u001b[31m\u001b[1mAn exception occurred: 'str' object has no attribute 'items'\u001b[0m\n\u001b[32m2023-11-07 23:08:22.027\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mutils\u001b[0m:\u001b[36mlog_training_details\u001b[0m:\u001b[36m296\u001b[0m - \u001b[1m\u001b[34mTraining model: LGBMR\u001b[0m\u001b[0m\n\u001b[32m2023-11-07 23:08:22.029\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mutils\u001b[0m:\u001b[36mlog_training_details\u001b[0m:\u001b[36m300\u001b[0m - \u001b[1m\u001b[32mTrial 2    | n_estimators: 500 | learning_rate: 0.01060853529220129 | max_depth: 13 | num_leaves: 67 | min_child_samples: 60 | subsample: 0.9096677666110287 | colsample_bytree: 1.0 | min_split_gain: 1.1108420249969302 | reg_alpha: 2.4998917587575056 | reg_lambda: 1.2143623822761067\u001b[0m\u001b[0m\n\u001b[32m2023-11-07 23:08:22.031\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mutils\u001b[0m:\u001b[36mlog_training_details\u001b[0m:\u001b[36m308\u001b[0m - \u001b[1mFold  |  MAE                 \u001b[0m\n\u001b[32m2023-11-07 23:08:22.032\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mutils\u001b[0m:\u001b[36mlog_training_details\u001b[0m:\u001b[36m309\u001b[0m - \u001b[1m----- |  --------------------\u001b[0m\n","output_type":"stream"},{"name":"stdout","text":"defaultdict(<class 'collections.OrderedDict'>, {'valid_0': OrderedDict([('l1', 5.946109049767642)])})\ntrial: 2\nTraining until validation scores don't improve for 100 rounds\n[100]\tvalid_0's l1: 5.98261\n[200]\tvalid_0's l1: 5.9672\n[300]\tvalid_0's l1: 5.95974\n[400]\tvalid_0's l1: 5.9558\n","output_type":"stream"},{"name":"stderr","text":"\u001b[32m2023-11-07 23:23:53.083\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mobjective\u001b[0m:\u001b[36m64\u001b[0m - \u001b[1m1     |  5.953109348027679   \u001b[0m\n\u001b[32m2023-11-07 23:23:53.094\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mobjective\u001b[0m:\u001b[36m97\u001b[0m - \u001b[31m\u001b[1mAn exception occurred: 'str' object has no attribute 'items'\u001b[0m\n","output_type":"stream"},{"name":"stdout","text":"[500]\tvalid_0's l1: 5.95311\nDid not meet early stopping. Best iteration is:\n[500]\tvalid_0's l1: 5.95311\ndefaultdict(<class 'collections.OrderedDict'>, {'valid_0': OrderedDict([('l1', 5.953109348027679)])})\n","output_type":"stream"}]},{"cell_type":"code","source":"args = {\n    'trial': trial,\n    'df_train': df_train,\n    'cv_mthd': cv_mthd,\n    'experiment_purpose': experiment_purpose,\n    'experiment_name': experiment_name,\n    'VERSION_NB': VERSION_NB,\n    'dict_models': dict_models,\n    'model_params_dict': model_params_dict,\n    'logger': logger,\n    'time_series_split': time_series_split,\n    'n_splits': n_splits,\n    'n_test_split': n_test_split,\n    'create_model': create_model,\n    'log_training_details': log_training_details,\n    'log_feature_importance': log_feature_importance,\n    'experiment_date_str': experiment_date_str,\n    'path_experiments_storage': path_experiments_storage,\n    'target_col': 'target'\n}\n","metadata":{"execution":{"iopub.status.busy":"2023-11-08T13:13:24.513099Z","iopub.execute_input":"2023-11-08T13:13:24.513567Z","iopub.status.idle":"2023-11-08T13:13:24.820508Z","shell.execute_reply.started":"2023-11-08T13:13:24.513531Z","shell.execute_reply":"2023-11-08T13:13:24.819027Z"},"trusted":true},"execution_count":17,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[17], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m args \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m----> 2\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrial\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[43mtrial\u001b[49m,\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdf_train\u001b[39m\u001b[38;5;124m'\u001b[39m: df_train,\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcv_mthd\u001b[39m\u001b[38;5;124m'\u001b[39m: cv_mthd,\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexperiment_purpose\u001b[39m\u001b[38;5;124m'\u001b[39m: experiment_purpose,\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexperiment_name\u001b[39m\u001b[38;5;124m'\u001b[39m: experiment_name,\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mVERSION_NB\u001b[39m\u001b[38;5;124m'\u001b[39m: VERSION_NB,\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdict_models\u001b[39m\u001b[38;5;124m'\u001b[39m: dict_models,\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_params_dict\u001b[39m\u001b[38;5;124m'\u001b[39m: model_params_dict,\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlogger\u001b[39m\u001b[38;5;124m'\u001b[39m: logger,\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtime_series_split\u001b[39m\u001b[38;5;124m'\u001b[39m: time_series_split,\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_splits\u001b[39m\u001b[38;5;124m'\u001b[39m: n_splits,\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_test_split\u001b[39m\u001b[38;5;124m'\u001b[39m: n_test_split,\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcreate_model\u001b[39m\u001b[38;5;124m'\u001b[39m: create_model,\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlog_training_details\u001b[39m\u001b[38;5;124m'\u001b[39m: log_training_details,\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlog_feature_importance\u001b[39m\u001b[38;5;124m'\u001b[39m: log_feature_importance,\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexperiment_date_str\u001b[39m\u001b[38;5;124m'\u001b[39m: experiment_date_str,\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpath_experiments_storage\u001b[39m\u001b[38;5;124m'\u001b[39m: path_experiments_storage,\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtarget_col\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtarget\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     20\u001b[0m }\n","\u001b[0;31mNameError\u001b[0m: name 'trial' is not defined"],"ename":"NameError","evalue":"name 'trial' is not defined","output_type":"error"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def run_mlflow_experiment(args):\n    trial = args['trial']\n    df_train = args['df_train']\n    cv_mthd = args['cv_mthd']\n    experiment_purpose = args['experiment_purpose']\n    experiment_name = args['experiment_name']\n    VERSION_NB = args['VERSION_NB']\n    dict_models = args['dict_models']\n    model_params_dict = args['model_params_dict']\n    logger = args['logger']\n    time_series_split = args['time_series_split']\n    n_splits = args['n_splits']\n    n_test_split = args['n_test_split']\n    create_model = args['create_model']\n    log_training_details = args['log_training_details']\n    log_feature_importance = args['log_feature_importance']\n    experiment_date_str = args['experiment_date_str']\n    path_experiments_storage = args['path_experiments_storage']\n    target_col = args['target_col']\n    \n    \n    with mlflow.start_run() as run:\n        mlflow.log_param(\"cv_mthd\", cv_mthd)\n        mlflow.set_tag(\"experiment_purpose\", experiment_purpose)\n        mlflow.set_tag(\"experiment_name\", experiment_name)\n        mlflow.set_tag(\"version_nb\", VERSION_NB)\n        mlflow.set_tag(\"trial\",trial)\n        \n        score_list = []\n        for model_name, model_class in dict_models.items():\n            model = create_model(\n                trial,\n                model_class,\n                model_params_dict[model_name][\"static_params\"],\n                model_params_dict[model_name][\"dynamic_params\"],\n            )\n\n            log_training_details(logger, model, trial, model_name)\n\n            for fold_n, (train_indices, test_indices) in enumerate(time_series_split(df_train, n_splits=n_splits, n_test_splits=n_test_split)):\n                with mlflow.start_run(run_name=f\"fold_{fold_n+1}\", nested=True) as nested_run:\n                    mask_train = df_train[\"factorized\"].isin(train_indices)\n                    mask_test = df_train[\"factorized\"].isin(test_indices)\n\n                    y_train = df_train.loc[mask_train, target_col]\n                    y_val = df_train.loc[mask_test, target_col]\n                    X_train = df_train.loc[mask_train].drop([target_col, \"factorized\"], axis=1)\n                    X_val = df_train.loc[mask_test].drop([target_col, \"factorized\"], axis=1)\n\n                    mlflow.log_param(\"train_rows\", X_train.shape[0])\n                    mlflow.log_param(\"train_cols\", X_train.shape[1])\n\n                    model.fit(\n                        X_train,\n                        y_train,\n                        eval_set=[(X_val, y_val)],\n                        eval_metric=\"mae\",\n                        callbacks=[\n                            lgbm.callback.early_stopping(stopping_rounds=100),\n                            lgbm.callback.log_evaluation(period=100),\n                        ],\n                    )\n\n                    log_feature_importance(\n                        trial.number,\n                        model,\n                        X_train,\n                        fold_n,\n                        experiment_purpose,\n                        experiment_date_str,\n                    )\n\n                    fold_mae = model.best_score_[\"valid_0\"][\"l1\"]\n                    print(model.best_score_)\n                    mae_list.append(fold_mae)\n                    logger.info(f\"Fold {fold_n + 1} | MAE: {fold_mae}\")\n\n                    mlflow.log_param(\"fold_mae\", fold_mae)\n                    mlflow.log_param(\"fold_number\", fold_n + 1)\n                    mlflow.log_param(\"model_name\", model_name)\n\n                    params_to_log = model.get_params()\n\n                    current_time_str = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n                    model_log_name = f\"{model_name}_{trial.number}_{current_time_str}\"\n\n                    mlflow.log_param(\"model_log_name\", model_log_name)\n                    mlflow.sklearn.log_model(model, model_log_name)\n\n                    mlflow.log_param(\"run_time\", current_time_str)\n\n                    nested_run_id = nested_run.info.run_id\n                    model_path = f\"{path_experiments_storage}/{run.info.experiment_id}/{nested_run_id}/artifacts/{model_log_name}/model.pkl\"\n                    mlflow.log_param(\"model_path\", model_path)\n\n        avg_mae = sum(mae_list) / len(mae_list)\n        mlflow.log_metric(\"avg_mae\", avg_mae)\n\n        return avg_mae","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def run_mlflow_experiment(trial, df_train, dict_models, model_params_dict, n_splits, n_test_split, logger, VERSION_NB, experiment_purpose, experiment_name, path_experiments_storage, time_series_split, create_model, log_training_details, log_feature_importance):\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n\n\ndef objective(trial, df_train):\n    try:\n        print(f\"trial: {trial.number}\")\n        with mlflow.start_run() as run:\n            mlflow.log_param(\"cv_mthd\", cv_mthd)\n            mlflow.set_tag(\"experiment_purpose\", experiment_purpose)\n            mlflow.set_tag(\"experiment_name\", experiment_name)\n            mlflow.set_tag(\"version_nb\", VERSION_NB)\n            for model_name, model_class in dict_models.items():\n                model = create_model(\n                    trial,\n                    dict_models[model_name],\n                    model_params_dict[model_name][\"static_params\"],\n                    model_params_dict[model_name][\"dynamic_params\"],\n                )\n                mae_list = []\n\n                log_training_details(logger, model, trial, model_name)\n\n                for fold_n, (train_indices, test_indices) in enumerate(time_series_split(df_train, n_splits = n_splits, n_test_splits = n_test_split)\n                ):\n                  \n                    with mlflow.start_run(\n                        run_name=f\"fold_{fold_n+1}\", nested=True\n                    ) as nested_run:\n                        mask_train = df_train[\"factorized\"].isin(list(train_indices))\n                        mask_test = df_train[\"factorized\"].isin(list(test_indices))\n\n                        # Filter based on the 'factorized' field\n                        y_train = df_train.loc[mask_train, \"target\"].squeeze()\n                        y_val = df_train.loc[mask_test, \"target\"].squeeze()\n                        X_train = df_train[mask_train].drop([\"target\",\"factorized\"], axis=1)\n                        X_val = df_train[mask_test].drop([\"target\",\"factorized\"], axis=1)\n\n                        mlflow.log_param(\"training_data_rows\", X_train.shape[0])\n                        mlflow.log_param(\"training_data_columns\", X_train.shape[1])\n                        \n                        \n\n                        model.fit(\n                            X_train,\n                            y_train,\n                            eval_set=[(X_val, y_val)],\n                            eval_metric=\"mae\",\n                            callbacks=[\n                                lgbm.callback.early_stopping(stopping_rounds=100),\n                                lgbm.callback.log_evaluation(period=100),\n                            ],\n                            \n                        )\n\n                        log_feature_importance(\n                            trial.number,\n                            model,\n                            X_train,\n                            fold_n,\n                            experiment_purpose,\n                            experiment_date_str,\n                        )\n\n                        fold_mae = model.best_score_[\"valid_0\"][\"l1\"]\n                        print(model.best_score_)\n                        mae_list.append(fold_mae)\n                        logger.info(f\"{fold_n + 1:<5} {'|':<2} {fold_mae:<20}\")\n                        \n                        mlflow.log_param(\"fold_mae\", fold_mae)\n\n                      \n                        mlflow.log_param(\"fold_number\", fold_n + 1)\n                        mlflow.log_param(\"model_name\", model_name)\n                \n\n                        params_to_log = model.get_params()\n                 \n                      \n                        current_time_str = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n                        model_log_name = (\n                            f\"{model_name}_{trial.number}_{current_time_str}\"\n                        )\n\n                        mlflow.log_param(\"model_log_name\", model_log_name)\n\n                        mlflow.sklearn.log_model(model, model_log_name)\n\n                        mlflow.log_param(\"run_time\", current_time_str)\n\n                        nested_run_id = nested_run.info.run_id\n                        model_path = f\"{path_experiments_storage}/{run.info.experiment_id}/{nested_run_id}/artifacts/{model_log_name}/model.pkl\"\n                        mlflow.log_param(\"model_path\", model_path)\n                avg_mae = sum(mae_list) / len(mae_list)\n\n                mlflow.log_param(\"model_name\", model_name)\n   \n                return avg_mae\n\n    except Exception as e:\n        logger.error(f\"An exception occurred: {e}\")\n        return float(\"inf\")","metadata":{"_cell_guid":"334e81a6-b09b-4a3d-96a1-fe0278deff13","_uuid":"4288a49c-b472-4af2-b2bb-95cf0cd2bd6d","collapsed":false,"jupyter":{"outputs_hidden":false},"papermill":{"duration":0.038424,"end_time":"2023-10-29T17:16:44.947628","exception":false,"start_time":"2023-10-29T17:16:44.909204","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-11-07T23:23:53.162097Z","iopub.execute_input":"2023-11-07T23:23:53.162475Z","iopub.status.idle":"2023-11-07T23:23:53.246139Z","shell.execute_reply.started":"2023-11-07T23:23:53.162443Z","shell.execute_reply":"2023-11-07T23:23:53.244684Z"},"trusted":true},"execution_count":60,"outputs":[]},{"cell_type":"code","source":"def experiments_data(list_experiment_id = None, save_df = None, list_columns = None):\n    \"\"\"\n    Ogni volta che viene chiamata questa funzione legge tutti gli esperimenti e ritorna una nuova versione del file con tutti gli esperimenti storicizzati\n    \"\"\"\n    experiments = client.search_experiments()\n    all_runs_data = []\n    for exp in experiments:\n        experiment_id = exp.experiment_id\n        if (list_experiment_id == None) or (experiment_id in list_experiment_id):\n        \n            run_infos = client.search_runs(experiment_ids=[experiment_id])\n\n            for run_info in run_infos:\n                run_data = {\n                    \"experiment_id\": experiment_id,\n                    \"experiment_name\": exp.name,\n                    \"run_id\": run_info.info.run_id,\n                }\n\n                # Add metrics to run_data\n                for key, value in run_info.data.metrics.items():\n                    run_data[f\"{key}\"] = value\n\n                # Add params to run_data\n                for key, value in run_info.data.params.items():\n                    run_data[f\"{key}\"] = value\n\n                all_runs_data.append(run_data)\n        \n    df_runs_new = pd.DataFrame(all_runs_data)\n    \n\n    \n    df_runs_new = df_runs_new[~df_runs_new[\"fold_number\"].isna()]\n    \n    if list_columns:\n        df_runs_new = df_runs_new[list_columns]\n        \n    if save_df:\n        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M\")\n        csv_filename = f\"df_runs_{timestamp}.csv\"\n        df_runs_new.to_csv(csv_filename, index=False)\n\n        print(f\"DataFrame saved to {csv_filename}, Shape: {df_unique.shape}\")\n\n    return df_runs_new","metadata":{"_cell_guid":"51030a77-aba3-469f-9c08-7963bc8a09d2","_uuid":"6ff7d358-62f0-4ac9-b028-d1c920bb4eaf","collapsed":false,"jupyter":{"outputs_hidden":false},"papermill":{"duration":0.022231,"end_time":"2023-10-29T17:16:45.128848","exception":false,"start_time":"2023-10-29T17:16:45.106617","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-11-07T23:23:53.248284Z","iopub.execute_input":"2023-11-07T23:23:53.248667Z","iopub.status.idle":"2023-11-07T23:23:53.309811Z","shell.execute_reply.started":"2023-11-07T23:23:53.248635Z","shell.execute_reply":"2023-11-07T23:23:53.308557Z"},"trusted":true},"execution_count":61,"outputs":[]},{"cell_type":"code","source":"if TRAIN:\n    df_exp  = experiments_data(list_experiment_id = None, save_df = None, list_columns = None)\n    list_base_cols = ['run_time','experiment_id','run_id','model_name','fold_number','mae','training_data_rows','training_data_columns'] \n    list_dynamic_params = list(model_params_dict[\"LGBMR\"]['dynamic_params'].keys())\n    \n    \n    \n    list_cols_exp = list_base_cols + list_dynamic_params +['model_path']\n    \n    \n    df_exp = df_exp[list_cols_exp]\n    \n    df_exp['run_time'] = pd.to_datetime(df_exp['run_time'], format='%Y%m%d_%H%M%S', errors='coerce')","metadata":{"execution":{"iopub.status.busy":"2023-11-07T23:23:53.311327Z","iopub.execute_input":"2023-11-07T23:23:53.311684Z","iopub.status.idle":"2023-11-07T23:23:54.103552Z","shell.execute_reply.started":"2023-11-07T23:23:53.311654Z","shell.execute_reply":"2023-11-07T23:23:54.102502Z"},"trusted":true},"execution_count":62,"outputs":[]},{"cell_type":"code","source":"if TRAIN:\n    list_files_feat_importance = ['/kaggle/working/feat_impor_optiver_trading_at_the_close_20231107_14_59_05.csv']\n\n\n\n    aggregate_feature_importance( list_files_feat_importance)\n","metadata":{"papermill":{"duration":0.011597,"end_time":"2023-10-29T17:16:45.39348","exception":false,"start_time":"2023-10-29T17:16:45.381883","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-11-07T23:23:54.105129Z","iopub.execute_input":"2023-11-07T23:23:54.106010Z","iopub.status.idle":"2023-11-07T23:23:54.172291Z","shell.execute_reply.started":"2023-11-07T23:23:54.105975Z","shell.execute_reply":"2023-11-07T23:23:54.171268Z"},"trusted":true},"execution_count":63,"outputs":[]},{"cell_type":"code","source":"#aggregate_feature_importance( list_files_feat_importance).tail(60)","metadata":{"execution":{"iopub.status.busy":"2023-11-07T23:23:54.173502Z","iopub.execute_input":"2023-11-07T23:23:54.173831Z","iopub.status.idle":"2023-11-07T23:23:54.227713Z","shell.execute_reply.started":"2023-11-07T23:23:54.173788Z","shell.execute_reply":"2023-11-07T23:23:54.226255Z"},"trusted":true},"execution_count":64,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def ensemble_predict(model_paths, X_test):\n    models = []\n    predictions = []\n\n    # Load models based on full artifact paths\n    for model_path in model_paths:\n        try:\n            # If using direct path to pkl\n            if model_path.endswith(\".pkl\"):\n                model = joblib.load(model_path)\n            else:\n                print(f\"Unsupported model format for {model_path}. Skipping.\")\n                continue  # Skip this iteration\n\n            models.append(model)\n        except Exception as e:\n            print(f\"Failed to load model at {model_path}. Error: {e}\")\n\n    # Make predictions\n    for model in models:\n        try:\n            pred = model.predict(X_test)\n            predictions.append(pred)\n        except Exception as e:\n            print(f\"Failed to make prediction with model. Error: {e}\")\n\n    # Average predictions\n    if len(predictions) > 0:\n        ensemble_pred = np.mean(predictions, axis=0)\n    else:\n        print(\"No valid models loaded. Cannot make ensemble predictions.\")\n        ensemble_pred = None\n\n    return ensemble_pred","metadata":{"_cell_guid":"95e6c8a8-62ed-4412-944a-5b08508702fc","_uuid":"60c6783c-1e42-43ca-ba32-bda313deb585","collapsed":false,"jupyter":{"outputs_hidden":false},"papermill":{"duration":0.02353,"end_time":"2023-10-29T17:16:45.534872","exception":false,"start_time":"2023-10-29T17:16:45.511342","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-11-07T23:23:54.229095Z","iopub.execute_input":"2023-11-07T23:23:54.229409Z","iopub.status.idle":"2023-11-07T23:23:54.288082Z","shell.execute_reply.started":"2023-11-07T23:23:54.229382Z","shell.execute_reply":"2023-11-07T23:23:54.286426Z"},"trusted":true},"execution_count":65,"outputs":[]},{"cell_type":"code","source":"","metadata":{"papermill":{"duration":0.012259,"end_time":"2023-10-29T17:16:45.559669","exception":false,"start_time":"2023-10-29T17:16:45.54741","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n\nif TRAIN:\n    model_paths = list(df_exp[(df_exp['experiment_id'] == \"795875964308489224\") & (df_exp['learning_rate'] == \"0.041316718960638196\") ]['model_path'])\n\n    if not os.path.exists(models_dir):\n        os.makedirs(models_dir)\n\n    for model_path in model_paths:\n        print(f\"Checking if model path exists: {model_path}\")\n\n        if not os.path.exists(model_path):\n            print(f\"File does not exist: {model_path}\")\n            continue  # Skip to the next iteration\n\n        specific_part = model_path.split(\"/\")[-2]\n        dest_path = os.path.join(models_dir, f\"{specific_part}.pkl\")\n        if not os.path.exists(dest_path):\n            print(f\"Copying from {model_path} to {dest_path}\")\n            shutil.copy(model_path, dest_path)\n        else:\n            print(f\"File {dest_path} already exists. Skipping copy.\")\n\n    zipf = zipfile.ZipFile(\n        f\"/kaggle/working/{models_dir}.zip\", \"w\", zipfile.ZIP_DEFLATED\n    )\n\n    # Navigate through the folder and add each file to the ZIP\n    for root, dirs, files in os.walk(f\"/kaggle/working/{models_dir}\"):\n        for file in files:\n            zipf.write(\n                os.path.join(root, file),\n                os.path.relpath(\n                    os.path.join(root, file), f\"/kaggle/working/{models_dir}\"\n                ),\n            )\n\n    zipf.close()","metadata":{"papermill":{"duration":0.025212,"end_time":"2023-10-29T17:16:45.655277","exception":false,"start_time":"2023-10-29T17:16:45.630065","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-11-07T23:23:54.292962Z","iopub.execute_input":"2023-11-07T23:23:54.293376Z","iopub.status.idle":"2023-11-07T23:23:55.887553Z","shell.execute_reply.started":"2023-11-07T23:23:54.293344Z","shell.execute_reply":"2023-11-07T23:23:55.886523Z"},"trusted":true},"execution_count":66,"outputs":[{"name":"stdout","text":"Checking if model path exists: /kaggle/working/experiments_storage/795875964308489224/4669db01bde5486a900a13a8a758e6b1/artifacts/LGBMR_0_20231107_161833/model.pkl\nFile models_5/LGBMR_0_20231107_161833.pkl already exists. Skipping copy.\nChecking if model path exists: /kaggle/working/experiments_storage/795875964308489224/93f449d6453249d79b3b92b05b9fcd8e/artifacts/LGBMR_0_20231107_160237/model.pkl\nFile models_5/LGBMR_0_20231107_160237.pkl already exists. Skipping copy.\nChecking if model path exists: /kaggle/working/experiments_storage/795875964308489224/3bdf0c22c9344c91a559e134a99ab6b6/artifacts/LGBMR_0_20231107_154648/model.pkl\nFile models_5/LGBMR_0_20231107_154648.pkl already exists. Skipping copy.\nChecking if model path exists: /kaggle/working/experiments_storage/795875964308489224/dd1edce349be4a0e9a1b6038f25f8f51/artifacts/LGBMR_0_20231107_153045/model.pkl\nFile models_5/LGBMR_0_20231107_153045.pkl already exists. Skipping copy.\nChecking if model path exists: /kaggle/working/experiments_storage/795875964308489224/3d945d77595d4333b02c62b659e194ab/artifacts/LGBMR_0_20231107_151455/model.pkl\nFile models_5/LGBMR_0_20231107_151455.pkl already exists. Skipping copy.\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"papermill":{"duration":0.012391,"end_time":"2023-10-29T17:16:45.680337","exception":false,"start_time":"2023-10-29T17:16:45.667946","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_paths = []\nmodels_dir_input = models_dir.replace(\"_\", \"-\")\ndirectory = f\"/kaggle/input/{models_dir_input}\"\n\n# Check if the directory exists\nif os.path.exists(directory):\n    # Traverse the directory and collect file paths\n    for filename in os.listdir(directory):\n        full_path = os.path.join(directory, filename)\n\n        # Check if the item is a file (and not a sub-directory)\n        if os.path.isfile(full_path):\n            model_paths.append(full_path)\nelse:\n    print(f\"The directory {directory} does not exist.\")\n\n# Print or return the list of file paths\nprint(\"List of file paths:\", model_paths)","metadata":{"papermill":{"duration":0.026356,"end_time":"2023-10-29T17:16:45.718974","exception":false,"start_time":"2023-10-29T17:16:45.692618","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-11-07T23:23:55.889376Z","iopub.execute_input":"2023-11-07T23:23:55.890314Z","iopub.status.idle":"2023-11-07T23:23:55.953396Z","shell.execute_reply.started":"2023-11-07T23:23:55.890266Z","shell.execute_reply":"2023-11-07T23:23:55.952072Z"},"trusted":true},"execution_count":67,"outputs":[{"name":"stdout","text":"List of file paths: ['/kaggle/input/models-5/LGBMR_0_20231107_154648.pkl', '/kaggle/input/models-5/LGBMR_0_20231107_151455.pkl', '/kaggle/input/models-5/LGBMR_0_20231107_161833.pkl', '/kaggle/input/models-5/LGBMR_0_20231107_160237.pkl', '/kaggle/input/models-5/LGBMR_0_20231107_153045.pkl']\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"papermill":{"duration":0.01212,"end_time":"2023-10-29T17:16:45.74376","exception":false,"start_time":"2023-10-29T17:16:45.73164","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Assuming X_test for predict\n# ensemble_predictions = ensemble_predict(model_paths, df_test, mlflow_client)","metadata":{"_cell_guid":"79aa495a-7580-497c-b913-8941cf7d0c61","_uuid":"b3efe16f-ca61-4000-94d7-568364d0e11b","collapsed":false,"jupyter":{"outputs_hidden":false},"papermill":{"duration":0.019319,"end_time":"2023-10-29T17:16:45.775373","exception":false,"start_time":"2023-10-29T17:16:45.756054","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-11-07T23:23:55.955221Z","iopub.execute_input":"2023-11-07T23:23:55.955570Z","iopub.status.idle":"2023-11-07T23:23:56.008195Z","shell.execute_reply.started":"2023-11-07T23:23:55.955542Z","shell.execute_reply":"2023-11-07T23:23:56.006914Z"},"trusted":true},"execution_count":68,"outputs":[]},{"cell_type":"code","source":"import optiver2023\n\nenv = optiver2023.make_env()\niter_test = env.iter_test()","metadata":{"_cell_guid":"d4bfb979-2758-4fe9-8947-399b1c3a574c","_uuid":"bef1a199-0988-4743-813e-99d4308fc9bb","collapsed":false,"jupyter":{"outputs_hidden":false},"papermill":{"duration":0.04228,"end_time":"2023-10-29T17:16:45.830037","exception":false,"start_time":"2023-10-29T17:16:45.787757","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-11-08T07:29:05.962337Z","iopub.execute_input":"2023-11-08T07:29:05.962824Z","iopub.status.idle":"2023-11-08T07:29:06.034070Z","shell.execute_reply.started":"2023-11-08T07:29:05.962789Z","shell.execute_reply":"2023-11-08T07:29:06.032610Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"counter = 0\nfor test, revealed_targets, sample_prediction in iter_test:\n    # df_test_raw = pl.DataFrame(test)\n    break\n\n    feat = feat_engineering(test)\n    \n    list_cols_drop = [\"date_id\",\"stock_id\"]\n    feat.drop(list_cols_drop, axis=1, inplace=True)\n\n    sample_prediction[\"target\"] = ensemble_predict(model_paths, feat)\n    env.predict(sample_prediction)\n    counter += 1","metadata":{"_cell_guid":"d30f3862-dca2-4242-9db0-ab9750118622","_uuid":"efd9073c-aaef-4135-bf7c-1e892e9aa831","collapsed":false,"jupyter":{"outputs_hidden":false},"papermill":{"duration":105.467568,"end_time":"2023-10-29T17:18:31.345732","exception":false,"start_time":"2023-10-29T17:16:45.878164","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-11-08T07:29:10.730345Z","iopub.execute_input":"2023-11-08T07:29:10.731147Z","iopub.status.idle":"2023-11-08T07:29:11.071940Z","shell.execute_reply.started":"2023-11-08T07:29:10.731101Z","shell.execute_reply":"2023-11-08T07:29:11.070717Z"},"trusted":true},"execution_count":33,"outputs":[{"name":"stdout","text":"This version of the API is not optimized and should not be used to estimate the runtime of your code on the hidden test set.\n","output_type":"stream"}]},{"cell_type":"code","source":"test.sort_values('seconds_in_bucket')","metadata":{"execution":{"iopub.status.busy":"2023-11-08T07:30:53.789385Z","iopub.execute_input":"2023-11-08T07:30:53.789849Z","iopub.status.idle":"2023-11-08T07:30:53.855573Z","shell.execute_reply.started":"2023-11-08T07:30:53.789811Z","shell.execute_reply":"2023-11-08T07:30:53.854201Z"},"trusted":true},"execution_count":37,"outputs":[{"execution_count":37,"output_type":"execute_result","data":{"text/plain":"     stock_id  date_id  seconds_in_bucket  imbalance_size  \\\n0           0      478                  0      3753451.43   \n127       127      478                  0      4225037.90   \n128       128      478                  0       100334.92   \n129       129      478                  0      1666274.13   \n130       130      478                  0     11414578.85   \n..        ...      ...                ...             ...   \n70         70      478                  0       185826.72   \n71         71      478                  0     10466128.33   \n72         72      478                  0     18037836.92   \n62         62      478                  0       767473.37   \n199       199      478                  0      1560808.25   \n\n     imbalance_buy_sell_flag  reference_price  matched_size  far_price  \\\n0                         -1         0.999875   11548975.43        NaN   \n127                        1         1.000889    5204117.67        NaN   \n128                       -1         0.999819    4219820.57        NaN   \n129                       -1         0.999733    5435654.58        NaN   \n130                       -1         0.999967   22861069.51        NaN   \n..                       ...              ...           ...        ...   \n70                        -1         1.000392     612766.26        NaN   \n71                         1         0.999966   16000317.50        NaN   \n72                         1         0.998715   61918888.62        NaN   \n62                         1         1.000857    1573691.38        NaN   \n199                        1         0.999441    5375971.35        NaN   \n\n     near_price  bid_price  bid_size  ask_price  ask_size  wap     row_id  \n0           NaN   0.999875  22940.00   1.000050   9177.60  1.0    478_0_0  \n127         NaN   0.999991    400.93   1.000889  40129.00  1.0  478_0_127  \n128         NaN   0.999819  33840.00   1.000085  15984.25  1.0  478_0_128  \n129         NaN   0.999733  10437.10   1.000025    992.67  1.0  478_0_129  \n130         NaN   0.999967  21076.86   1.000069  43931.00  1.0  478_0_130  \n..          ...        ...       ...        ...       ...  ...        ...  \n70          NaN   0.999621   1297.00   1.001163   3987.93  1.0   478_0_70  \n71          NaN   0.999966   1550.14   1.000753  34129.92  1.0   478_0_71  \n72          NaN   0.999718  18937.00   1.000141   9472.50  1.0   478_0_72  \n62          NaN   0.999983    549.56   1.001585  52154.19  1.0   478_0_62  \n199         NaN   0.999761  28143.00   1.000081   9540.40  1.0  478_0_199  \n\n[200 rows x 15 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>stock_id</th>\n      <th>date_id</th>\n      <th>seconds_in_bucket</th>\n      <th>imbalance_size</th>\n      <th>imbalance_buy_sell_flag</th>\n      <th>reference_price</th>\n      <th>matched_size</th>\n      <th>far_price</th>\n      <th>near_price</th>\n      <th>bid_price</th>\n      <th>bid_size</th>\n      <th>ask_price</th>\n      <th>ask_size</th>\n      <th>wap</th>\n      <th>row_id</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>478</td>\n      <td>0</td>\n      <td>3753451.43</td>\n      <td>-1</td>\n      <td>0.999875</td>\n      <td>11548975.43</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.999875</td>\n      <td>22940.00</td>\n      <td>1.000050</td>\n      <td>9177.60</td>\n      <td>1.0</td>\n      <td>478_0_0</td>\n    </tr>\n    <tr>\n      <th>127</th>\n      <td>127</td>\n      <td>478</td>\n      <td>0</td>\n      <td>4225037.90</td>\n      <td>1</td>\n      <td>1.000889</td>\n      <td>5204117.67</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.999991</td>\n      <td>400.93</td>\n      <td>1.000889</td>\n      <td>40129.00</td>\n      <td>1.0</td>\n      <td>478_0_127</td>\n    </tr>\n    <tr>\n      <th>128</th>\n      <td>128</td>\n      <td>478</td>\n      <td>0</td>\n      <td>100334.92</td>\n      <td>-1</td>\n      <td>0.999819</td>\n      <td>4219820.57</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.999819</td>\n      <td>33840.00</td>\n      <td>1.000085</td>\n      <td>15984.25</td>\n      <td>1.0</td>\n      <td>478_0_128</td>\n    </tr>\n    <tr>\n      <th>129</th>\n      <td>129</td>\n      <td>478</td>\n      <td>0</td>\n      <td>1666274.13</td>\n      <td>-1</td>\n      <td>0.999733</td>\n      <td>5435654.58</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.999733</td>\n      <td>10437.10</td>\n      <td>1.000025</td>\n      <td>992.67</td>\n      <td>1.0</td>\n      <td>478_0_129</td>\n    </tr>\n    <tr>\n      <th>130</th>\n      <td>130</td>\n      <td>478</td>\n      <td>0</td>\n      <td>11414578.85</td>\n      <td>-1</td>\n      <td>0.999967</td>\n      <td>22861069.51</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.999967</td>\n      <td>21076.86</td>\n      <td>1.000069</td>\n      <td>43931.00</td>\n      <td>1.0</td>\n      <td>478_0_130</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>70</th>\n      <td>70</td>\n      <td>478</td>\n      <td>0</td>\n      <td>185826.72</td>\n      <td>-1</td>\n      <td>1.000392</td>\n      <td>612766.26</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.999621</td>\n      <td>1297.00</td>\n      <td>1.001163</td>\n      <td>3987.93</td>\n      <td>1.0</td>\n      <td>478_0_70</td>\n    </tr>\n    <tr>\n      <th>71</th>\n      <td>71</td>\n      <td>478</td>\n      <td>0</td>\n      <td>10466128.33</td>\n      <td>1</td>\n      <td>0.999966</td>\n      <td>16000317.50</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.999966</td>\n      <td>1550.14</td>\n      <td>1.000753</td>\n      <td>34129.92</td>\n      <td>1.0</td>\n      <td>478_0_71</td>\n    </tr>\n    <tr>\n      <th>72</th>\n      <td>72</td>\n      <td>478</td>\n      <td>0</td>\n      <td>18037836.92</td>\n      <td>1</td>\n      <td>0.998715</td>\n      <td>61918888.62</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.999718</td>\n      <td>18937.00</td>\n      <td>1.000141</td>\n      <td>9472.50</td>\n      <td>1.0</td>\n      <td>478_0_72</td>\n    </tr>\n    <tr>\n      <th>62</th>\n      <td>62</td>\n      <td>478</td>\n      <td>0</td>\n      <td>767473.37</td>\n      <td>1</td>\n      <td>1.000857</td>\n      <td>1573691.38</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.999983</td>\n      <td>549.56</td>\n      <td>1.001585</td>\n      <td>52154.19</td>\n      <td>1.0</td>\n      <td>478_0_62</td>\n    </tr>\n    <tr>\n      <th>199</th>\n      <td>199</td>\n      <td>478</td>\n      <td>0</td>\n      <td>1560808.25</td>\n      <td>1</td>\n      <td>0.999441</td>\n      <td>5375971.35</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.999761</td>\n      <td>28143.00</td>\n      <td>1.000081</td>\n      <td>9540.40</td>\n      <td>1.0</td>\n      <td>478_0_199</td>\n    </tr>\n  </tbody>\n</table>\n<p>200 rows  15 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"papermill":{"duration":0.022343,"end_time":"2023-10-29T17:18:31.445104","exception":false,"start_time":"2023-10-29T17:18:31.422761","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"papermill":{"duration":0.02223,"end_time":"2023-10-29T17:18:31.490205","exception":false,"start_time":"2023-10-29T17:18:31.467975","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"papermill":{"duration":0.022172,"end_time":"2023-10-29T17:18:31.534807","exception":false,"start_time":"2023-10-29T17:18:31.512635","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"papermill":{"duration":0.022005,"end_time":"2023-10-29T17:18:31.579172","exception":false,"start_time":"2023-10-29T17:18:31.557167","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"_cell_guid":"18344629-3cbc-4244-ac34-c8af6b88b0d8","_uuid":"667a4d83-d235-446e-a72c-cf02718a1a7b","collapsed":false,"jupyter":{"outputs_hidden":false},"papermill":{"duration":0.022032,"end_time":"2023-10-29T17:18:31.623976","exception":false,"start_time":"2023-10-29T17:18:31.601944","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"papermill":{"duration":0.022491,"end_time":"2023-10-29T17:18:31.668688","exception":false,"start_time":"2023-10-29T17:18:31.646197","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#clean_directory_except_one('/kaggle/working/', 'submission.csv')","metadata":{"papermill":{"duration":0.030147,"end_time":"2023-10-29T17:18:31.721197","exception":false,"start_time":"2023-10-29T17:18:31.69105","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-11-07T23:23:56.153019Z","iopub.status.idle":"2023-11-07T23:23:56.153502Z","shell.execute_reply.started":"2023-11-07T23:23:56.153285Z","shell.execute_reply":"2023-11-07T23:23:56.153308Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"_cell_guid":"3ef4ae54-1bb4-4cc3-a7da-a6691a272d30","_uuid":"495a4e04-554d-4ef1-919d-62bc59f089c4","collapsed":false,"jupyter":{"outputs_hidden":false},"papermill":{"duration":0.022039,"end_time":"2023-10-29T17:18:31.765571","exception":false,"start_time":"2023-10-29T17:18:31.743532","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"papermill":{"duration":0.023232,"end_time":"2023-10-29T17:18:31.811015","exception":false,"start_time":"2023-10-29T17:18:31.787783","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"_cell_guid":"5498ae4b-62e8-4050-91e7-075fc549380f","_uuid":"ce9653b2-7dff-4c5b-af77-5f0ef88b6e86","collapsed":false,"jupyter":{"outputs_hidden":false},"papermill":{"duration":0.022222,"end_time":"2023-10-29T17:18:31.856624","exception":false,"start_time":"2023-10-29T17:18:31.834402","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"_cell_guid":"92a8ed18-3a63-464e-9d62-17cea3baed59","_uuid":"0b017968-3b5a-4465-820e-e89659b10afd","collapsed":false,"jupyter":{"outputs_hidden":false},"papermill":{"duration":0.022485,"end_time":"2023-10-29T17:18:31.901283","exception":false,"start_time":"2023-10-29T17:18:31.878798","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"_cell_guid":"f9d19e8d-0074-4ae2-adfb-4c039b700215","_uuid":"b6096d27-d452-4593-9832-d15bfbd83e1b","collapsed":false,"jupyter":{"outputs_hidden":false},"papermill":{"duration":0.022606,"end_time":"2023-10-29T17:18:31.94609","exception":false,"start_time":"2023-10-29T17:18:31.923484","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"_cell_guid":"855f8db3-e344-4d4d-b577-36499775f352","_uuid":"fde5edb9-d23d-41ac-92c6-593f299230c2","collapsed":false,"jupyter":{"outputs_hidden":false},"papermill":{"duration":0.021902,"end_time":"2023-10-29T17:18:31.990239","exception":false,"start_time":"2023-10-29T17:18:31.968337","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"_cell_guid":"82330d89-f35e-4cae-876c-de852438527d","_uuid":"f90f329d-b882-464c-84fc-74a815cfe1cd","collapsed":false,"jupyter":{"outputs_hidden":false},"papermill":{"duration":0.021851,"end_time":"2023-10-29T17:18:32.034578","exception":false,"start_time":"2023-10-29T17:18:32.012727","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"_cell_guid":"ed96eca6-9765-4c90-a6bf-3071518ba2f0","_uuid":"ca1d48b9-5537-491b-9de2-57360edd9844","collapsed":false,"jupyter":{"outputs_hidden":false},"papermill":{"duration":0.022409,"end_time":"2023-10-29T17:18:32.079037","exception":false,"start_time":"2023-10-29T17:18:32.056628","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### ","metadata":{"_cell_guid":"06b24b0a-6515-48c6-b4a0-240865cfc1ea","_uuid":"5e9b8536-f812-432a-ba05-c2f0b0f8994f","jupyter":{"outputs_hidden":false},"papermill":{"duration":0.021691,"end_time":"2023-10-29T17:18:32.12318","exception":false,"start_time":"2023-10-29T17:18:32.101489","status":"completed"},"tags":[]}}]}