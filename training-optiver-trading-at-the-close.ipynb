{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"papermill":{"default_parameters":{},"duration":112.139146,"end_time":"2023-10-29T17:18:32.866665","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2023-10-29T17:16:40.727519","version":"2.4.0"}},"nbformat_minor":5,"nbformat":4,"cells":[{"cell_type":"code","source":"# Constants\nTRAIN = True\nOVERWRITE = False\nDEBUG = False\n\nmodels_dir = \"models_5\"\n\nN_TRIALS = 3\n\nVERSION_NB = 4\n\nstate = 42\n\ndownload_kaggle_data = False\n\n# External general-purpose modules\nimport os\nimport shutil\nimport warnings\nfrom datetime import datetime\nimport glob\nfrom itertools import combinations\nfrom warnings import simplefilter\n\nimport numpy as np\nimport pandas as pd\nimport polars as pl\nfrom dotenv import load_dotenv\nfrom joblib import dump\nimport joblib\nimport os\n\n# Setting pandas options and warning filters\npd.set_option(\"display.max_columns\", None)\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\nwarnings.filterwarnings(\"ignore\")\nsimplefilter(action=\"ignore\", category=pd.errors.PerformanceWarning)\n\n# Load environment variables\nload_dotenv()","metadata":{"papermill":{"duration":0.660513,"end_time":"2023-10-29T17:16:44.604311","exception":false,"start_time":"2023-10-29T17:16:43.943798","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-11-05T20:56:31.087201Z","iopub.execute_input":"2023-11-05T20:56:31.088479Z","iopub.status.idle":"2023-11-05T20:56:31.961507Z","shell.execute_reply.started":"2023-11-05T20:56:31.088440Z","shell.execute_reply":"2023-11-05T20:56:31.959906Z"},"trusted":true},"execution_count":1,"outputs":[{"execution_count":1,"output_type":"execute_result","data":{"text/plain":"False"},"metadata":{}}]},{"cell_type":"code","source":"\npath_project_dir = os.getcwd()\nif path_project_dir not in [\"/kaggle/working\", \"/content\"]:\n    path_project_dir = os.getenv(\"ROOT_PATH\")\n\n# Imports and setup for training\nif TRAIN:\n    from numba import njit, prange\n\n    import itertools as itt\n    # Install packages and import logging libraries\n    if path_project_dir == '/kaggle/working':\n        !pip install loguru mlflow optuna > /dev/null\n        \n\n    from utils import log_feature_importance, create_model, log_training_details, aggregate_feature_importance,  get_data, clean_directory_except_one\n    \n    from loguru import logger\n    import mlflow\n    import optuna\n    from optuna.integration.mlflow import MLflowCallback\n    from mlflow.tracking import MlflowClient\n    import zipfile\n    \n    from tqdm import tqdm\n\n    # Import machine learning libraries\n    import lightgbm as lgbm\n    \n    from lightgbm import log_evaluation, early_stopping, LGBMRegressor as LGBMR\n    from sklearn.model_selection import KFold\n    from xgboost import XGBRegressor as XGBR\n\n    # Set logging\n    logger.add(\"logs.log\", format=\"{time:YYYY-MM-DD HH:mm} | {level} | {message}\")\n    optuna.logging.set_verbosity(optuna.logging.WARNING)\n    warnings.filterwarnings(\"ignore\", category=optuna.exceptions.ExperimentalWarning)\n    \n    # Auto-reload modules\n    %load_ext autoreload\n    %autoreload 2\n\n    # Initialize MLflow callback\n    mlflow_callback = MLflowCallback(\n        tracking_uri=mlflow.get_tracking_uri(), metric_name=\"mae\"\n    )","metadata":{"_cell_guid":"3bdaa15b-0b9e-4b3b-9c47-58e6b9b18b5e","_uuid":"206043b9-f1d5-4691-b03a-935f2177086d","collapsed":false,"jupyter":{"outputs_hidden":false},"papermill":{"duration":0.032481,"end_time":"2023-10-29T17:16:44.649372","exception":false,"start_time":"2023-10-29T17:16:44.616891","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-11-05T20:56:31.963599Z","iopub.execute_input":"2023-11-05T20:56:31.964354Z","iopub.status.idle":"2023-11-05T20:56:54.667031Z","shell.execute_reply.started":"2023-11-05T20:56:31.964301Z","shell.execute_reply":"2023-11-05T20:56:54.665376Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"if path_project_dir == \"/kaggle/working\":\n    path_data_project_dir = \"/kaggle/input/optiver-trading-at-the-close\"\n    path_experiments_storage = os.path.join(path_project_dir, \"experiments_storage\")\n\n    path_dataset_train_raw = \"/kaggle/input/optiver-trading-at-the-close/train.csv\"\n    path_dataset_test_raw = (\n        \"/kaggle/input/optiver-trading-at-the-close/example_test_files/test.csv\"\n    )\n\n    path_dataset_processed = \"/kaggle/working/processed_data\"\n    path_dataset_train = os.path.join(path_dataset_processed, \"train.csv\")\n    path_dataset_test = os.path.join(path_dataset_processed, \"test.csv\")\n\nelse:\n    name_folder_data_project = \"kaggle_optiver_trading_at_the_close\"\n\n    path_data_dir = os.path.join(path_project_dir, \"data\")\n    path_dataset_train_raw = os.path.join(\n        path_data_dir, \"kaggle_optiver_trading_at_the_close/raw\", \"train.csv\"\n    )\n    path_dataset_processed = os.path.join(\n        path_data_dir, \"kaggle_optiver_trading_at_the_close/processed\"\n    )\n\n    path_data_project_dir = os.path.join(path_data_dir, name_folder_data_project)\n\n    path_config_dir = os.path.join(path_project_dir, \"config\")\n    path_config_train = os.path.join(path_config_dir, \"train_config.yaml\")\n\n    path_experiments_storage = os.path.join(\n        path_data_project_dir, \"experiments_storage\"\n    )\n\n    if download_kaggle_data:\n        dataset_name = \"ravi20076/optiver-memoryreduceddatasets\"\n        kaggle_json_path = os.path.join(path_project_dir, \"kaggle.json\")\n        get_data(\n            kaggle_json_path,\n            path_data_project_dir,\n            dataset_name=dataset_name,\n            specific_file=None,\n        )\n\n    file_name_df_train = \"train.csv\"\n    file_name_df_test = \"test.csv\"\n\n    path_dataset_train = os.path.join(path_data_project_dir, file_name_df_train)\n    path_dataset_test = os.path.join(path_data_project_dir, file_name_df_test)\n\nif TRAIN:\n    mlflow.set_tracking_uri(path_experiments_storage)\n    client = MlflowClient()","metadata":{"_cell_guid":"9765420b-f8ab-46ba-8b4c-e4487eb04b76","_uuid":"c4174134-1441-41f8-adbf-f03ad3f7d7dc","collapsed":false,"jupyter":{"outputs_hidden":false},"papermill":{"duration":0.024352,"end_time":"2023-10-29T17:16:44.685842","exception":false,"start_time":"2023-10-29T17:16:44.661490","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-11-05T20:56:54.669344Z","iopub.execute_input":"2023-11-05T20:56:54.669753Z","iopub.status.idle":"2023-11-05T20:56:54.710044Z","shell.execute_reply.started":"2023-11-05T20:56:54.669721Z","shell.execute_reply":"2023-11-05T20:56:54.708858Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"def reduce_mem_usage(df, verbose=0):\n    \"\"\"\n    Iterate through all numeric columns of a dataframe and modify the data type\n    to reduce memory usage.\n    \"\"\"\n\n    start_mem = df.memory_usage().sum() / 1024**2\n\n    for col in df.columns:\n        col_type = df[col].dtype\n\n        if (col_type != object) and (col != \"target\"):\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == \"int\":\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)\n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float32)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float32)\n\n    if verbose:\n        logger.info(f\"Memory usage of dataframe is {start_mem:.2f} MB\")\n        end_mem = df.memory_usage().sum() / 1024**2\n        logger.info(f\"Memory usage after optimization is: {end_mem:.2f} MB\")\n        decrease = 100 * (start_mem - end_mem) / start_mem\n        logger.info(f\"Decreased by {decrease:.2f}%\")\n\n    return df\n","metadata":{"execution":{"iopub.status.busy":"2023-11-05T20:56:54.712810Z","iopub.execute_input":"2023-11-05T20:56:54.713174Z","iopub.status.idle":"2023-11-05T20:56:54.761251Z","shell.execute_reply.started":"2023-11-05T20:56:54.713145Z","shell.execute_reply":"2023-11-05T20:56:54.759107Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if TRAIN:\n    if not os.path.exists(path_dataset_processed):\n        os.makedirs(path_dataset_processed)\n\n    if not os.path.exists(path_dataset_train) or OVERWRITE:\n        df_train_raw = pd.read_csv(path_dataset_train_raw)\n        \n    else:\n        df_train_raw = pd.read_csv(path_dataset_train)\n\n    if DEBUG:\n        df_train_raw = df_train_raw[df_train_raw[\"stock_id\"].isin([0, 1, 2])]\n        \n","metadata":{"papermill":{"duration":0.021683,"end_time":"2023-10-29T17:16:44.743683","exception":false,"start_time":"2023-10-29T17:16:44.722000","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-11-05T20:56:54.762969Z","iopub.execute_input":"2023-11-05T20:56:54.763385Z","iopub.status.idle":"2023-11-05T20:57:10.189403Z","shell.execute_reply.started":"2023-11-05T20:56:54.763355Z","shell.execute_reply":"2023-11-05T20:57:10.188107Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"if TRAIN:\n    # Dropping rows with null targets:-\n    drop_idx = df_train_raw.loc[df_train_raw[\"target\"].isna(), \"target\"].index.to_list()\n    df_train_raw = df_train_raw.drop(drop_idx, axis=0)\n    df_train_raw.reset_index(drop=True, inplace=True)\n    df_train_raw = df_train_raw.drop([\"time_id\",\"row_id\"], axis = 1)","metadata":{"papermill":{"duration":0.020681,"end_time":"2023-10-29T17:16:44.777236","exception":false,"start_time":"2023-10-29T17:16:44.756555","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-11-05T20:57:10.190662Z","iopub.execute_input":"2023-11-05T20:57:10.191612Z","iopub.status.idle":"2023-11-05T20:57:10.726879Z","shell.execute_reply.started":"2023-11-05T20:57:10.191574Z","shell.execute_reply":"2023-11-05T20:57:10.725339Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"def feat_engineering(df_train):\n    df = pl.DataFrame(df_train)\n    # 7. Handle Missing Values\n    df = df.with_columns(\n        [\n            pl.col(\"far_price\").fill_null(strategy=\"forward\").alias(\"far_price\"),\n            pl.col(\"near_price\").fill_null(strategy=\"forward\").alias(\"near_price\"),\n        ]\n    )\n    # Level 1 Features\n    level_one_features = [\n        (pl.col(\"imbalance_size\") / pl.col(\"matched_size\")).alias(\n            \"imbalance_to_matched_size\"\n        ),\n        (pl.col(\"imbalance_size\") * pl.col(\"imbalance_buy_sell_flag\")).alias(\n            \"imbalance_flag_to_size\"\n        ),\n        (pl.col(\"ask_price\") - pl.col(\"bid_price\")).alias(\"spread\"),\n        (pl.col(\"bid_size\") - pl.col(\"ask_size\")).alias(\"bid_ask_imbalance\"),\n        (pl.col(\"bid_size\") / pl.col(\"ask_size\")).alias(\"liquidity\"),\n      #  (pl.col(\"ask_size\") - pl.col(\"wap\")).alias(\"size_diff_ask_to_wap\"),\n        (pl.col(\"wap\") - pl.col(\"wap\").shift(1).over([\"stock_id\", \"date_id\"])).alias(\n            \"wap_velocity\"\n        ),\n        (\n            pl.col(\"wap\") / pl.col(\"wap\").shift(5).over([\"stock_id\", \"date_id\"]) - 1\n        ).alias(\"wap_momentum_5\"),\n        (\n            pl.col(\"wap\")\n            .std()\n            .over([\"stock_id\", \"date_id\"])\n            .alias(\"short_term_volatility\")\n        ),\n        (\n            (\n                pl.col(\"imbalance_size\")\n                / (pl.col(\"matched_size\") + pl.col(\"imbalance_size\"))\n            ).alias(\"price_impact\")\n        ),\n        (\n            (pl.col(\"bid_size\") - pl.col(\"ask_size\"))\n            / (pl.col(\"bid_size\") + pl.col(\"ask_size\"))\n        ).alias(\"order_imbalance_ratio\"),\n        (\n            (pl.col(\"ask_price\") - pl.col(\"bid_price\"))\n            / (pl.col(\"ask_price\") + pl.col(\"bid_price\"))\n        ).alias(\"price_skewness\"),\n        (pl.col(\"seconds_in_bucket\") / 600).alias(\"time_decay\"),\n    ]\n\n    # Level 2 Features\n    level_two_features = [\n        (\n            pl.col(\"wap_velocity\")\n            - pl.col(\"wap_velocity\").shift(1).over([\"stock_id\", \"date_id\"])\n        ).alias(\"wap_acceleration\"),\n        (\n            pl.col(\"short_term_volatility\").shift(1).over([\"stock_id\", \"date_id\"])\n            - pl.col(\"short_term_volatility\")\n        ).alias(\"volatility_rate_of_change\"),\n        (\n            (\n                pl.col(\"liquidity\")\n                - pl.col(\"liquidity\").shift(1).over([\"stock_id\", \"date_id\"])\n            )\n            / pl.col(\"liquidity\").shift(1).over([\"stock_id\", \"date_id\"])\n        ).alias(\"liquidity_ratio_change\"),\n        (\n            (\n                pl.col(\"order_imbalance_ratio\")\n                - pl.col(\"order_imbalance_ratio\").shift(1).over([\"stock_id\", \"date_id\"])\n            )\n            / pl.col(\"order_imbalance_ratio\").shift(1).over([\"stock_id\", \"date_id\"])\n        ).alias(\"order_imbalance_over_time\"),\n        (\n            (\n                pl.col(\"price_skewness\")\n                - pl.col(\"price_skewness\").shift(1).over([\"stock_id\", \"date_id\"])\n            )\n            / pl.col(\"price_skewness\").shift(1).over([\"stock_id\", \"date_id\"])\n        ).alias(\"price_skewness_rate_of_change\"),\n    ]\n\n    # Level 3 Features\n    level_three_aggregations = [\n        pl.col(\"wap\").mean().alias(\"avg_wap_by_market\"),\n        pl.col(\"near_price\").mean().alias(\"avg_near_price_by_market\"),\n        pl.col(\"matched_size\").mean().alias(\"avg_matched_size_by_market\"),\n        pl.col(\"imbalance_to_matched_size\")\n        .mean()\n        .alias(\"avg_imbalance_to_matched_size_by_market\"),\n        pl.col(\"spread\").mean().alias(\"avg_spread_by_market\"),\n        pl.col(\"liquidity\").mean().alias(\"avg_liquidity_by_market\"),\n        pl.col(\"short_term_volatility\").mean().alias(\"avg_market_volatility\"),\n        pl.col(\"order_imbalance_ratio\").mean().alias(\"avg_market_imbalance\"),\n        pl.col(\"liquidity\").mean().alias(\"avg_market_liquidity\"),\n        pl.col(\"price_impact\").mean().alias(\"avg_market_price_impact\"),\n        pl.col(\"price_skewness\").mean().alias(\"avg_market_price_skewness\"),\n    ]\n\n    # Adding all features and performing join operation\n    df = df.with_columns(level_one_features)\n    df = df.with_columns(level_two_features)\n    group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n        *level_three_aggregations\n    )\n    df = df.join(group_by_market, on=[\"date_id\", \"seconds_in_bucket\"], how=\"left\")\n\n    polynomial_and_interaction_features = [\n        (pl.col(\"seconds_in_bucket\") * pl.col(\"near_price\")).alias(\n            \"seconds_in_bucket_X_near_price\"\n        ),\n        (pl.col(\"matched_size\") * pl.col(\"near_price\")).alias(\n            \"matched_size_X_near_price\"\n        ),\n        (pl.col(\"near_price\") ** 2).alias(\"near_price_squared\"),\n        (pl.col(\"matched_size\") ** 2).alias(\"matched_size_squared\"),\n        (pl.col(\"seconds_in_bucket\") * pl.col(\"imbalance_flag_to_size\")).alias(\n            \"seconds_in_bucket_X_imbalance_flag_to_size\"\n        ),\n        (pl.col(\"seconds_in_bucket\") ** 2).alias(\"seconds_in_bucket_squared\"),\n        (pl.col(\"imbalance_flag_to_size\") ** 2).alias(\"imbalance_flag_to_size_squared\"),\n    ]\n\n    # Relative to Market Features\n    relative_to_market_features = [\n        (pl.col(\"wap\") / pl.col(\"avg_wap_by_market\")).alias(\"relative_wap_to_market\"),\n        (pl.col(\"near_price\") / pl.col(\"avg_near_price_by_market\")).alias(\n            \"relative_near_price_to_market\"\n        ),\n        (pl.col(\"matched_size\") / pl.col(\"avg_matched_size_by_market\")).alias(\n            \"relative_matched_size_to_market\"\n        ),\n        (\n            pl.col(\"imbalance_to_matched_size\")\n            / pl.col(\"avg_imbalance_to_matched_size_by_market\")\n        ).alias(\"relative_imbalance_to_matched_size_to_market\"),\n        (pl.col(\"spread\") / pl.col(\"avg_spread_by_market\")).alias(\n            \"relative_spread_to_market\"\n        ),\n        (pl.col(\"liquidity\") / pl.col(\"avg_liquidity_by_market\")).alias(\n            \"relative_liquidity_to_market\"\n        ),\n    ]\n\n    # Combine all Level 4 features and add them to the DataFrame\n    all_level_four_features = (\n        polynomial_and_interaction_features + relative_to_market_features\n    )\n    df = df.with_columns(all_level_four_features)\n\n    for window in [5, 10]:\n        rolling_group = df.group_by_rolling(\n            index_column=\"seconds_in_bucket\",\n            period=f\"{window}i\",  # 'i' denotes index count (integer)\n            by=[\"stock_id\", \"date_id\"],\n            closed=\"left\",  # Adjust as needed\n        )\n\n        # Apply to basic and new features\n        for col in [\n            \"wap\",\n            \"imbalance_size\",\n            \"bid_price\",\n            \"ask_price\",\n            \"relative_wap_to_market\",\n            \"wap_momentum_5\",\n        ]:\n            df = df.join(\n                rolling_group.agg(pl.col(col).mean().alias(f\"{col}_mean_{window}\")),\n                on=[\"stock_id\", \"date_id\", \"seconds_in_bucket\"],\n                how=\"left\",\n            )\n\n    low_importance_cols = [\n        \"wap_mean_5\",\n        #\"imbalance_buy_sell_flag\",\n        \"imbalance_flag_to_size_squared\",\n        \"imbalance_size_mean_5\",\n        \"bid_price_mean_5\",\n        \"ask_price_mean_5\",\n        \"wap_momentum_5_mean_5\",\n        \"relative_wap_to_market_mean_5\",\n        \"volatility_rate_of_change\",\n        \"avg_market_liquidity\",\n        \"seconds_in_bucket_squared\",\n        \"order_imbalance_over_time\"\n        # Add more columns as needed\n    ]\n\n    existing_cols = df.columns\n\n    # Drop columns only if they exist in DataFrame\n    cols_to_drop = [col for col in low_importance_cols if col in existing_cols]\n\n    \n    prices = [\"reference_price\", \"far_price\", \"near_price\", \"ask_price\", \"bid_price\", \"wap\"]\n    sizes = [\"matched_size\", \"bid_size\", \"ask_size\", \"imbalance_size\"]\n\n   \n    df = df.with_columns(\n        (pl.col(\"ask_size\") + pl.col(\"bid_size\")).alias(\"volume\")\n    )\n    df = df.with_columns(\n        ((pl.col(\"ask_price\") + pl.col(\"bid_price\")) / 2).alias(\"mid_price\")\n    )\n    df = df.with_columns(\n        ((pl.col(\"bid_size\") - pl.col(\"ask_size\")) / (pl.col(\"bid_size\") + pl.col(\"ask_size\"))).alias(\"liquidity_imbalance\")\n    )\n\n    for c in combinations(prices, 2):\n        df = df.with_columns(\n            ((pl.col(c[0]) - pl.col(c[1])) / (pl.col(c[0]) + pl.col(c[1]))).alias(f\"{c[0]}_{c[1]}_imb\")\n        )\n    \n    \n    if cols_to_drop:\n        engineered_df = df.drop(cols_to_drop)\n    else:\n        engineered_df = df.to_pandas()\n\n    engineered_df = engineered_df.to_pandas()\n    \n  \n\n    print(\"# V2\")\n    engineered_df[\"imbalance_momentum\"] = engineered_df.groupby(['stock_id'])['imbalance_size'].diff(periods=1) / engineered_df['matched_size']\n \n\n    engineered_df['price_pressure'] = engineered_df['imbalance_size'] * (engineered_df['ask_price'] - engineered_df['bid_price'])\n    engineered_df['market_urgency'] = engineered_df['spread'] * engineered_df['liquidity_imbalance']\n    engineered_df['depth_pressure'] = (engineered_df['ask_size'] - engineered_df['bid_size']) * (engineered_df['far_price'] - engineered_df['near_price'])\n    for func in [\"mean\", \"std\", \"skew\", \"kurt\"]:\n        engineered_df[f\"all_prices_{func}\"] = engineered_df[prices].agg(func, axis=1)\n        engineered_df[f\"all_sizes_{func}\"] = engineered_df[sizes].agg(func, axis=1)\n        \n    for c in [['ask_price', 'bid_price', 'wap', 'reference_price'], sizes]:\n        triplet_feature = calculate_triplet_imbalance_numba(c, engineered_df)\n        engineered_df[triplet_feature.columns] = triplet_feature.values\n\n    print(\"V3\")\n    for col in ['matched_size', 'imbalance_size', 'reference_price', 'imbalance_buy_sell_flag']:\n        for window in [ 3]:\n            engineered_df[f\"{col}_shift_{window}\"] = engineered_df.groupby(['stock_id','date_id'])[col].shift(window)\n            engineered_df[f\"{col}_ret_{window}\"] = engineered_df.groupby(['stock_id','date_id'])[col].pct_change(window)\n            \n    for col in ['ask_price', 'bid_price', 'ask_size', 'bid_size']:\n        for window in [ 3]:\n            engineered_df[f\"{col}_diff_{window}\"] = engineered_df.groupby([\"stock_id\",'date_id'])[col].diff(window)\n    \n    engineered_df[\"dow\"] = engineered_df[\"date_id\"] % 5\n    engineered_df[\"seconds\"] = engineered_df[\"seconds_in_bucket\"] % 60\n    engineered_df[\"minute\"] = engineered_df[\"seconds_in_bucket\"] // 60\n    \n    \n    \n    cols_to_drop = ['near_price_squared', 'matched_size_bid_size_imbalance_size_imb2',\n       'minute', 'all_prices_kurt', 'imbalance_size_mean_10',\n       'avg_liquidity_by_market', 'imbalance_size',\n       'far_price_ask_price_imb', 'far_price_bid_price_imb',\n       'wap_mean_10', 'matched_size_ask_size_imbalance_size_imb2',\n       'all_prices_skew', 'all_sizes_skew', 'price_pressure',\n       'all_sizes_kurt', 'reference_price_far_price_imb',\n       'far_price_wap_imb', 'relative_liquidity_to_market',\n       'bid_size_diff_3', 'imbalance_buy_sell_flag',\n       'matched_size_squared', 'liquidity_ratio_change', 'price_impact',\n       'ask_price_bid_price_imb', 'seconds',\n       'bid_size_ask_size_imbalance_size_imb2', 'wap_acceleration',\n       'ask_size_diff_3', 'depth_pressure',\n       'imbalance_buy_sell_flag_ret_3','reference_price_far_price_imb', 'all_sizes_skew',\n       'matched_imbalance', 'relative_near_price_to_market',\n       'avg_market_imbalance', 'all_sizes_kurt',\n       'avg_market_price_skewness',\n       'relative_imbalance_to_matched_size_to_market', 'liquidity',\n       'far_price_wap_imb', 'relative_wap_to_market_mean_10',\n       'time_decay', 'matched_size', 'wap_velocity', 'bid_size_diff_3',\n       'imbalance_to_matched_size',\n       'bid_size_ask_size_imbalance_size_imb2', 'ask_size_diff_3',\n       'matched_size_X_near_price', 'bid_size', 'depth_pressure',\n       'relative_spread_to_market', 'seconds', 'spread',\n       'bid_price_mean_10', 'ask_price_mean_10', 'ask_size',\n       'order_imbalance_ratio', 'price_skewness', 'bid_ask_imbalance',\n       'price_skewness_rate_of_change', 'imbalance_size_mean_10',\n       'avg_liquidity_by_market', 'imbalance_buy_sell_flag',\n       'imbalance_size', 'size_diff_ask_to_wap', 'price_diff_ask_to_wap',\n       'price_diff_bid_to_wap', 'size_diff_bid_to_wap',\n       'imbalance_buy_sell_flag_ret_3', 'wap_mean_10', 'price_spread',\n       'wap_acceleration', 'liquidity_ratio_change',\n       'ask_price_bid_price_imb', 'near_price_squared',\n       'relative_liquidity_to_market', 'minute', 'liquidity_imbalance',\n       'price_impact', 'matched_size_squared', 'size_imbalance',\n       'ask_price_mean_1', 'wap_mean_1', 'wap_momentum_5_mean_3',\n       'wap_momentum_5_mean_2', 'wap_momentum_5_mean_1', 'wap_mean_3',\n       'wap_mean_2', 'bid_price_mean_2', 'bid_price_mean_3',\n       'imbalance_size_mean_2', 'ask_price_mean_3', 'ask_price_mean_2',\n       'imbalance_size_mean_1', 'relative_wap_to_market_mean_1',\n       'bid_price_mean_1', 'relative_wap_to_market_mean_2',\n       'relative_wap_to_market_mean_3', 'imbalance_size_mean_3']\n    cols_drop_in = [col for col in cols_to_drop if col in engineered_df.columns]\n    \n    engineered_df = engineered_df.drop(cols_drop_in, axis=1)\n\n            \n    return engineered_df.replace([np.inf, -np.inf], 0)","metadata":{"papermill":{"duration":0.042863,"end_time":"2023-10-29T17:16:44.832191","exception":false,"start_time":"2023-10-29T17:16:44.789328","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-11-05T20:57:10.729489Z","iopub.execute_input":"2023-11-05T20:57:10.730072Z","iopub.status.idle":"2023-11-05T20:57:10.799992Z","shell.execute_reply.started":"2023-11-05T20:57:10.730039Z","shell.execute_reply":"2023-11-05T20:57:10.798036Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"\n@njit(parallel=True)\ndef compute_triplet_imbalance(df_values, comb_indices):\n    num_rows = df_values.shape[0]\n    num_combinations = len(comb_indices)\n    imbalance_features = np.empty((num_rows, num_combinations))\n\n    for i in prange(num_combinations):\n        a, b, c = comb_indices[i]\n        for j in range(num_rows):\n            max_val = max(df_values[j, a], df_values[j, b], df_values[j, c])\n            min_val = min(df_values[j, a], df_values[j, b], df_values[j, c])\n            mid_val = df_values[j, a] + df_values[j, b] + df_values[j, c] - min_val - max_val\n            if mid_val == min_val:  # Prevent division by zero\n                imbalance_features[j, i] = np.nan\n            else:\n                imbalance_features[j, i] = (max_val - mid_val) / (mid_val - min_val)\n\n    return imbalance_features\n\n\ndef calculate_triplet_imbalance_numba(price, df):\n    # Convert DataFrame to numpy array for Numba compatibility\n    df_values = df[price].values\n    comb_indices = [(price.index(a), price.index(b), price.index(c)) for a, b, c in combinations(price, 3)]\n\n    # Calculate the triplet imbalance\n    features_array = compute_triplet_imbalance(df_values, comb_indices)\n\n    # Create a DataFrame from the results\n    columns = [f\"{a}_{b}_{c}_imb2\" for a, b, c in combinations(price, 3)]\n    features = pd.DataFrame(features_array, columns=columns)\n\n    return features","metadata":{"execution":{"iopub.status.busy":"2023-11-05T20:57:10.801837Z","iopub.execute_input":"2023-11-05T20:57:10.802208Z","iopub.status.idle":"2023-11-05T20:57:10.930410Z","shell.execute_reply.started":"2023-11-05T20:57:10.802181Z","shell.execute_reply":"2023-11-05T20:57:10.928958Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"def feat_engineering(df_train):\n    weights = [\n    0.004, 0.001, 0.002, 0.006, 0.004, 0.004, 0.002, 0.006, 0.006, 0.002, 0.002, 0.008,\n    0.006, 0.002, 0.008, 0.006, 0.002, 0.006, 0.004, 0.002, 0.004, 0.001, 0.006, 0.004,\n    0.002, 0.002, 0.004, 0.002, 0.004, 0.004, 0.001, 0.001, 0.002, 0.002, 0.006, 0.004,\n    0.004, 0.004, 0.006, 0.002, 0.002, 0.04 , 0.002, 0.002, 0.004, 0.04 , 0.002, 0.001,\n    0.006, 0.004, 0.004, 0.006, 0.001, 0.004, 0.004, 0.002, 0.006, 0.004, 0.006, 0.004,\n    0.006, 0.004, 0.002, 0.001, 0.002, 0.004, 0.002, 0.008, 0.004, 0.004, 0.002, 0.004,\n    0.006, 0.002, 0.004, 0.004, 0.002, 0.004, 0.004, 0.004, 0.001, 0.002, 0.002, 0.008,\n    0.02 , 0.004, 0.006, 0.002, 0.02 , 0.002, 0.002, 0.006, 0.004, 0.002, 0.001, 0.02,\n    0.006, 0.001, 0.002, 0.004, 0.001, 0.002, 0.006, 0.006, 0.004, 0.006, 0.001, 0.002,\n    0.004, 0.006, 0.006, 0.001, 0.04 , 0.006, 0.002, 0.004, 0.002, 0.002, 0.006, 0.002,\n    0.002, 0.004, 0.006, 0.006, 0.002, 0.002, 0.008, 0.006, 0.004, 0.002, 0.006, 0.002,\n    0.004, 0.006, 0.002, 0.004, 0.001, 0.004, 0.002, 0.004, 0.008, 0.006, 0.008, 0.002,\n    0.004, 0.002, 0.001, 0.004, 0.004, 0.004, 0.006, 0.008, 0.004, 0.001, 0.001, 0.002,\n    0.006, 0.004, 0.001, 0.002, 0.006, 0.004, 0.006, 0.008, 0.002, 0.002, 0.004, 0.002,\n    0.04 , 0.002, 0.002, 0.004, 0.002, 0.002, 0.006, 0.02 , 0.004, 0.002, 0.006, 0.02,\n    0.001, 0.002, 0.006, 0.004, 0.006, 0.004, 0.004, 0.004, 0.004, 0.002, 0.004, 0.04,\n    0.002, 0.008, 0.002, 0.004, 0.001, 0.004, 0.006, 0.004,\n    ]\n    df = df_train.copy()\n    weights = {int(k):v for k,v in enumerate(weights)}\n    df[\"stock_weights\"] = df[\"stock_id\"].map(weights)\n\n    df[\"volume\"] = df.eval(\"ask_size + bid_size\")\n    df[\"mid_price\"] = df.eval(\"(ask_price + bid_price) / 2\")\n    df[\"liquidity_imbalance\"] = df.eval(\"(bid_size-ask_size)/(bid_size+ask_size)\")\n    df[\"matched_imbalance\"] = df.eval(\"(imbalance_size-matched_size)/(matched_size+imbalance_size)\")\n    df[\"size_imbalance\"] = df.eval(\"bid_size / ask_size\")\n    \n    prices = [\"reference_price\", \"far_price\", \"near_price\", \"ask_price\", \"bid_price\", \"wap\"]\n    sizes = [\"matched_size\", \"bid_size\", \"ask_size\", \"imbalance_size\"]\n    \n    for c in combinations(prices, 2):\n        df[f\"{c[0]}_{c[1]}_imb\"] = df.eval(f\"({c[0]} - {c[1]})/({c[0]} + {c[1]})\")\n\n    for c in [['ask_price', 'bid_price', 'wap', 'reference_price'], sizes]:\n        triplet_feature = calculate_triplet_imbalance_numba(c, df)\n        df[triplet_feature.columns] = triplet_feature.values\n        \n\n    df[\"weighted_wap\"] = df[\"stock_weights\"] * df[\"wap\"]\n    df['wap_momentum'] = df.groupby('stock_id')['weighted_wap'].pct_change(periods=6)\n    df[\"imbalance_momentum\"] = df.groupby(['stock_id'])['imbalance_size'].diff(periods=1) / df['matched_size']\n    df[\"price_spread\"] = df[\"ask_price\"] - df[\"bid_price\"]\n    df[\"spread_intensity\"] = df.groupby(['stock_id'])['price_spread'].diff()\n    df['price_pressure'] = df['imbalance_size'] * (df['ask_price'] - df['bid_price'])\n    df['market_urgency'] = df['price_spread'] * df['liquidity_imbalance']\n    df['depth_pressure'] = (df['ask_size'] - df['bid_size']) * (df['far_price'] - df['near_price'])\n    df['spread_depth_ratio'] = (df['ask_price'] - df['bid_price']) / (df['bid_size'] + df['ask_size'])\n    df['mid_price_movement'] = df['mid_price'].diff(periods=5).apply(lambda x: 1 if x > 0 else (-1 if x < 0 else 0))\n    df['micro_price'] = ((df['bid_price'] * df['ask_size']) + (df['ask_price'] * df['bid_size'])) / (df['bid_size'] + df['ask_size'])\n    df['relative_spread'] = (df['ask_price'] - df['bid_price']) / df['wap']\n    \n    \n\n    \n    for func in [\"mean\", \"std\", \"skew\", \"kurt\"]:\n        df[f\"all_prices_{func}\"] = df[prices].agg(func, axis=1)\n        df[f\"all_sizes_{func}\"] = df[sizes].agg(func, axis=1)\n        \n    df = df.replace([np.inf, -np.inf], 0)\n    \n    df[\"seconds\"] = df[\"seconds_in_bucket\"] % 60\n    df[\"minute\"] = df[\"seconds_in_bucket\"] // 60\n    \n    df = pl.DataFrame(df)\n    df = df.sort([\"stock_id\", \"date_id\", \"seconds_in_bucket\"])\n\n    df = df.with_columns(\n        (pl.col(\"seconds_in_bucket\") / 10).cast(pl.Int32).alias(\"seconds_in_bucket\")\n    )\n    print(\"rolling mean...\")\n    \n    list_cols = list(df.columns)\n    list_cols_ma = [col for col in list_cols if col not in ['stock_id', 'date_id', 'seconds_in_bucket', \n       'imbalance_buy_sell_flag', 'reference_price', 'matched_size',\n       'far_price', 'near_price', 'bid_price', 'bid_size', 'ask_price',\n       'ask_size', 'wap', 'target', 'time_id', 'row_id','seconds','minute','mid_price_movement','stock_weights'] ]\n    \n    for window in [4,8]:\n        if TRAIN:\n            print(f\"Processing window size: {window}\")\n        rolling_group = df.group_by_rolling(\n            index_column=\"seconds_in_bucket\",\n            period=f\"{window}i\",  # 'i' denotes index count (integer)\n            by=[\"stock_id\", \"date_id\"],\n            closed=\"left\",  # Adjust as needed\n        )\n\n        # Apply to basic and new features\n        for col in list_cols_ma:\n            df = df.join(\n                rolling_group.agg(pl.col(col).mean().alias(f\"{col}_mean_{window}\")),\n                on=[\"stock_id\", \"date_id\", \"seconds_in_bucket\"],\n                how=\"left\",\n            )\n            if window > 6:\n                df = df.join(\n                    rolling_group.agg(pl.col(col).std().alias(f\"{col}_std_{window}\")),\n                    on=[\"stock_id\", \"date_id\", \"seconds_in_bucket\"],\n                    how=\"left\",\n                )\n                \n        \n    df = df.to_pandas()\n    \n    list_files_feat_importance = ['/kaggle/working/feat_impor_optiver_trading_at_the_close_20231102_22_19_40.csv']\n\n    tot_cols = [col for col in df.columns if col not in list(aggregate_feature_importance( list_files_feat_importance).tail(100)['feat'])]\n\n    return df[tot_cols]","metadata":{"execution":{"iopub.status.busy":"2023-11-05T20:57:10.931963Z","iopub.execute_input":"2023-11-05T20:57:10.932447Z","iopub.status.idle":"2023-11-05T20:57:10.982813Z","shell.execute_reply.started":"2023-11-05T20:57:10.932417Z","shell.execute_reply":"2023-11-05T20:57:10.981775Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"if TRAIN:\n    df_train = feat_engineering(df_train_raw)\n    \n    df_train = reduce_mem_usage(df_train, verbose=1)","metadata":{"papermill":{"duration":0.020101,"end_time":"2023-10-29T17:16:44.864351","exception":false,"start_time":"2023-10-29T17:16:44.844250","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-11-05T20:57:10.986533Z","iopub.execute_input":"2023-11-05T20:57:10.986824Z","iopub.status.idle":"2023-11-05T21:02:46.422069Z","shell.execute_reply.started":"2023-11-05T20:57:10.986801Z","shell.execute_reply":"2023-11-05T21:02:46.421272Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"rolling mean...\nProcessing window size: 4\nProcessing window size: 8\n","output_type":"stream"},{"name":"stderr","text":"\u001b[32m2023-11-05 21:02:46.405\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mreduce_mem_usage\u001b[0m:\u001b[36m33\u001b[0m - \u001b[1mMemory usage of dataframe is 5814.46 MB\u001b[0m\n\u001b[32m2023-11-05 21:02:46.414\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mreduce_mem_usage\u001b[0m:\u001b[36m35\u001b[0m - \u001b[1mMemory usage after optimization is: 2857.28 MB\u001b[0m\n\u001b[32m2023-11-05 21:02:46.416\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mreduce_mem_usage\u001b[0m:\u001b[36m37\u001b[0m - \u001b[1mDecreased by 50.86%\u001b[0m\n","output_type":"stream"}]},{"cell_type":"code","source":"#df_train.to_csv(\"train_processed.csv\", index = False)","metadata":{"execution":{"iopub.status.busy":"2023-11-05T21:02:46.423431Z","iopub.execute_input":"2023-11-05T21:02:46.423920Z","iopub.status.idle":"2023-11-05T21:02:46.466059Z","shell.execute_reply.started":"2023-11-05T21:02:46.423890Z","shell.execute_reply":"2023-11-05T21:02:46.464762Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"#engineered_df = df_train.copy()\n#numerical_cols = list(engineered_df.select_dtypes(include=['number']).columns)\n#list_cols = [col for col in numerical_cols if col not in ['stock_id', 'date_id', 'seconds_in_bucket']]\n#correlation_matrix = engineered_df[list_cols].corr()\n#high_correlation = correlation_matrix[correlation_matrix > 0.8]\n\n# Or, for the highest N correlations\n#N = 30\n#highest_correlations = correlation_matrix.unstack().sort_values(ascending=False).drop_duplicates()\n#highest_N_correlations = highest_correlations.head(N)","metadata":{"execution":{"iopub.status.busy":"2023-11-05T21:02:46.467856Z","iopub.execute_input":"2023-11-05T21:02:46.468169Z","iopub.status.idle":"2023-11-05T21:02:46.505457Z","shell.execute_reply.started":"2023-11-05T21:02:46.468144Z","shell.execute_reply":"2023-11-05T21:02:46.504365Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n\n\ndef time_series_split(X, n_splits, n_test_splits, embargo_td=2):\n    factorized_indices = np.unique(X[\"factorized\"])\n\n    # Compute the fold boundaries\n    fold_bounds = [\n        (fold[0], fold[-1] + 1) for fold in np.array_split(factorized_indices, n_splits)\n    ]\n\n    # Create the list of all tests test_fold_bounds that will become the test sets\n    selected_fold_bounds = list(itt.combinations(fold_bounds, n_test_splits))\n\n    # Reverse to start the testing from the most recent part of the dataset\n    selected_fold_bounds.reverse()\n\n    for fold_bound_list in selected_fold_bounds:\n        test_factorized_indices = np.empty(0)\n        test_fold_bounds = []\n\n        for fold_start, fold_end in fold_bound_list:\n            # Records the boundaries of the current test split\n            if not test_fold_bounds or fold_start != test_fold_bounds[-1][-1]:\n                test_fold_bounds.append((fold_start, fold_end))\n            elif fold_start == test_fold_bounds[-1][-1]:\n                test_fold_bounds[-1] = (test_fold_bounds[-1][0], fold_end)\n\n            test_factorized_indices = np.union1d(\n                test_factorized_indices, factorized_indices[fold_start:fold_end]\n            ).astype(int)\n\n        # Compute the train set indices\n        train_indices = np.setdiff1d(factorized_indices, test_factorized_indices)\n\n        # Purge and embargo can be added here if needed\n        # ...\n\n        yield train_indices, test_factorized_indices\n\n\n# # Example usage:\n","metadata":{"execution":{"iopub.status.busy":"2023-11-05T21:02:46.506793Z","iopub.execute_input":"2023-11-05T21:02:46.507177Z","iopub.status.idle":"2023-11-05T21:02:46.553691Z","shell.execute_reply.started":"2023-11-05T21:02:46.507145Z","shell.execute_reply":"2023-11-05T21:02:46.551313Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"if TRAIN:\n    col_split = \"date_id\"\n    df_train.sort_values([col_split], inplace=True)\n    df_train.reset_index(drop=True, inplace=True)\n    df_train[\"factorized\"] = pd.factorize(df_train[col_split])[0]\n    \n    list_cols_drop = [\"date_id\",\"stock_id\"]\n    df_train.drop(list_cols_drop, axis=1, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2023-11-05T21:02:46.556158Z","iopub.execute_input":"2023-11-05T21:02:46.556767Z","iopub.status.idle":"2023-11-05T21:02:52.278761Z","shell.execute_reply.started":"2023-11-05T21:02:46.556724Z","shell.execute_reply":"2023-11-05T21:02:52.277968Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if DEBUG:\n    n_estimators_min =n_estimators_max= 50\nelse:\n    n_estimators_min = 500\n    n_estimators_max = 500\n    ","metadata":{"execution":{"iopub.status.busy":"2023-11-05T21:02:52.279818Z","iopub.execute_input":"2023-11-05T21:02:52.280210Z","iopub.status.idle":"2023-11-05T21:02:52.314995Z","shell.execute_reply.started":"2023-11-05T21:02:52.280188Z","shell.execute_reply":"2023-11-05T21:02:52.313910Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gpu_switch = \"OFF\"\nn_splits = 5\nn_test_split = 1\nembargo_td = 100\n\nn_repeats = 1\nnbrnd_erly_stp = 130\n\ncv_mthd = \"KF\"\n\n# Cross-Validation Setup\nif TRAIN:\n    all_cv = {\"KF\": KFold(n_splits=n_splits, shuffle=True, random_state=state)}\n    cv = all_cv[cv_mthd]\n\n    model_params_dict = {\n        \"LGBMR\": {\n            \"static_params\": {\n                \"device\": \"gpu\" if gpu_switch == \"ON\" else \"cpu\",\n                \"objective\": \"mae\",\n                \"boosting_type\": \"gbdt\",\n                \"random_state\": state,\n                \"n_jobs\" : 4,\n                \"verbose\": -1,\n                \"importance_type\" : \"gain\",\n            },\n            \"dynamic_params\": {\n                \"n_estimators\": {\n                    \"type\": \"int\",\n                    \"low\": n_estimators_min,\n                    \"high\": n_estimators_max,\n                },\n                \"learning_rate\": {\n                    \"type\": \"float\",\n                    \"low\": 0.01,\n                    \"high\": 0.05,\n                },\n                \"max_depth\": {\"type\": \"int\", \"low\": 20, \"high\": 70},\n                \"num_leaves\": {\n                    \"type\": \"int\",\n                    \"low\": 60,\n                    \"high\": 300,\n                },\n                \"min_child_samples\": {\n                    \"type\": \"int\",\n                    \"low\": 20,\n                    \"high\": 40,\n                },\n                \"subsample\": {\n                    \"type\": \"float\",\n                    \"low\": 0.7,\n                    \"high\": 1,\n                },\n                \"colsample_bytree\": {\n                    \"type\": \"float\",\n                    \"low\": 1,\n                    \"high\": 1,\n                },\n                \"min_split_gain\": {\n                    \"type\": \"float\",\n                    \"low\": 0,\n                    \"high\": 1,\n                },\n                \"reg_alpha\": {\n                    \"type\": \"float\",\n                    \"low\": 0,\n                    \"high\": 1,\n                },\n                \"reg_lambda\": {\n                    \"type\": \"float\",\n                    \"low\": 0,\n                    \"high\": 1,\n                },\n            },\n        },\n    }\n\n    dict_models = {\"LGBMR\": LGBMR}\n\n    log_model = True\n\n    experiment_date_str = datetime.now().strftime(\"%Y%m%d_%H_%M_%S\")\n    experiment_purpose = \"optiver_trading_at_the_close\"\n    experiment_name = f\"{experiment_purpose}_{experiment_date_str}\"\n\n    mlflow.set_experiment(experiment_name)\n","metadata":{"execution":{"iopub.status.busy":"2023-11-05T21:02:52.316270Z","iopub.execute_input":"2023-11-05T21:02:52.316560Z","iopub.status.idle":"2023-11-05T21:02:52.382283Z","shell.execute_reply.started":"2023-11-05T21:02:52.316530Z","shell.execute_reply":"2023-11-05T21:02:52.381037Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stderr","text":"2023/11/05 21:02:52 INFO mlflow.tracking.fluent: Experiment with name 'optiver_trading_at_the_close_20231105_21_02_52' does not exist. Creating a new experiment.\n","output_type":"stream"}]},{"cell_type":"code","source":"\n\n\ndef objective(trial, df_train):\n    try:\n        print(f\"trial: {trial.number}\")\n        with mlflow.start_run() as run:\n            mlflow.log_param(\"cv_mthd\", cv_mthd)\n            mlflow.set_tag(\"experiment_purpose\", experiment_purpose)\n            mlflow.set_tag(\"experiment_name\", experiment_name)\n            mlflow.set_tag(\"version_nb\", VERSION_NB)\n            for model_name, model_class in dict_models.items():\n                model = create_model(\n                    trial,\n                    dict_models[model_name],\n                    model_params_dict[model_name][\"static_params\"],\n                    model_params_dict[model_name][\"dynamic_params\"],\n                )\n                mae_list = []\n\n                log_training_details(logger, model, trial, model_name)\n\n                for fold_n, (train_indices, test_indices) in enumerate(time_series_split(df_train, n_splits = n_splits, n_test_splits = n_test_split)\n                ):\n                  \n                    with mlflow.start_run(\n                        run_name=f\"fold_{fold_n+1}\", nested=True\n                    ) as nested_run:\n                        mask_train = df_train[\"factorized\"].isin(list(train_indices))\n                        mask_test = df_train[\"factorized\"].isin(list(test_indices))\n\n                        # Filter based on the 'factorized' field\n                        y_train = df_train.loc[mask_train, \"target\"].squeeze()\n                        y_val = df_train.loc[mask_test, \"target\"].squeeze()\n                        X_train = df_train[mask_train].drop([\"target\",\"factorized\"], axis=1)\n                        X_val = df_train[mask_test].drop([\"target\",\"factorized\"], axis=1)\n\n                        mlflow.log_param(\"training_data_rows\", X_train.shape[0])\n                        mlflow.log_param(\"training_data_columns\", X_train.shape[1])\n                        \n                        \n\n                        model.fit(\n                            X_train,\n                            y_train,\n                            eval_set=[(X_val, y_val)],\n                            eval_metric=\"mae\",\n                            callbacks=[\n                                lgbm.callback.early_stopping(stopping_rounds=100),\n                                lgbm.callback.log_evaluation(period=100),\n                            ],\n                            \n                        )\n\n                        log_feature_importance(\n                            trial.number,\n                            model,\n                            X_train,\n                            fold_n,\n                            experiment_purpose,\n                            experiment_date_str,\n                        )\n\n                        fold_mae = model.best_score_[\"valid_0\"][\"l1\"]\n                        print(model.best_score_)\n                        mae_list.append(fold_mae)\n                        logger.info(f\"{fold_n + 1:<5} {'|':<2} {fold_mae:<20}\")\n\n                      \n                        mlflow.log_param(\"fold_number\", fold_n + 1)\n                        mlflow.log_param(\"model_name\", model_name)\n                        mlflow.log_param(\"log_model\", log_model)\n\n                        params_to_log = model.get_params()\n                        mlflow.log_params(params_to_log)\n\n                        if log_model:\n                            current_time_str = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n                            model_log_name = (\n                                f\"{model_name}_{trial.number}_{current_time_str}\"\n                            )\n\n                            mlflow.log_param(\"model_log_name\", model_log_name)\n\n                            mlflow.sklearn.log_model(model, model_log_name)\n\n                            mlflow.log_param(\"run_time\", current_time_str)\n\n                        nested_run_id = nested_run.info.run_id\n                        model_path = f\"{path_experiments_storage}/{run.info.experiment_id}/{nested_run_id}/artifacts/{model_log_name}/model.pkl\"\n                        mlflow.log_param(\"model_path\", model_path)\n                avg_mae = sum(mae_list) / len(mae_list)\n\n                mlflow.log_param(\"model_name\", model_name)\n                mlflow.log_param(\"mae\", avg_mae)\n\n                return avg_mae\n\n    except Exception as e:\n        logger.error(f\"An exception occurred: {e}\")\n        return float(\"inf\")","metadata":{"_cell_guid":"334e81a6-b09b-4a3d-96a1-fe0278deff13","_uuid":"4288a49c-b472-4af2-b2bb-95cf0cd2bd6d","collapsed":false,"jupyter":{"outputs_hidden":false},"papermill":{"duration":0.038424,"end_time":"2023-10-29T17:16:44.947628","exception":false,"start_time":"2023-10-29T17:16:44.909204","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-11-05T21:02:52.383895Z","iopub.execute_input":"2023-11-05T21:02:52.387604Z","iopub.status.idle":"2023-11-05T21:02:52.434945Z","shell.execute_reply.started":"2023-11-05T21:02:52.387566Z","shell.execute_reply":"2023-11-05T21:02:52.433332Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"# Run the Optuna study\nif TRAIN:\n    study = optuna.create_study(\n        direction=\"minimize\",\n        study_name=\"Your Study Name\",\n        load_if_exists=True,\n    )\n    study.optimize(lambda trial: objective(trial, df_train), n_trials=N_TRIALS)","metadata":{"execution":{"iopub.status.busy":"2023-11-05T21:02:52.436539Z","iopub.execute_input":"2023-11-05T21:02:52.437000Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":"\u001b[32m2023-11-05 21:02:52.565\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mutils\u001b[0m:\u001b[36mlog_training_details\u001b[0m:\u001b[36m296\u001b[0m - \u001b[1m\u001b[34mTraining model: LGBMR\u001b[0m\u001b[0m\n\u001b[32m2023-11-05 21:02:52.567\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mutils\u001b[0m:\u001b[36mlog_training_details\u001b[0m:\u001b[36m300\u001b[0m - \u001b[1m\u001b[32mTrial 0    | n_estimators: 500 | learning_rate: 0.04168763748081836 | max_depth: 29 | num_leaves: 196 | min_child_samples: 34 | subsample: 0.7347951501746435 | colsample_bytree: 1.0 | min_split_gain: 0.042359060349905886 | reg_alpha: 0.3909708947059364 | reg_lambda: 0.2557216548335325\u001b[0m\u001b[0m\n\u001b[32m2023-11-05 21:02:52.569\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mutils\u001b[0m:\u001b[36mlog_training_details\u001b[0m:\u001b[36m308\u001b[0m - \u001b[1mFold  |  MAE                 \u001b[0m\n\u001b[32m2023-11-05 21:02:52.571\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mutils\u001b[0m:\u001b[36mlog_training_details\u001b[0m:\u001b[36m309\u001b[0m - \u001b[1m----- |  --------------------\u001b[0m\n","output_type":"stream"},{"name":"stdout","text":"trial: 0\nTraining until validation scores don't improve for 100 rounds\n[100]\tvalid_0's l1: 5.93959\n[200]\tvalid_0's l1: 5.93288\n[300]\tvalid_0's l1: 5.93199\n[400]\tvalid_0's l1: 5.93206\n","output_type":"stream"},{"name":"stderr","text":"\u001b[32m2023-11-05 21:18:29.145\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mobjective\u001b[0m:\u001b[36m64\u001b[0m - \u001b[1m1     |  5.931904796751411   \u001b[0m\n","output_type":"stream"},{"name":"stdout","text":"Early stopping, best iteration is:\n[318]\tvalid_0's l1: 5.9319\ndefaultdict(<class 'collections.OrderedDict'>, {'valid_0': OrderedDict([('l1', 5.931904796751411)])})\nTraining until validation scores don't improve for 100 rounds\n[100]\tvalid_0's l1: 6.30634\n[200]\tvalid_0's l1: 6.29789\n[300]\tvalid_0's l1: 6.29652\n[400]\tvalid_0's l1: 6.29637\n","output_type":"stream"},{"name":"stderr","text":"\u001b[32m2023-11-05 21:35:34.738\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mobjective\u001b[0m:\u001b[36m64\u001b[0m - \u001b[1m2     |  6.296165824143439   \u001b[0m\n","output_type":"stream"},{"name":"stdout","text":"Early stopping, best iteration is:\n[380]\tvalid_0's l1: 6.29617\ndefaultdict(<class 'collections.OrderedDict'>, {'valid_0': OrderedDict([('l1', 6.296165824143439)])})\nTraining until validation scores don't improve for 100 rounds\n[100]\tvalid_0's l1: 6.36937\n[200]\tvalid_0's l1: 6.36082\n[300]\tvalid_0's l1: 6.35949\n","output_type":"stream"}]},{"cell_type":"code","source":"# Run the Optuna study\nif TRAIN:\n    study = optuna.create_study(\n        direction=\"minimize\",\n        study_name=\"Your Study Name\",\n        load_if_exists=True,\n    )\n    study.optimize(lambda trial: objective(trial, df_train), n_trials=N_TRIALS)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def experiments_data(list_experiment_id = None, save_df = None, list_columns = None):\n    \"\"\"\n    Ogni volta che viene chiamata questa funzione legge tutti gli esperimenti e ritorna una nuova versione del file con tutti gli esperimenti storicizzati\n    \"\"\"\n    experiments = client.search_experiments()\n    all_runs_data = []\n    for exp in experiments:\n        experiment_id = exp.experiment_id\n        if (list_experiment_id == None) or (experiment_id in list_experiment_id):\n        \n            run_infos = client.search_runs(experiment_ids=[experiment_id])\n\n            for run_info in run_infos:\n                run_data = {\n                    \"experiment_id\": experiment_id,\n                    \"experiment_name\": exp.name,\n                    \"run_id\": run_info.info.run_id,\n                }\n\n                # Add metrics to run_data\n                for key, value in run_info.data.metrics.items():\n                    run_data[f\"{key}\"] = value\n\n                # Add params to run_data\n                for key, value in run_info.data.params.items():\n                    run_data[f\"{key}\"] = value\n\n                all_runs_data.append(run_data)\n        \n    df_runs_new = pd.DataFrame(all_runs_data)\n    \n\n    \n    df_runs_new = df_runs_new[~df_runs_new[\"fold_number\"].isna()]\n    \n    if list_columns:\n        df_runs_new = df_runs_new[list_columns]\n        \n    if save_df:\n        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M\")\n        csv_filename = f\"df_runs_{timestamp}.csv\"\n        df_runs_new.to_csv(csv_filename, index=False)\n\n        print(f\"DataFrame saved to {csv_filename}, Shape: {df_unique.shape}\")\n\n    return df_runs_new","metadata":{"_cell_guid":"51030a77-aba3-469f-9c08-7963bc8a09d2","_uuid":"6ff7d358-62f0-4ac9-b028-d1c920bb4eaf","collapsed":false,"jupyter":{"outputs_hidden":false},"papermill":{"duration":0.022231,"end_time":"2023-10-29T17:16:45.128848","exception":false,"start_time":"2023-10-29T17:16:45.106617","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if TRAIN:\n    df_exp  = experiments_data(list_experiment_id = None, save_df = None, list_columns = None)\n    list_base_cols = ['run_time','experiment_id','model_name','fold_number','mae','training_data_rows','training_data_columns'] \n    list_dynamic_params = list(model_params_dict[\"LGBMR\"]['dynamic_params'].keys())\n    \n    \n    \n    list_cols_exp = list_base_cols + list_dynamic_params +['model_path']\n    \n    \n    df_exp = df_exp[list_cols_exp]\n    \n    df_exp['run_time'] = pd.to_datetime(df_exp['run_time'], format='%Y%m%d_%H%M%S', errors='coerce')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"TRAIN = True","metadata":{"papermill":{"duration":0.011661,"end_time":"2023-10-29T17:16:45.369942","exception":false,"start_time":"2023-10-29T17:16:45.358281","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if TRAIN:\n    list_files_feat_importance = ['/kaggle/working/feat_impor_optiver_trading_at_the_close_20231102_22_19_40.csv']\n\n\n\n    aggregate_feature_importance( list_files_feat_importance)\n","metadata":{"papermill":{"duration":0.011597,"end_time":"2023-10-29T17:16:45.393480","exception":false,"start_time":"2023-10-29T17:16:45.381883","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#aggregate_feature_importance( list_files_feat_importance).shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#aggregate_feature_importance( list_files_feat_importance).tail(60)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def ensemble_predict(model_paths, X_test):\n    models = []\n    predictions = []\n\n    # Load models based on full artifact paths\n    for model_path in model_paths:\n        try:\n            # If using direct path to pkl\n            if model_path.endswith(\".pkl\"):\n                model = joblib.load(model_path)\n            else:\n                print(f\"Unsupported model format for {model_path}. Skipping.\")\n                continue  # Skip this iteration\n\n            models.append(model)\n        except Exception as e:\n            print(f\"Failed to load model at {model_path}. Error: {e}\")\n\n    # Make predictions\n    for model in models:\n        try:\n            pred = model.predict(X_test)\n            predictions.append(pred)\n        except Exception as e:\n            print(f\"Failed to make prediction with model. Error: {e}\")\n\n    # Average predictions\n    if len(predictions) > 0:\n        ensemble_pred = np.mean(predictions, axis=0)\n    else:\n        print(\"No valid models loaded. Cannot make ensemble predictions.\")\n        ensemble_pred = None\n\n    return ensemble_pred","metadata":{"_cell_guid":"95e6c8a8-62ed-4412-944a-5b08508702fc","_uuid":"60c6783c-1e42-43ca-ba32-bda313deb585","collapsed":false,"jupyter":{"outputs_hidden":false},"papermill":{"duration":0.02353,"end_time":"2023-10-29T17:16:45.534872","exception":false,"start_time":"2023-10-29T17:16:45.511342","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"papermill":{"duration":0.012259,"end_time":"2023-10-29T17:16:45.559669","exception":false,"start_time":"2023-10-29T17:16:45.547410","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n\nif TRAIN:\n    #model_paths = list(df_exp[df_exp['run_time'] >= pd.to_datetime(\"2023-11-01 22:10:54\")]['model_path'])\n    model_paths = list(df_exp['model_path'])\n    if not os.path.exists(models_dir):\n        os.makedirs(models_dir)\n\n    for model_path in model_paths:\n        print(f\"Checking if model path exists: {model_path}\")\n\n        if not os.path.exists(model_path):\n            print(f\"File does not exist: {model_path}\")\n            continue  # Skip to the next iteration\n\n        specific_part = model_path.split(\"/\")[-2]\n        dest_path = os.path.join(models_dir, f\"{specific_part}.pkl\")\n        if not os.path.exists(dest_path):\n            print(f\"Copying from {model_path} to {dest_path}\")\n            shutil.copy(model_path, dest_path)\n        else:\n            print(f\"File {dest_path} already exists. Skipping copy.\")\n\n    zipf = zipfile.ZipFile(\n        f\"/kaggle/working/{models_dir}.zip\", \"w\", zipfile.ZIP_DEFLATED\n    )\n\n    # Navigate through the folder and add each file to the ZIP\n    for root, dirs, files in os.walk(f\"/kaggle/working/{models_dir}\"):\n        for file in files:\n            zipf.write(\n                os.path.join(root, file),\n                os.path.relpath(\n                    os.path.join(root, file), f\"/kaggle/working/{models_dir}\"\n                ),\n            )\n\n    zipf.close()","metadata":{"papermill":{"duration":0.025212,"end_time":"2023-10-29T17:16:45.655277","exception":false,"start_time":"2023-10-29T17:16:45.630065","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"papermill":{"duration":0.012391,"end_time":"2023-10-29T17:16:45.680337","exception":false,"start_time":"2023-10-29T17:16:45.667946","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_paths = []\nmodels_dir_input = models_dir.replace(\"_\", \"-\")\ndirectory = f\"/kaggle/input/{models_dir_input}\"\n\n# Check if the directory exists\nif os.path.exists(directory):\n    # Traverse the directory and collect file paths\n    for filename in os.listdir(directory):\n        full_path = os.path.join(directory, filename)\n\n        # Check if the item is a file (and not a sub-directory)\n        if os.path.isfile(full_path):\n            model_paths.append(full_path)\nelse:\n    print(f\"The directory {directory} does not exist.\")\n\n# Print or return the list of file paths\nprint(\"List of file paths:\", model_paths)","metadata":{"papermill":{"duration":0.026356,"end_time":"2023-10-29T17:16:45.718974","exception":false,"start_time":"2023-10-29T17:16:45.692618","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"papermill":{"duration":0.01212,"end_time":"2023-10-29T17:16:45.743760","exception":false,"start_time":"2023-10-29T17:16:45.731640","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Assuming X_test for predict\n# ensemble_predictions = ensemble_predict(model_paths, df_test, mlflow_client)","metadata":{"_cell_guid":"79aa495a-7580-497c-b913-8941cf7d0c61","_uuid":"b3efe16f-ca61-4000-94d7-568364d0e11b","collapsed":false,"jupyter":{"outputs_hidden":false},"papermill":{"duration":0.019319,"end_time":"2023-10-29T17:16:45.775373","exception":false,"start_time":"2023-10-29T17:16:45.756054","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import optiver2023\n\nenv = optiver2023.make_env()\niter_test = env.iter_test()","metadata":{"_cell_guid":"d4bfb979-2758-4fe9-8947-399b1c3a574c","_uuid":"bef1a199-0988-4743-813e-99d4308fc9bb","collapsed":false,"jupyter":{"outputs_hidden":false},"papermill":{"duration":0.04228,"end_time":"2023-10-29T17:16:45.830037","exception":false,"start_time":"2023-10-29T17:16:45.787757","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"counter = 0\nfor test, revealed_targets, sample_prediction in iter_test:\n    # df_test_raw = pl.DataFrame(test)\n\n    feat = feat_engineering(test)\n    \n    list_cols_drop = [\"date_id\",\"stock_id\",\"row_id\"]\n    feat.drop(list_cols_drop, axis=1, inplace=True)\n\n    # feat = df_test.to_pandas()\n\n    #list_cols_drop = [\"stock_id\", \"date_id\", \"row_id\"]\n    #feat = feat.drop(list_cols_drop, axis=1)\n\n    sample_prediction[\"target\"] = ensemble_predict(model_paths, feat)\n    env.predict(sample_prediction)\n    counter += 1","metadata":{"_cell_guid":"d30f3862-dca2-4242-9db0-ab9750118622","_uuid":"efd9073c-aaef-4135-bf7c-1e892e9aa831","collapsed":false,"jupyter":{"outputs_hidden":false},"papermill":{"duration":105.467568,"end_time":"2023-10-29T17:18:31.345732","exception":false,"start_time":"2023-10-29T17:16:45.878164","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"papermill":{"duration":0.022343,"end_time":"2023-10-29T17:18:31.445104","exception":false,"start_time":"2023-10-29T17:18:31.422761","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"papermill":{"duration":0.02223,"end_time":"2023-10-29T17:18:31.490205","exception":false,"start_time":"2023-10-29T17:18:31.467975","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"papermill":{"duration":0.022172,"end_time":"2023-10-29T17:18:31.534807","exception":false,"start_time":"2023-10-29T17:18:31.512635","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"papermill":{"duration":0.022005,"end_time":"2023-10-29T17:18:31.579172","exception":false,"start_time":"2023-10-29T17:18:31.557167","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"_cell_guid":"18344629-3cbc-4244-ac34-c8af6b88b0d8","_uuid":"667a4d83-d235-446e-a72c-cf02718a1a7b","collapsed":false,"jupyter":{"outputs_hidden":false},"papermill":{"duration":0.022032,"end_time":"2023-10-29T17:18:31.623976","exception":false,"start_time":"2023-10-29T17:18:31.601944","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"papermill":{"duration":0.022491,"end_time":"2023-10-29T17:18:31.668688","exception":false,"start_time":"2023-10-29T17:18:31.646197","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#clean_directory_except_one('/kaggle/working/', 'submission.csv')","metadata":{"papermill":{"duration":0.030147,"end_time":"2023-10-29T17:18:31.721197","exception":false,"start_time":"2023-10-29T17:18:31.691050","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"_cell_guid":"3ef4ae54-1bb4-4cc3-a7da-a6691a272d30","_uuid":"495a4e04-554d-4ef1-919d-62bc59f089c4","collapsed":false,"jupyter":{"outputs_hidden":false},"papermill":{"duration":0.022039,"end_time":"2023-10-29T17:18:31.765571","exception":false,"start_time":"2023-10-29T17:18:31.743532","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"papermill":{"duration":0.023232,"end_time":"2023-10-29T17:18:31.811015","exception":false,"start_time":"2023-10-29T17:18:31.787783","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"_cell_guid":"5498ae4b-62e8-4050-91e7-075fc549380f","_uuid":"ce9653b2-7dff-4c5b-af77-5f0ef88b6e86","collapsed":false,"jupyter":{"outputs_hidden":false},"papermill":{"duration":0.022222,"end_time":"2023-10-29T17:18:31.856624","exception":false,"start_time":"2023-10-29T17:18:31.834402","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"_cell_guid":"92a8ed18-3a63-464e-9d62-17cea3baed59","_uuid":"0b017968-3b5a-4465-820e-e89659b10afd","collapsed":false,"jupyter":{"outputs_hidden":false},"papermill":{"duration":0.022485,"end_time":"2023-10-29T17:18:31.901283","exception":false,"start_time":"2023-10-29T17:18:31.878798","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"_cell_guid":"f9d19e8d-0074-4ae2-adfb-4c039b700215","_uuid":"b6096d27-d452-4593-9832-d15bfbd83e1b","collapsed":false,"jupyter":{"outputs_hidden":false},"papermill":{"duration":0.022606,"end_time":"2023-10-29T17:18:31.946090","exception":false,"start_time":"2023-10-29T17:18:31.923484","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"_cell_guid":"855f8db3-e344-4d4d-b577-36499775f352","_uuid":"fde5edb9-d23d-41ac-92c6-593f299230c2","collapsed":false,"jupyter":{"outputs_hidden":false},"papermill":{"duration":0.021902,"end_time":"2023-10-29T17:18:31.990239","exception":false,"start_time":"2023-10-29T17:18:31.968337","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"_cell_guid":"82330d89-f35e-4cae-876c-de852438527d","_uuid":"f90f329d-b882-464c-84fc-74a815cfe1cd","collapsed":false,"jupyter":{"outputs_hidden":false},"papermill":{"duration":0.021851,"end_time":"2023-10-29T17:18:32.034578","exception":false,"start_time":"2023-10-29T17:18:32.012727","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"_cell_guid":"ed96eca6-9765-4c90-a6bf-3071518ba2f0","_uuid":"ca1d48b9-5537-491b-9de2-57360edd9844","collapsed":false,"jupyter":{"outputs_hidden":false},"papermill":{"duration":0.022409,"end_time":"2023-10-29T17:18:32.079037","exception":false,"start_time":"2023-10-29T17:18:32.056628","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"_cell_guid":"06b24b0a-6515-48c6-b4a0-240865cfc1ea","_uuid":"5e9b8536-f812-432a-ba05-c2f0b0f8994f","collapsed":false,"jupyter":{"outputs_hidden":false},"papermill":{"duration":0.021691,"end_time":"2023-10-29T17:18:32.123180","exception":false,"start_time":"2023-10-29T17:18:32.101489","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]}]}