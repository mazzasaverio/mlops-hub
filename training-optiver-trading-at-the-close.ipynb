{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c5ad15f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-29T17:13:54.418207Z",
     "iopub.status.busy": "2023-10-29T17:13:54.416807Z",
     "iopub.status.idle": "2023-10-29T17:13:55.119145Z",
     "shell.execute_reply": "2023-10-29T17:13:55.117970Z"
    },
    "papermill": {
     "duration": 0.724823,
     "end_time": "2023-10-29T17:13:55.121952",
     "exception": false,
     "start_time": "2023-10-29T17:13:54.397129",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "    # Constants\n",
    "TRAIN = False\n",
    "OVERWRITE = False\n",
    "DEBUG = False\n",
    "DEBUG_SAMPLE = 10000\n",
    "\n",
    "N_TRIALS = 10\n",
    "\n",
    "VERSION_NB = 2\n",
    "\n",
    "state = 42\n",
    "\n",
    "download_kaggle_data = False\n",
    "\n",
    "# External general-purpose modules\n",
    "import os\n",
    "import shutil\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "import glob\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "from dotenv import load_dotenv\n",
    "from joblib import dump\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "# Setting pandas options and warning filters\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "faad09fe",
   "metadata": {
    "_cell_guid": "3bdaa15b-0b9e-4b3b-9c47-58e6b9b18b5e",
    "_uuid": "206043b9-f1d5-4691-b03a-935f2177086d",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2023-10-29T17:13:55.159931Z",
     "iopub.status.busy": "2023-10-29T17:13:55.159163Z",
     "iopub.status.idle": "2023-10-29T17:13:55.180023Z",
     "shell.execute_reply": "2023-10-29T17:13:55.178974Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.043465,
     "end_time": "2023-10-29T17:13:55.183271",
     "exception": false,
     "start_time": "2023-10-29T17:13:55.139806",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/working\n"
     ]
    }
   ],
   "source": [
    "# Setting up the project directory path\n",
    "path_project_dir = os.getcwd()\n",
    "if path_project_dir not in [\"/kaggle/working\", \"/content\"]:\n",
    "    path_project_dir = os.getenv(\"ROOT_PATH\")\n",
    "\n",
    "print(path_project_dir)\n",
    "\n",
    "# Imports and setup for training\n",
    "if TRAIN:\n",
    "    # Install packages and import logging libraries\n",
    "    if path_project_dir == '/kaggle/working':\n",
    "        !pip install loguru mlflow optuna > /dev/null\n",
    "        from utils_1 import get_data, log_training_details, clean_directory_except_one\n",
    "\n",
    "    from utils_1 import log_feature_importance\n",
    "    \n",
    "    from loguru import logger\n",
    "    import mlflow\n",
    "    import optuna\n",
    "    from optuna.integration.mlflow import MLflowCallback\n",
    "    from mlflow.tracking import MlflowClient\n",
    "    import zipfile\n",
    "    \n",
    "    from tqdm import tqdm\n",
    "\n",
    "    # Import machine learning libraries\n",
    "    from lightgbm import log_evaluation, early_stopping, LGBMRegressor as LGBMR\n",
    "    from sklearn.model_selection import KFold\n",
    "    from xgboost import XGBRegressor as XGBR\n",
    "\n",
    "    # Set logging\n",
    "    logger.add(\"logs.log\", format=\"{time:YYYY-MM-DD HH:mm} | {level} | {message}\")\n",
    "    optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "    warnings.filterwarnings(\"ignore\", category=optuna.exceptions.ExperimentalWarning)\n",
    "    \n",
    "    # Auto-reload modules\n",
    "    %load_ext autoreload\n",
    "    %autoreload 2\n",
    "\n",
    "    # Initialize MLflow callback\n",
    "    mlflow_callback = MLflowCallback(\n",
    "        tracking_uri=mlflow.get_tracking_uri(), metric_name=\"mae\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d9e0074e",
   "metadata": {
    "_cell_guid": "9765420b-f8ab-46ba-8b4c-e4487eb04b76",
    "_uuid": "c4174134-1441-41f8-adbf-f03ad3f7d7dc",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2023-10-29T17:13:55.221492Z",
     "iopub.status.busy": "2023-10-29T17:13:55.221081Z",
     "iopub.status.idle": "2023-10-29T17:13:55.232588Z",
     "shell.execute_reply": "2023-10-29T17:13:55.231495Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.033919,
     "end_time": "2023-10-29T17:13:55.235071",
     "exception": false,
     "start_time": "2023-10-29T17:13:55.201152",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if path_project_dir == \"/kaggle/working\":\n",
    "    path_data_project_dir = \"/kaggle/input/optiver-trading-at-the-close\"\n",
    "    path_experiments_storage = os.path.join(path_project_dir, \"experiments_storage\")\n",
    "\n",
    "    path_dataset_train_raw = \"/kaggle/input/optiver-trading-at-the-close/train.csv\"\n",
    "    path_dataset_test_raw = (\n",
    "        \"/kaggle/input/optiver-trading-at-the-close/example_test_files/test.csv\"\n",
    "    )\n",
    "\n",
    "    path_dataset_processed = \"/kaggle/working/processed_data\"\n",
    "    path_dataset_train = os.path.join(path_dataset_processed, \"train.csv\")\n",
    "    path_dataset_test = os.path.join(path_dataset_processed, \"test.csv\")\n",
    "\n",
    "else:\n",
    "    name_folder_data_project = \"kaggle_optiver_trading_at_the_close\"\n",
    "\n",
    "    path_data_dir = os.path.join(path_project_dir, \"data\")\n",
    "    path_dataset_train_raw = os.path.join(\n",
    "        path_data_dir, \"kaggle_optiver_trading_at_the_close/raw\", \"train.csv\"\n",
    "    )\n",
    "    path_dataset_processed = os.path.join(\n",
    "        path_data_dir, \"kaggle_optiver_trading_at_the_close/processed\"\n",
    "    )\n",
    "\n",
    "    path_data_project_dir = os.path.join(path_data_dir, name_folder_data_project)\n",
    "\n",
    "    path_config_dir = os.path.join(path_project_dir, \"config\")\n",
    "    path_config_train = os.path.join(path_config_dir, \"train_config.yaml\")\n",
    "\n",
    "    path_experiments_storage = os.path.join(\n",
    "        path_data_project_dir, \"experiments_storage\"\n",
    "    )\n",
    "\n",
    "    if download_kaggle_data:\n",
    "        dataset_name = \"ravi20076/optiver-memoryreduceddatasets\"\n",
    "        kaggle_json_path = os.path.join(path_project_dir, \"kaggle.json\")\n",
    "        get_data(\n",
    "            kaggle_json_path,\n",
    "            path_data_project_dir,\n",
    "            dataset_name=dataset_name,\n",
    "            specific_file=None,\n",
    "        )\n",
    "\n",
    "    file_name_df_train = \"train.csv\"\n",
    "    file_name_df_test = \"test.csv\"\n",
    "\n",
    "    path_dataset_train = os.path.join(path_data_project_dir, file_name_df_train)\n",
    "    path_dataset_test = os.path.join(path_data_project_dir, file_name_df_test)\n",
    "\n",
    "if TRAIN:\n",
    "    mlflow.set_tracking_uri(path_experiments_storage)\n",
    "    client = MlflowClient()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df38f425",
   "metadata": {
    "_cell_guid": "b8d09bc0-2c22-4c0c-9142-ab698ac4e6ea",
    "_uuid": "858fa67e-742c-461a-9c05-5ec2e4554328",
    "papermill": {
     "duration": 0.017387,
     "end_time": "2023-10-29T17:13:55.270312",
     "exception": false,
     "start_time": "2023-10-29T17:13:55.252925",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Constants and Settings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3d41b7ef",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-29T17:13:55.307421Z",
     "iopub.status.busy": "2023-10-29T17:13:55.307039Z",
     "iopub.status.idle": "2023-10-29T17:13:55.313608Z",
     "shell.execute_reply": "2023-10-29T17:13:55.312402Z"
    },
    "papermill": {
     "duration": 0.028082,
     "end_time": "2023-10-29T17:13:55.315937",
     "exception": false,
     "start_time": "2023-10-29T17:13:55.287855",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if TRAIN:\n",
    "    if not os.path.exists(path_dataset_processed):\n",
    "        os.makedirs(path_dataset_processed)\n",
    "\n",
    "    if not os.path.exists(path_dataset_train) or OVERWRITE:\n",
    "        df_train = pd.read_csv(path_dataset_train_raw)\n",
    "    else:\n",
    "        df_train = pd.read_csv(path_dataset_train)\n",
    "\n",
    "    if DEBUG:\n",
    "        df_train = df_train[df_train[\"stock_id\"].isin([0, 1, 2, 3, 4, 5])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7384a64d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-29T17:13:55.353584Z",
     "iopub.status.busy": "2023-10-29T17:13:55.352242Z",
     "iopub.status.idle": "2023-10-29T17:13:55.358727Z",
     "shell.execute_reply": "2023-10-29T17:13:55.357503Z"
    },
    "papermill": {
     "duration": 0.027435,
     "end_time": "2023-10-29T17:13:55.361109",
     "exception": false,
     "start_time": "2023-10-29T17:13:55.333674",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if TRAIN:\n",
    "    # Dropping rows with null targets:-\n",
    "    drop_idx = df_train.loc[df_train[\"target\"].isna(), \"target\"].index.to_list()\n",
    "    df_train = df_train.drop(drop_idx, axis=0)\n",
    "    # df_train.drop(\"row_id\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a71c840f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-29T17:13:55.399376Z",
     "iopub.status.busy": "2023-10-29T17:13:55.398647Z",
     "iopub.status.idle": "2023-10-29T17:13:55.435930Z",
     "shell.execute_reply": "2023-10-29T17:13:55.434672Z"
    },
    "papermill": {
     "duration": 0.059318,
     "end_time": "2023-10-29T17:13:55.438826",
     "exception": false,
     "start_time": "2023-10-29T17:13:55.379508",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def feat_engineering(df_train):\n",
    "    df_train_pol = pl.DataFrame(df_train)\n",
    "    df = df_train_pol\n",
    "    # 7. Handle Missing Values\n",
    "    df = df.with_columns(\n",
    "        [\n",
    "            pl.col(\"far_price\").fill_null(strategy=\"forward\").alias(\"far_price\"),\n",
    "            pl.col(\"near_price\").fill_null(strategy=\"forward\").alias(\"near_price\"),\n",
    "        ]\n",
    "    )\n",
    "    # Level 1 Features\n",
    "    level_one_features = [\n",
    "        (pl.col(\"imbalance_size\") / pl.col(\"matched_size\")).alias(\n",
    "            \"imbalance_to_matched_size\"\n",
    "        ),\n",
    "        (pl.col(\"imbalance_size\") * pl.col(\"imbalance_buy_sell_flag\")).alias(\n",
    "            \"imbalance_flag_to_size\"\n",
    "        ),\n",
    "        (pl.col(\"ask_price\") - pl.col(\"bid_price\")).alias(\"spread\"),\n",
    "        (pl.col(\"bid_size\") - pl.col(\"ask_size\")).alias(\"bid_ask_imbalance\"),\n",
    "        (pl.col(\"bid_size\") / pl.col(\"ask_size\")).alias(\"liquidity\"),\n",
    "        (pl.col(\"bid_price\") - pl.col(\"wap\")).alias(\"price_diff_bid_to_wap\"),\n",
    "        (pl.col(\"ask_price\") - pl.col(\"wap\")).alias(\"price_diff_ask_to_wap\"),\n",
    "        (pl.col(\"bid_size\") - pl.col(\"wap\")).alias(\"size_diff_bid_to_wap\"),\n",
    "        (pl.col(\"ask_size\") - pl.col(\"wap\")).alias(\"size_diff_ask_to_wap\"),\n",
    "        (pl.col(\"wap\") - pl.col(\"wap\").shift(1).over([\"stock_id\", \"date_id\"])).alias(\n",
    "            \"wap_velocity\"\n",
    "        ),\n",
    "        (\n",
    "            pl.col(\"wap\") / pl.col(\"wap\").shift(5).over([\"stock_id\", \"date_id\"]) - 1\n",
    "        ).alias(\"wap_momentum_5\"),\n",
    "        (\n",
    "            pl.col(\"wap\")\n",
    "            .std()\n",
    "            .over([\"stock_id\", \"date_id\"])\n",
    "            .alias(\"short_term_volatility\")\n",
    "        ),\n",
    "        (\n",
    "            (\n",
    "                pl.col(\"imbalance_size\")\n",
    "                / (pl.col(\"matched_size\") + pl.col(\"imbalance_size\"))\n",
    "            ).alias(\"price_impact\")\n",
    "        ),\n",
    "        (\n",
    "            (pl.col(\"bid_size\") - pl.col(\"ask_size\"))\n",
    "            / (pl.col(\"bid_size\") + pl.col(\"ask_size\"))\n",
    "        ).alias(\"order_imbalance_ratio\"),\n",
    "        (\n",
    "            (pl.col(\"ask_price\") - pl.col(\"bid_price\"))\n",
    "            / (pl.col(\"ask_price\") + pl.col(\"bid_price\"))\n",
    "        ).alias(\"price_skewness\"),\n",
    "        (pl.col(\"seconds_in_bucket\") / 600).alias(\"time_decay\"),\n",
    "    ]\n",
    "\n",
    "    # Level 2 Features\n",
    "    level_two_features = [\n",
    "        (\n",
    "            pl.col(\"wap_velocity\")\n",
    "            - pl.col(\"wap_velocity\").shift(1).over([\"stock_id\", \"date_id\"])\n",
    "        ).alias(\"wap_acceleration\"),\n",
    "        (\n",
    "            pl.col(\"short_term_volatility\").shift(1).over([\"stock_id\", \"date_id\"])\n",
    "            - pl.col(\"short_term_volatility\")\n",
    "        ).alias(\"volatility_rate_of_change\"),\n",
    "        (\n",
    "            (\n",
    "                pl.col(\"liquidity\")\n",
    "                - pl.col(\"liquidity\").shift(1).over([\"stock_id\", \"date_id\"])\n",
    "            )\n",
    "            / pl.col(\"liquidity\").shift(1).over([\"stock_id\", \"date_id\"])\n",
    "        ).alias(\"liquidity_ratio_change\"),\n",
    "        (\n",
    "            (\n",
    "                pl.col(\"order_imbalance_ratio\")\n",
    "                - pl.col(\"order_imbalance_ratio\").shift(1).over([\"stock_id\", \"date_id\"])\n",
    "            )\n",
    "            / pl.col(\"order_imbalance_ratio\").shift(1).over([\"stock_id\", \"date_id\"])\n",
    "        ).alias(\"order_imbalance_over_time\"),\n",
    "        (\n",
    "            (\n",
    "                pl.col(\"price_skewness\")\n",
    "                - pl.col(\"price_skewness\").shift(1).over([\"stock_id\", \"date_id\"])\n",
    "            )\n",
    "            / pl.col(\"price_skewness\").shift(1).over([\"stock_id\", \"date_id\"])\n",
    "        ).alias(\"price_skewness_rate_of_change\"),\n",
    "    ]\n",
    "\n",
    "    # Level 3 Features\n",
    "    level_three_aggregations = [\n",
    "        pl.col(\"wap\").mean().alias(\"avg_wap_by_market\"),\n",
    "        pl.col(\"near_price\").mean().alias(\"avg_near_price_by_market\"),\n",
    "        pl.col(\"matched_size\").mean().alias(\"avg_matched_size_by_market\"),\n",
    "        pl.col(\"imbalance_to_matched_size\")\n",
    "        .mean()\n",
    "        .alias(\"avg_imbalance_to_matched_size_by_market\"),\n",
    "        pl.col(\"spread\").mean().alias(\"avg_spread_by_market\"),\n",
    "        pl.col(\"liquidity\").mean().alias(\"avg_liquidity_by_market\"),\n",
    "        pl.col(\"short_term_volatility\").mean().alias(\"avg_market_volatility\"),\n",
    "        pl.col(\"order_imbalance_ratio\").mean().alias(\"avg_market_imbalance\"),\n",
    "        pl.col(\"liquidity\").mean().alias(\"avg_market_liquidity\"),\n",
    "        pl.col(\"price_impact\").mean().alias(\"avg_market_price_impact\"),\n",
    "        pl.col(\"price_skewness\").mean().alias(\"avg_market_price_skewness\"),\n",
    "    ]\n",
    "\n",
    "    # Adding all features and performing join operation\n",
    "    df = df.with_columns(level_one_features)\n",
    "    df = df.with_columns(level_two_features)\n",
    "    group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
    "        *level_three_aggregations\n",
    "    )\n",
    "    df = df.join(group_by_market, on=[\"date_id\", \"seconds_in_bucket\"], how=\"left\")\n",
    "\n",
    "\n",
    "    polynomial_and_interaction_features = [\n",
    "        (pl.col(\"seconds_in_bucket\") * pl.col(\"near_price\")).alias(\n",
    "            \"seconds_in_bucket_X_near_price\"\n",
    "        ),\n",
    "        (pl.col(\"matched_size\") * pl.col(\"near_price\")).alias(\n",
    "            \"matched_size_X_near_price\"\n",
    "        ),\n",
    "        (pl.col(\"near_price\") ** 2).alias(\"near_price_squared\"),\n",
    "        (pl.col(\"matched_size\") ** 2).alias(\"matched_size_squared\"),\n",
    "        (pl.col(\"seconds_in_bucket\") * pl.col(\"imbalance_flag_to_size\")).alias(\n",
    "            \"seconds_in_bucket_X_imbalance_flag_to_size\"\n",
    "        ),\n",
    "        (pl.col(\"seconds_in_bucket\") ** 2).alias(\"seconds_in_bucket_squared\"),\n",
    "        (pl.col(\"imbalance_flag_to_size\") ** 2).alias(\"imbalance_flag_to_size_squared\"),\n",
    "    ]\n",
    "\n",
    "    # Relative to Market Features\n",
    "    relative_to_market_features = [\n",
    "        (pl.col(\"wap\") / pl.col(\"avg_wap_by_market\")).alias(\"relative_wap_to_market\"),\n",
    "        (pl.col(\"near_price\") / pl.col(\"avg_near_price_by_market\")).alias(\n",
    "            \"relative_near_price_to_market\"\n",
    "        ),\n",
    "        (pl.col(\"matched_size\") / pl.col(\"avg_matched_size_by_market\")).alias(\n",
    "            \"relative_matched_size_to_market\"\n",
    "        ),\n",
    "        (\n",
    "            pl.col(\"imbalance_to_matched_size\")\n",
    "            / pl.col(\"avg_imbalance_to_matched_size_by_market\")\n",
    "        ).alias(\"relative_imbalance_to_matched_size_to_market\"),\n",
    "        (pl.col(\"spread\") / pl.col(\"avg_spread_by_market\")).alias(\n",
    "            \"relative_spread_to_market\"\n",
    "        ),\n",
    "        (pl.col(\"liquidity\") / pl.col(\"avg_liquidity_by_market\")).alias(\n",
    "            \"relative_liquidity_to_market\"\n",
    "        ),\n",
    "    ]\n",
    "\n",
    "    # Combine all Level 4 features and add them to the DataFrame\n",
    "    all_level_four_features = (\n",
    "        polynomial_and_interaction_features + relative_to_market_features\n",
    "    )\n",
    "    df = df.with_columns(all_level_four_features)\n",
    "\n",
    "    for window in [5, 10]:\n",
    "        rolling_group = df.group_by_rolling(\n",
    "            index_column=\"seconds_in_bucket\",\n",
    "            period=f\"{window}i\",  # 'i' denotes index count (integer)\n",
    "            by=[\"stock_id\", \"date_id\"],\n",
    "            closed=\"left\",  # Adjust as needed\n",
    "        )\n",
    "\n",
    "        # Apply to basic and new features\n",
    "        for col in [\n",
    "            \"wap\",\n",
    "            \"imbalance_size\",\n",
    "            \"bid_price\",\n",
    "            \"ask_price\",\n",
    "            \"relative_wap_to_market\",\n",
    "            \"wap_momentum_5\",\n",
    "        ]:\n",
    "            df = df.join(\n",
    "                rolling_group.agg(pl.col(col).mean().alias(f\"{col}_mean_{window}\")),\n",
    "                on=[\"stock_id\", \"date_id\", \"seconds_in_bucket\"],\n",
    "                how=\"left\",\n",
    "            )\n",
    "\n",
    "    low_importance_cols = [\n",
    "        \"wap_mean_5\",\n",
    "        \"imbalance_buy_sell_flag\",\n",
    "        \"imbalance_flag_to_size_squared\",\n",
    "        \"imbalance_size_mean_5\",\n",
    "        \"bid_price_mean_5\",\n",
    "        \"ask_price_mean_5\",\n",
    "        \"wap_momentum_5_mean_5\",\n",
    "        \"relative_wap_to_market_mean_5\",\n",
    "        \"volatility_rate_of_change\"\n",
    "        # Add more columns as needed\n",
    "    ]\n",
    "\n",
    "    existing_cols = df.columns\n",
    "\n",
    "    # Drop columns only if they exist in DataFrame\n",
    "    cols_to_drop = [col for col in low_importance_cols if col in existing_cols]\n",
    "\n",
    "    if cols_to_drop:\n",
    "        engineered_df = df.drop(cols_to_drop)\n",
    "    else:\n",
    "        engineered_df = df.to_pandas()\n",
    "\n",
    "    engineered_df = engineered_df.to_pandas()\n",
    "\n",
    "    return engineered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "562bebdd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-29T17:13:55.477021Z",
     "iopub.status.busy": "2023-10-29T17:13:55.475686Z",
     "iopub.status.idle": "2023-10-29T17:13:55.481309Z",
     "shell.execute_reply": "2023-10-29T17:13:55.480089Z"
    },
    "papermill": {
     "duration": 0.026887,
     "end_time": "2023-10-29T17:13:55.483771",
     "exception": false,
     "start_time": "2023-10-29T17:13:55.456884",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if TRAIN:\n",
    "    engineered_df = feat_engineering(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "77a15628",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-29T17:13:55.522096Z",
     "iopub.status.busy": "2023-10-29T17:13:55.521679Z",
     "iopub.status.idle": "2023-10-29T17:13:55.528009Z",
     "shell.execute_reply": "2023-10-29T17:13:55.526767Z"
    },
    "papermill": {
     "duration": 0.027359,
     "end_time": "2023-10-29T17:13:55.530328",
     "exception": false,
     "start_time": "2023-10-29T17:13:55.502969",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if TRAIN:\n",
    "    list_cols_drop = [\"stock_id\", \"date_id\", \"row_id\", \"time_id\", \"target\"]\n",
    "\n",
    "    y_train = engineered_df[\"target\"]\n",
    "\n",
    "    X_train = engineered_df.drop(list_cols_drop, axis=1).copy()\n",
    "\n",
    "    y_train = y_train.loc[X_train.index].squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aec6e80c",
   "metadata": {
    "_cell_guid": "334e81a6-b09b-4a3d-96a1-fe0278deff13",
    "_uuid": "4288a49c-b472-4af2-b2bb-95cf0cd2bd6d",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2023-10-29T17:13:55.568514Z",
     "iopub.status.busy": "2023-10-29T17:13:55.568102Z",
     "iopub.status.idle": "2023-10-29T17:13:55.600306Z",
     "shell.execute_reply": "2023-10-29T17:13:55.599131Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.05445,
     "end_time": "2023-10-29T17:13:55.602850",
     "exception": false,
     "start_time": "2023-10-29T17:13:55.548400",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "gpu_switch = \"OFF\"\n",
    "n_splits = 4\n",
    "n_test_split = 1\n",
    "embargo_td = 100\n",
    "\n",
    "n_repeats = 1\n",
    "nbrnd_erly_stp = 130\n",
    "\n",
    "cv_mthd = \"KF\"\n",
    "\n",
    "# Cross-Validation Setup\n",
    "if TRAIN:\n",
    "    all_cv = {\"KF\": KFold(n_splits=n_splits, shuffle=True, random_state=state)}\n",
    "    cv = all_cv[cv_mthd]\n",
    "\n",
    "    model_params_dict = {\n",
    "        \"LGBMR\": {\n",
    "            \"static_params\": {\n",
    "                \"device\": \"gpu\" if gpu_switch == \"ON\" else \"cpu\",\n",
    "                \"objective\": \"regression_l1\",\n",
    "                \"boosting_type\": \"gbdt\",\n",
    "                \"random_state\": state,\n",
    "                \"verbose\": -1,\n",
    "                \"verbose_eval\": False,\n",
    "            },\n",
    "            \"dynamic_params\": {\n",
    "                \"n_estimators\": {\n",
    "                    \"type\": \"int\",\n",
    "                    \"low\": 620,\n",
    "                    \"high\": 730,\n",
    "                },\n",
    "                \"learning_rate\": {\n",
    "                    \"type\": \"float\",\n",
    "                    \"low\": 0.05,\n",
    "                    \"high\": 0.07,\n",
    "                },\n",
    "                \"max_depth\": {\"type\": \"int\", \"low\": 14, \"high\": 19},\n",
    "                \"num_leaves\": {\n",
    "                    \"type\": \"int\",\n",
    "                    \"low\": 65,\n",
    "                    \"high\": 85,\n",
    "                },\n",
    "                \"min_child_samples\": {\n",
    "                    \"type\": \"int\",\n",
    "                    \"low\": 71,\n",
    "                    \"high\": 74,\n",
    "                },\n",
    "                \"subsample\": {\n",
    "                    \"type\": \"float\",\n",
    "                    \"low\": 0.63,\n",
    "                    \"high\": 0.71,\n",
    "                },\n",
    "                \"colsample_bytree\": {\n",
    "                    \"type\": \"float\",\n",
    "                    \"low\": 0.74,\n",
    "                    \"high\": 0.77,\n",
    "                },\n",
    "                \"min_split_gain\": {\n",
    "                    \"type\": \"float\",\n",
    "                    \"low\": 0.08,\n",
    "                    \"high\": 0.11,\n",
    "                },\n",
    "                \"reg_alpha\": {\n",
    "                    \"type\": \"float\",\n",
    "                    \"low\": 0.7,\n",
    "                    \"high\": 1.3,\n",
    "                },\n",
    "                \"reg_lambda\": {\n",
    "                    \"type\": \"float\",\n",
    "                    \"low\": 1.9,\n",
    "                    \"high\": 3.0,\n",
    "                },\n",
    "            },\n",
    "        },\n",
    "    }\n",
    "\n",
    "    dict_models = {\"LGBMR\": LGBMR}\n",
    "\n",
    "    log_model = True\n",
    "\n",
    "    experiment_date_str = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    experiment_purpose = \"optiver_trading_at_the_close\"\n",
    "    experiment_name = f\"{experiment_purpose}_{experiment_date_str}\"\n",
    "\n",
    "    mlflow.set_experiment(experiment_name)\n",
    "\n",
    "\n",
    "def create_model(trial, model_class, static_params, dynamic_params):\n",
    "    dynamic_params_values = {}\n",
    "    for param_name, suggestions in dynamic_params.items():\n",
    "        suggestion_type = suggestions[\"type\"]\n",
    "        if suggestion_type == \"int\":\n",
    "            dynamic_params_values[param_name] = trial.suggest_int(\n",
    "                param_name, suggestions[\"low\"], suggestions[\"high\"]\n",
    "            )\n",
    "        elif suggestion_type == \"float\":\n",
    "            dynamic_params_values[param_name] = trial.suggest_float(\n",
    "                param_name, suggestions[\"low\"], suggestions[\"high\"]\n",
    "            )\n",
    "        elif suggestion_type == \"categorical\":\n",
    "            dynamic_params_values[param_name] = trial.suggest_categorical(\n",
    "                param_name, suggestions[\"choices\"]\n",
    "            )\n",
    "        elif suggestion_type == \"discrete_uniform\":\n",
    "            dynamic_params_values[param_name] = trial.suggest_discrete_uniform(\n",
    "                param_name, suggestions[\"low\"], suggestions[\"high\"], suggestions[\"q\"]\n",
    "            )\n",
    "        elif suggestion_type == \"loguniform\":\n",
    "            dynamic_params_values[param_name] = trial.suggest_loguniform(\n",
    "                param_name, suggestions[\"low\"], suggestions[\"high\"]\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported suggestion type: {suggestion_type}\")\n",
    "\n",
    "    model_params = {**static_params, **dynamic_params_values}\n",
    "    return model_class(**model_params)\n",
    "\n",
    "\n",
    "def objective(trial, X, y):\n",
    "    try:\n",
    "        with mlflow.start_run() as run:\n",
    "            mlflow.log_param(\"cv_mthd\", cv_mthd)\n",
    "            mlflow.set_tag(\"experiment_purpose\", experiment_purpose)\n",
    "            mlflow.set_tag(\"experiment_name\", experiment_name)\n",
    "            mlflow.set_tag(\"version_nb\",VERSION_NB)\n",
    "            for model_name, model_class in dict_models.items():\n",
    "                model = create_model(\n",
    "                    trial,\n",
    "                    dict_models[model_name],\n",
    "                    model_params_dict[model_name][\"static_params\"],\n",
    "                    model_params_dict[model_name][\"dynamic_params\"],\n",
    "                )\n",
    "                mae_list = []\n",
    "\n",
    "                log_training_details(logger, model, trial, model_name)\n",
    "\n",
    "                for fold_n, (train_idx, val_idx) in enumerate(cv.split(X, y)):\n",
    "                    with mlflow.start_run(\n",
    "                        run_name=f\"fold_{fold_n+1}\", nested=True\n",
    "                    ) as nested_run:\n",
    "                        X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "                        y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "\n",
    "                        mlflow.log_param(\"training_data_rows\", X_train.shape[0])\n",
    "                        mlflow.log_param(\"training_data_columns\", X_train.shape[1])\n",
    "\n",
    "                        model.fit(\n",
    "                            X_train,\n",
    "                            y_train,\n",
    "                            eval_set=[(X_val, y_val)],\n",
    "                            eval_metric=\"mae\",\n",
    "                            early_stopping_rounds=nbrnd_erly_stp,\n",
    "                            callbacks=[\n",
    "                                log_evaluation(0),\n",
    "                                early_stopping(nbrnd_erly_stp, verbose=False),\n",
    "                            ],\n",
    "                        )\n",
    "\n",
    "                        log_feature_importance(model, X, fold_n, experiment_purpose, experiment_date_str)\n",
    "\n",
    "                        fold_mae = model.best_score_[\"valid_0\"][\"l1\"]\n",
    "                        mae_list.append(fold_mae)\n",
    "                        logger.info(f\"{fold_n + 1:<5} {'|':<2} {fold_mae:<20}\")\n",
    "\n",
    "                        mlflow.log_metric(\"mae\", fold_mae)\n",
    "                        mlflow.log_param(\"fold_number\", fold_n + 1)\n",
    "                        mlflow.log_param(\"model_name\", model_name)\n",
    "                        mlflow.log_param(\"log_model\", log_model)\n",
    "\n",
    "                        params_to_log = model.get_params()\n",
    "                        mlflow.log_params(params_to_log)\n",
    "\n",
    "                        if log_model:\n",
    "                            current_time_str = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "                            model_log_name = (\n",
    "                                f\"{model_name}_{trial.number}_{current_time_str}\"\n",
    "                            )\n",
    "\n",
    "                            mlflow.log_param(\"model_log_name\", model_log_name)\n",
    "\n",
    "                            mlflow.sklearn.log_model(model, model_log_name)\n",
    "\n",
    "                            mlflow.log_param(\"run_time\", current_time_str)\n",
    "\n",
    "                        nested_run_id = nested_run.info.run_id\n",
    "                        model_path = f\"{path_experiments_storage}/{run.info.experiment_id}/{nested_run_id}/artifacts/{model_log_name}/model.pkl\"\n",
    "                        mlflow.log_param(\"model_path\", model_path)\n",
    "                avg_mae = sum(mae_list) / len(mae_list)\n",
    "\n",
    "                mlflow.log_param(\"model_name\", model_name)\n",
    "\n",
    "                return avg_mae\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"An exception occurred: {e}\")\n",
    "        return float(\"inf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f373b379",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-29T17:13:55.639611Z",
     "iopub.status.busy": "2023-10-29T17:13:55.639222Z",
     "iopub.status.idle": "2023-10-29T17:13:55.645599Z",
     "shell.execute_reply": "2023-10-29T17:13:55.644487Z"
    },
    "papermill": {
     "duration": 0.027222,
     "end_time": "2023-10-29T17:13:55.647787",
     "exception": false,
     "start_time": "2023-10-29T17:13:55.620565",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Run the Optuna study\n",
    "if TRAIN:\n",
    "    study = optuna.create_study(\n",
    "        direction=\"minimize\",\n",
    "        study_name=\"Your Study Name\",\n",
    "        load_if_exists=True,\n",
    "    )\n",
    "    study.optimize(lambda trial: objective(trial, X_train, y_train), n_trials=N_TRIALS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "574dd987",
   "metadata": {
    "papermill": {
     "duration": 0.076789,
     "end_time": "2023-10-29T17:13:55.741878",
     "exception": false,
     "start_time": "2023-10-29T17:13:55.665089",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "52609abd",
   "metadata": {
    "_cell_guid": "0a91a281-846a-4119-9fee-268d2f6e2195",
    "_uuid": "02e23e01-f0f7-426e-b170-55ac4273b88f",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2023-10-29T17:13:55.778360Z",
     "iopub.status.busy": "2023-10-29T17:13:55.777934Z",
     "iopub.status.idle": "2023-10-29T17:13:55.786986Z",
     "shell.execute_reply": "2023-10-29T17:13:55.785800Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.029979,
     "end_time": "2023-10-29T17:13:55.789215",
     "exception": false,
     "start_time": "2023-10-29T17:13:55.759236",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def save_and_version_df(df_new):\n",
    "    # Read existing df_runs CSV files and concatenate them with the new df_runs\n",
    "    existing_files = [\n",
    "        f for f in os.listdir() if f.startswith(\"df_runs_\") and f.endswith(\".csv\")\n",
    "    ]\n",
    "    dfs = [pd.read_csv(f) for f in existing_files]\n",
    "\n",
    "    for old_file, old_df in zip(existing_files, dfs):\n",
    "        print(f\"Removed old file: {old_file}, Shape: {old_df.shape}\")\n",
    "        os.remove(old_file)\n",
    "\n",
    "    dfs.append(df_new)\n",
    "    df_concatenated = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "    # Remove duplicates\n",
    "    df_unique = df_concatenated.drop_duplicates()\n",
    "\n",
    "    # Save the new concatenated and deduplicated df_runs to a new versioned CSV file\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M\")\n",
    "    csv_filename = f\"df_runs_{timestamp}.csv\"\n",
    "    df_unique.to_csv(csv_filename, index=False)\n",
    "\n",
    "    print(f\"DataFrame saved to {csv_filename}, Shape: {df_unique.shape}\")\n",
    "\n",
    "    return df_unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f70bcc77",
   "metadata": {
    "_cell_guid": "51030a77-aba3-469f-9c08-7963bc8a09d2",
    "_uuid": "6ff7d358-62f0-4ac9-b028-d1c920bb4eaf",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2023-10-29T17:13:55.825626Z",
     "iopub.status.busy": "2023-10-29T17:13:55.825245Z",
     "iopub.status.idle": "2023-10-29T17:13:55.834054Z",
     "shell.execute_reply": "2023-10-29T17:13:55.832972Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.029715,
     "end_time": "2023-10-29T17:13:55.836300",
     "exception": false,
     "start_time": "2023-10-29T17:13:55.806585",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def gather_runs_data():\n",
    "    experiments = client.search_experiments()\n",
    "    all_runs_data = []\n",
    "    for exp in experiments:\n",
    "        experiment_id = exp.experiment_id\n",
    "        run_infos = client.search_runs(experiment_ids=[experiment_id])\n",
    "\n",
    "        for run_info in run_infos:\n",
    "            run_data = {\n",
    "                \"experiment_id\": experiment_id,\n",
    "                \"experiment_name\": exp.name,\n",
    "                \"run_id\": run_info.info.run_id,\n",
    "            }\n",
    "\n",
    "            # Add metrics to run_data\n",
    "            for key, value in run_info.data.metrics.items():\n",
    "                run_data[f\"{key}\"] = value\n",
    "\n",
    "            # Add params to run_data\n",
    "            for key, value in run_info.data.params.items():\n",
    "                run_data[f\"{key}\"] = value\n",
    "\n",
    "            all_runs_data.append(run_data)\n",
    "\n",
    "    df_runs_new = pd.DataFrame(all_runs_data)\n",
    "\n",
    "\n",
    "    df_runs_unique = save_and_version_df(df_runs_new)\n",
    "\n",
    "    df_runs_unique = df_runs_unique[~df_runs_unique[\"model_name\"].isna()]\n",
    "\n",
    "    return df_runs_unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "88dec05c",
   "metadata": {
    "_cell_guid": "50c86d61-2198-45c1-8106-1ff027f1f87a",
    "_uuid": "6491f1de-d47e-4d76-8eb4-7cf869d5313a",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2023-10-29T17:13:55.872451Z",
     "iopub.status.busy": "2023-10-29T17:13:55.872033Z",
     "iopub.status.idle": "2023-10-29T17:13:55.878470Z",
     "shell.execute_reply": "2023-10-29T17:13:55.877398Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.02742,
     "end_time": "2023-10-29T17:13:55.881011",
     "exception": false,
     "start_time": "2023-10-29T17:13:55.853591",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "cols_id = [\"experiment_id\", \"run_id\"]\n",
    "cols_info = [\"training_data_rows\", \"training_data_columns\"]\n",
    "cols_metrics = [\"mae\"]\n",
    "cols_param_exp = [\"date_exp\", \"log_model\", \"cv_mthd\", \"fold_number\"]\n",
    "cols_others_info = [\n",
    "    \"experiment_name\",\n",
    "    \"model_path\",\n",
    "    \"device\",\n",
    "    \"n_jobs\",\n",
    "    \"importance_type\",\n",
    "    \"random_state\",\n",
    "    \"model_name\",\n",
    "    \"subsample_freq\",\n",
    "    \"verbose_eval\",\n",
    "    \"class_weight\",\n",
    "    \"model_log_name\",\n",
    "    \"verbose\",\n",
    "    \"silent\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "44ed8c71",
   "metadata": {
    "_cell_guid": "cb16e35c-caf0-4a29-803b-ce578697b66a",
    "_uuid": "d14695ff-57de-4e99-833e-5ef5e6f1038d",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2023-10-29T17:13:55.917508Z",
     "iopub.status.busy": "2023-10-29T17:13:55.917073Z",
     "iopub.status.idle": "2023-10-29T17:13:55.925899Z",
     "shell.execute_reply": "2023-10-29T17:13:55.924463Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.029768,
     "end_time": "2023-10-29T17:13:55.928167",
     "exception": false,
     "start_time": "2023-10-29T17:13:55.898399",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if TRAIN:\n",
    "    df_runs = gather_runs_data()\n",
    "    df_runs = df_runs.drop_duplicates()\n",
    "    cols_params = df_runs.columns.tolist()\n",
    "\n",
    "    for col in cols_param_exp + cols_metrics + cols_info + cols_id + cols_others_info:\n",
    "        cols_params.remove(col)\n",
    "\n",
    "    new_col_order = (\n",
    "        cols_param_exp\n",
    "        + cols_metrics\n",
    "        + cols_info\n",
    "        + cols_id\n",
    "        + cols_params\n",
    "        + cols_others_info\n",
    "    )\n",
    "\n",
    "    df_runs = df_runs[~df_runs[\"fold_number\"].isna()]\n",
    "    df_runs = df_runs[new_col_order]\n",
    "    \n",
    "    df_runs['experiment_time'] = df_runs['experiment_name'].apply(lambda x: x.split(\"_\")[-1])\n",
    "    df_runs['experiment_date'] = df_runs['experiment_name'].apply(lambda x: x.split(\"_\")[-2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e2299138",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-29T17:13:55.964849Z",
     "iopub.status.busy": "2023-10-29T17:13:55.964371Z",
     "iopub.status.idle": "2023-10-29T17:13:55.969322Z",
     "shell.execute_reply": "2023-10-29T17:13:55.968263Z"
    },
    "papermill": {
     "duration": 0.025878,
     "end_time": "2023-10-29T17:13:55.971570",
     "exception": false,
     "start_time": "2023-10-29T17:13:55.945692",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#df_runs[[\"date_exp\"] + cols_info + cols_metrics + cols_params].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e3fe55",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-29T17:06:26.885901Z",
     "iopub.status.busy": "2023-10-29T17:06:26.885111Z",
     "iopub.status.idle": "2023-10-29T17:06:26.935609Z",
     "shell.execute_reply": "2023-10-29T17:06:26.934616Z",
     "shell.execute_reply.started": "2023-10-29T17:06:26.885851Z"
    },
    "papermill": {
     "duration": 0.017028,
     "end_time": "2023-10-29T17:13:56.006197",
     "exception": false,
     "start_time": "2023-10-29T17:13:55.989169",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "802a71c3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-29T17:08:12.604440Z",
     "iopub.status.busy": "2023-10-29T17:08:12.603635Z",
     "iopub.status.idle": "2023-10-29T17:08:12.694417Z",
     "shell.execute_reply": "2023-10-29T17:08:12.693231Z",
     "shell.execute_reply.started": "2023-10-29T17:08:12.604404Z"
    },
    "papermill": {
     "duration": 0.017034,
     "end_time": "2023-10-29T17:13:56.040518",
     "exception": false,
     "start_time": "2023-10-29T17:13:56.023484",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc052fc",
   "metadata": {
    "papermill": {
     "duration": 0.01673,
     "end_time": "2023-10-29T17:13:56.074490",
     "exception": false,
     "start_time": "2023-10-29T17:13:56.057760",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0816de61",
   "metadata": {
    "papermill": {
     "duration": 0.016931,
     "end_time": "2023-10-29T17:13:56.108577",
     "exception": false,
     "start_time": "2023-10-29T17:13:56.091646",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf3060f",
   "metadata": {
    "papermill": {
     "duration": 0.016731,
     "end_time": "2023-10-29T17:13:56.142619",
     "exception": false,
     "start_time": "2023-10-29T17:13:56.125888",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6888a68",
   "metadata": {
    "papermill": {
     "duration": 0.016859,
     "end_time": "2023-10-29T17:13:56.176639",
     "exception": false,
     "start_time": "2023-10-29T17:13:56.159780",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d788428",
   "metadata": {
    "papermill": {
     "duration": 0.017001,
     "end_time": "2023-10-29T17:13:56.210942",
     "exception": false,
     "start_time": "2023-10-29T17:13:56.193941",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dc09763b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-29T17:13:56.247867Z",
     "iopub.status.busy": "2023-10-29T17:13:56.247480Z",
     "iopub.status.idle": "2023-10-29T17:13:56.260880Z",
     "shell.execute_reply": "2023-10-29T17:13:56.259793Z"
    },
    "papermill": {
     "duration": 0.034507,
     "end_time": "2023-10-29T17:13:56.263160",
     "exception": false,
     "start_time": "2023-10-29T17:13:56.228653",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def aggregate_feature_importance(path_to_csvs):\n",
    "    \"\"\"\n",
    "    Aggregates feature importances from multiple CSV files and calculates mean importance.\n",
    "    \"\"\"\n",
    "    all_files = glob.glob(f\"{path_to_csvs}/*.csv\")\n",
    "    list_of_dfs = [pd.read_csv(filename) for filename in all_files]\n",
    "\n",
    "    if not list_of_dfs:\n",
    "        print(\"No CSV files found.\")\n",
    "        return None\n",
    "\n",
    "    # Concatenate all Jdataframes\n",
    "    aggregated_df = pd.concat(list_of_dfs, ignore_index=True)\n",
    "\n",
    "    # Calculate mean importance for each feature\n",
    "    mean_importance = (\n",
    "        aggregated_df.groupby(\"Feature\")[\"Importance\"].mean().reset_index()\n",
    "    )\n",
    "    mean_importance = mean_importance.sort_values(by=\"Importance\", ascending=False)\n",
    "\n",
    "    return mean_importance\n",
    "\n",
    "\n",
    "def analyze_feature_importance(aggregated_df, top_n=100):\n",
    "    \"\"\"\n",
    "    Analyzes aggregated feature importances.\n",
    "    \"\"\"\n",
    "    if aggregated_df is None:\n",
    "        print(\"No aggregated DataFrame provided.\")\n",
    "        return None\n",
    "\n",
    "    top_n_features = aggregated_df.nlargest(top_n, \"Importance\")\n",
    "\n",
    "    return top_n_features\n",
    "\n",
    "\n",
    "def count_top_n_features(path_to_csvs, top_n=100):\n",
    "    \"\"\"\n",
    "    Counts how many times each feature appears in the top N most important features across multiple experiments.\n",
    "\n",
    "    Parameters:\n",
    "    path_to_csvs (str): Path to the folder containing feature importance CSV files.\n",
    "    top_n (int): The number of top features to consider.\n",
    "\n",
    "    Returns:\n",
    "    DataFrame: A DataFrame showing the count of appearances in the top N features for each feature.\n",
    "    \"\"\"\n",
    "\n",
    "    all_files = glob.glob(f\"{path_to_csvs}/*.csv\")\n",
    "    list_of_dfs = [pd.read_csv(filename) for filename in all_files]\n",
    "\n",
    "    if not list_of_dfs:\n",
    "        print(\"No CSV files found.\")\n",
    "        return None\n",
    "\n",
    "    feature_count = {}\n",
    "\n",
    "    # Count how many times each feature appears in the top N features\n",
    "    for df in list_of_dfs:\n",
    "        top_features = df.nlargest(top_n, \"Importance\")[\"Feature\"].tolist()\n",
    "        for feature in top_features:\n",
    "            if feature in feature_count:\n",
    "                feature_count[feature] += 1\n",
    "            else:\n",
    "                feature_count[feature] = 1\n",
    "\n",
    "    # Convert to DataFrame for easier analysis\n",
    "    feature_count_df = pd.DataFrame(\n",
    "        list(feature_count.items()), columns=[\"Feature\", \"Count\"]\n",
    "    )\n",
    "    feature_count_df = feature_count_df.sort_values(by=\"Count\", ascending=False)\n",
    "\n",
    "    return feature_count_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "76d52ec7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-29T17:13:56.300081Z",
     "iopub.status.busy": "2023-10-29T17:13:56.299648Z",
     "iopub.status.idle": "2023-10-29T17:13:56.305976Z",
     "shell.execute_reply": "2023-10-29T17:13:56.304791Z"
    },
    "papermill": {
     "duration": 0.027824,
     "end_time": "2023-10-29T17:13:56.308481",
     "exception": false,
     "start_time": "2023-10-29T17:13:56.280657",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Example usageJ\n",
    "if TRAIN:\n",
    "    path_to_csvs = \"/kaggle/working/\"\n",
    "    aggregated_df = aggregate_feature_importance(path_to_csvs)\n",
    "    top_n_features = analyze_feature_importance(aggregated_df)\n",
    "    list_features = [\n",
    "        col for col in list(top_n_features[\"Feature\"]) if col in engineered_df.columns\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4a106dd6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-29T17:13:56.345079Z",
     "iopub.status.busy": "2023-10-29T17:13:56.344692Z",
     "iopub.status.idle": "2023-10-29T17:13:56.348647Z",
     "shell.execute_reply": "2023-10-29T17:13:56.347889Z"
    },
    "papermill": {
     "duration": 0.024696,
     "end_time": "2023-10-29T17:13:56.350800",
     "exception": false,
     "start_time": "2023-10-29T17:13:56.326104",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#top_n_features[top_n_features[\"Feature\"].isin(list_features)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3faa268d",
   "metadata": {
    "_cell_guid": "95e6c8a8-62ed-4412-944a-5b08508702fc",
    "_uuid": "60c6783c-1e42-43ca-ba32-bda313deb585",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2023-10-29T17:13:56.387779Z",
     "iopub.status.busy": "2023-10-29T17:13:56.387351Z",
     "iopub.status.idle": "2023-10-29T17:13:56.396379Z",
     "shell.execute_reply": "2023-10-29T17:13:56.395164Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.030324,
     "end_time": "2023-10-29T17:13:56.398651",
     "exception": false,
     "start_time": "2023-10-29T17:13:56.368327",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def ensemble_predict(model_paths, X_test):\n",
    "    models = []\n",
    "    predictions = []\n",
    "\n",
    "    # Load models based on full artifact paths\n",
    "    for model_path in model_paths:\n",
    "        try:\n",
    "            # If using direct path to pkl\n",
    "            if model_path.endswith(\".pkl\"):\n",
    "                model = joblib.load(model_path)\n",
    "            else:\n",
    "                print(f\"Unsupported model format for {model_path}. Skipping.\")\n",
    "                continue  # Skip this iteration\n",
    "\n",
    "            models.append(model)\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to load model at {model_path}. Error: {e}\")\n",
    "\n",
    "    # Make predictions\n",
    "    for model in models:\n",
    "        try:\n",
    "            pred = model.predict(X_test)\n",
    "            predictions.append(pred)\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to make prediction with model. Error: {e}\")\n",
    "\n",
    "    # Average predictions\n",
    "    if len(predictions) > 0:\n",
    "        ensemble_pred = np.mean(predictions, axis=0)\n",
    "    else:\n",
    "        print(\"No valid models loaded. Cannot make ensemble predictions.\")\n",
    "        ensemble_pred = None\n",
    "\n",
    "    return ensemble_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a1c76ac",
   "metadata": {
    "papermill": {
     "duration": 0.017092,
     "end_time": "2023-10-29T17:13:56.433309",
     "exception": false,
     "start_time": "2023-10-29T17:13:56.416217",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "eb2782e5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-29T17:13:56.470543Z",
     "iopub.status.busy": "2023-10-29T17:13:56.470152Z",
     "iopub.status.idle": "2023-10-29T17:13:56.474552Z",
     "shell.execute_reply": "2023-10-29T17:13:56.473258Z"
    },
    "papermill": {
     "duration": 0.026076,
     "end_time": "2023-10-29T17:13:56.476827",
     "exception": false,
     "start_time": "2023-10-29T17:13:56.450751",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#df_runs = df_runs[df_runs['date_exp'] == 134604.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c739f5",
   "metadata": {
    "papermill": {
     "duration": 0.017707,
     "end_time": "2023-10-29T17:13:56.512113",
     "exception": false,
     "start_time": "2023-10-29T17:13:56.494406",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "32a04efb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-29T17:13:56.548575Z",
     "iopub.status.busy": "2023-10-29T17:13:56.548209Z",
     "iopub.status.idle": "2023-10-29T17:13:56.558986Z",
     "shell.execute_reply": "2023-10-29T17:13:56.557976Z"
    },
    "papermill": {
     "duration": 0.032152,
     "end_time": "2023-10-29T17:13:56.561521",
     "exception": false,
     "start_time": "2023-10-29T17:13:56.529369",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "models_dir = \"models_1\"\n",
    "\n",
    "if TRAIN:\n",
    "    \n",
    "\n",
    "    model_paths = list( df_runs[df_runs['experiment_time'] == \"152953\"][\"model_path\"])\n",
    "\n",
    "    \n",
    "    if not os.path.exists(models_dir):\n",
    "        os.makedirs(models_dir)\n",
    "\n",
    "    for model_path in model_paths:\n",
    "        print(f\"Checking if model path exists: {model_path}\")\n",
    "\n",
    "        if not os.path.exists(model_path):\n",
    "            print(f\"File does not exist: {model_path}\")\n",
    "            continue  # Skip to the next iteration\n",
    "\n",
    "        specific_part = model_path.split(\"/\")[-2]\n",
    "        dest_path = os.path.join(models_dir, f\"{specific_part}.pkl\")\n",
    "        if not os.path.exists(dest_path):\n",
    "            print(f\"Copying from {model_path} to {dest_path}\")\n",
    "            shutil.copy(model_path, dest_path)\n",
    "        else:\n",
    "            print(f\"File {dest_path} already exists. Skipping copy.\")\n",
    "\n",
    "    zipf = zipfile.ZipFile(f\"/kaggle/working/{models_dir}.zip\", \"w\", zipfile.ZIP_DEFLATED)\n",
    "\n",
    "    # Navigate through the folder and add each file to the ZIP\n",
    "    for root, dirs, files in os.walk(f\"/kaggle/working/{models_dir}\"):\n",
    "        for file in files:\n",
    "            zipf.write(\n",
    "                os.path.join(root, file),\n",
    "                os.path.relpath(os.path.join(root, file), f\"/kaggle/working/{models_dir}\"),\n",
    "            )\n",
    "\n",
    "\n",
    "    zipf.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6142281f",
   "metadata": {
    "papermill": {
     "duration": 0.017564,
     "end_time": "2023-10-29T17:13:56.596938",
     "exception": false,
     "start_time": "2023-10-29T17:13:56.579374",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3e74dd11",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-29T17:13:56.633630Z",
     "iopub.status.busy": "2023-10-29T17:13:56.633263Z",
     "iopub.status.idle": "2023-10-29T17:13:56.640862Z",
     "shell.execute_reply": "2023-10-29T17:13:56.639750Z"
    },
    "papermill": {
     "duration": 0.029316,
     "end_time": "2023-10-29T17:13:56.643710",
     "exception": false,
     "start_time": "2023-10-29T17:13:56.614394",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The directory /kaggle/input/models-1 does not exist.\n",
      "List of file paths: []\n"
     ]
    }
   ],
   "source": [
    "model_paths = []\n",
    "models_dir_input  = models_dir.replace(\"_\",\"-\")\n",
    "directory = f\"/kaggle/input/{models_dir_input}\"\n",
    "\n",
    "# Check if the directory exists\n",
    "if os.path.exists(directory):\n",
    "    # Traverse the directory and collect file paths\n",
    "    for filename in os.listdir(directory):\n",
    "        full_path = os.path.join(directory, filename)\n",
    "\n",
    "        # Check if the item is a file (and not a sub-directory)\n",
    "        if os.path.isfile(full_path):\n",
    "            model_paths.append(full_path)\n",
    "else:\n",
    "    print(f\"The directory {directory} does not exist.\")\n",
    "\n",
    "# Print or return the list of file paths\n",
    "print(\"List of file paths:\", model_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a0c897",
   "metadata": {
    "papermill": {
     "duration": 0.017197,
     "end_time": "2023-10-29T17:13:56.678714",
     "exception": false,
     "start_time": "2023-10-29T17:13:56.661517",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3f30970e",
   "metadata": {
    "_cell_guid": "79aa495a-7580-497c-b913-8941cf7d0c61",
    "_uuid": "b3efe16f-ca61-4000-94d7-568364d0e11b",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2023-10-29T17:13:56.717288Z",
     "iopub.status.busy": "2023-10-29T17:13:56.716884Z",
     "iopub.status.idle": "2023-10-29T17:13:56.721131Z",
     "shell.execute_reply": "2023-10-29T17:13:56.720288Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.026697,
     "end_time": "2023-10-29T17:13:56.723547",
     "exception": false,
     "start_time": "2023-10-29T17:13:56.696850",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Assuming X_test for predict\n",
    "#ensemble_predictions = ensemble_predict(model_paths, df_test, mlflow_client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "eaa2b525",
   "metadata": {
    "_cell_guid": "d4bfb979-2758-4fe9-8947-399b1c3a574c",
    "_uuid": "bef1a199-0988-4743-813e-99d4308fc9bb",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2023-10-29T17:13:56.760628Z",
     "iopub.status.busy": "2023-10-29T17:13:56.759805Z",
     "iopub.status.idle": "2023-10-29T17:13:56.783478Z",
     "shell.execute_reply": "2023-10-29T17:13:56.782504Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.044977,
     "end_time": "2023-10-29T17:13:56.786113",
     "exception": false,
     "start_time": "2023-10-29T17:13:56.741136",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import optiver2023\n",
    "\n",
    "env = optiver2023.make_env()\n",
    "iter_test = env.iter_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "450a0b58",
   "metadata": {
    "papermill": {
     "duration": 0.017321,
     "end_time": "2023-10-29T17:13:56.821357",
     "exception": false,
     "start_time": "2023-10-29T17:13:56.804036",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3bd87c40",
   "metadata": {
    "_cell_guid": "d30f3862-dca2-4242-9db0-ab9750118622",
    "_uuid": "efd9073c-aaef-4135-bf7c-1e892e9aa831",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2023-10-29T17:13:56.858074Z",
     "iopub.status.busy": "2023-10-29T17:13:56.857683Z",
     "iopub.status.idle": "2023-10-29T17:14:02.718290Z",
     "shell.execute_reply": "2023-10-29T17:14:02.716915Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 5.88238,
     "end_time": "2023-10-29T17:14:02.721289",
     "exception": false,
     "start_time": "2023-10-29T17:13:56.838909",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This version of the API is not optimized and should not be used to estimate the runtime of your code on the hidden test set.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_21/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_21/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_21/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_21/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_21/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No valid models loaded. Cannot make ensemble predictions.\n",
      "No valid models loaded. Cannot make ensemble predictions.\n",
      "No valid models loaded. Cannot make ensemble predictions.\n",
      "No valid models loaded. Cannot make ensemble predictions.\n",
      "No valid models loaded. Cannot make ensemble predictions.\n",
      "No valid models loaded. Cannot make ensemble predictions.\n",
      "No valid models loaded. Cannot make ensemble predictions.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_21/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_21/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_21/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_21/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_21/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_21/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_21/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_21/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No valid models loaded. Cannot make ensemble predictions.\n",
      "No valid models loaded. Cannot make ensemble predictions.\n",
      "No valid models loaded. Cannot make ensemble predictions.\n",
      "No valid models loaded. Cannot make ensemble predictions.\n",
      "No valid models loaded. Cannot make ensemble predictions.\n",
      "No valid models loaded. Cannot make ensemble predictions.\n",
      "No valid models loaded. Cannot make ensemble predictions.\n",
      "No valid models loaded. Cannot make ensemble predictions.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_21/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_21/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_21/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_21/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_21/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_21/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_21/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No valid models loaded. Cannot make ensemble predictions.\n",
      "No valid models loaded. Cannot make ensemble predictions.\n",
      "No valid models loaded. Cannot make ensemble predictions.\n",
      "No valid models loaded. Cannot make ensemble predictions.\n",
      "No valid models loaded. Cannot make ensemble predictions.\n",
      "No valid models loaded. Cannot make ensemble predictions.\n",
      "No valid models loaded. Cannot make ensemble predictions.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_21/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_21/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_21/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_21/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_21/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_21/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_21/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No valid models loaded. Cannot make ensemble predictions.\n",
      "No valid models loaded. Cannot make ensemble predictions.\n",
      "No valid models loaded. Cannot make ensemble predictions.\n",
      "No valid models loaded. Cannot make ensemble predictions.\n",
      "No valid models loaded. Cannot make ensemble predictions.\n",
      "No valid models loaded. Cannot make ensemble predictions.\n",
      "No valid models loaded. Cannot make ensemble predictions.\n",
      "No valid models loaded. Cannot make ensemble predictions.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_21/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_21/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_21/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_21/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_21/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_21/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_21/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_21/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No valid models loaded. Cannot make ensemble predictions.\n",
      "No valid models loaded. Cannot make ensemble predictions.\n",
      "No valid models loaded. Cannot make ensemble predictions.\n",
      "No valid models loaded. Cannot make ensemble predictions.\n",
      "No valid models loaded. Cannot make ensemble predictions.\n",
      "No valid models loaded. Cannot make ensemble predictions.\n",
      "No valid models loaded. Cannot make ensemble predictions.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_21/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_21/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_21/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_21/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_21/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_21/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_21/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No valid models loaded. Cannot make ensemble predictions.\n",
      "No valid models loaded. Cannot make ensemble predictions.\n",
      "No valid models loaded. Cannot make ensemble predictions.\n",
      "No valid models loaded. Cannot make ensemble predictions.\n",
      "No valid models loaded. Cannot make ensemble predictions.\n",
      "No valid models loaded. Cannot make ensemble predictions.\n",
      "No valid models loaded. Cannot make ensemble predictions.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_21/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_21/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_21/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_21/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_21/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_21/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_21/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No valid models loaded. Cannot make ensemble predictions.\n",
      "No valid models loaded. Cannot make ensemble predictions.\n",
      "No valid models loaded. Cannot make ensemble predictions.\n",
      "No valid models loaded. Cannot make ensemble predictions.\n",
      "No valid models loaded. Cannot make ensemble predictions.\n",
      "No valid models loaded. Cannot make ensemble predictions.\n",
      "No valid models loaded. Cannot make ensemble predictions.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_21/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_21/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_21/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_21/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_21/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_21/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_21/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No valid models loaded. Cannot make ensemble predictions.\n",
      "No valid models loaded. Cannot make ensemble predictions.\n",
      "No valid models loaded. Cannot make ensemble predictions.\n",
      "No valid models loaded. Cannot make ensemble predictions.\n",
      "No valid models loaded. Cannot make ensemble predictions.\n",
      "No valid models loaded. Cannot make ensemble predictions.\n",
      "No valid models loaded. Cannot make ensemble predictions.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_21/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_21/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_21/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_21/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_21/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_21/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_21/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No valid models loaded. Cannot make ensemble predictions.\n",
      "No valid models loaded. Cannot make ensemble predictions.\n",
      "No valid models loaded. Cannot make ensemble predictions.\n",
      "No valid models loaded. Cannot make ensemble predictions.\n",
      "No valid models loaded. Cannot make ensemble predictions.\n",
      "No valid models loaded. Cannot make ensemble predictions.\n",
      "No valid models loaded. Cannot make ensemble predictions.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_21/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_21/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_21/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_21/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_21/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_21/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_21/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No valid models loaded. Cannot make ensemble predictions.\n",
      "No valid models loaded. Cannot make ensemble predictions.\n",
      "No valid models loaded. Cannot make ensemble predictions.\n",
      "No valid models loaded. Cannot make ensemble predictions.\n",
      "No valid models loaded. Cannot make ensemble predictions.\n",
      "No valid models loaded. Cannot make ensemble predictions.\n",
      "No valid models loaded. Cannot make ensemble predictions.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_21/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_21/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_21/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_21/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_21/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_21/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_21/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No valid models loaded. Cannot make ensemble predictions.\n",
      "No valid models loaded. Cannot make ensemble predictions.\n",
      "No valid models loaded. Cannot make ensemble predictions.\n",
      "No valid models loaded. Cannot make ensemble predictions.\n",
      "No valid models loaded. Cannot make ensemble predictions.\n",
      "No valid models loaded. Cannot make ensemble predictions.\n",
      "No valid models loaded. Cannot make ensemble predictions.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_21/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_21/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_21/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_21/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_21/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_21/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_21/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No valid models loaded. Cannot make ensemble predictions.\n",
      "No valid models loaded. Cannot make ensemble predictions.\n",
      "No valid models loaded. Cannot make ensemble predictions.\n",
      "No valid models loaded. Cannot make ensemble predictions.\n",
      "No valid models loaded. Cannot make ensemble predictions.\n",
      "No valid models loaded. Cannot make ensemble predictions.\n",
      "No valid models loaded. Cannot make ensemble predictions.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_21/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_21/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_21/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_21/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_21/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_21/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_21/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No valid models loaded. Cannot make ensemble predictions.\n",
      "No valid models loaded. Cannot make ensemble predictions.\n",
      "No valid models loaded. Cannot make ensemble predictions.\n",
      "No valid models loaded. Cannot make ensemble predictions.\n",
      "No valid models loaded. Cannot make ensemble predictions.\n",
      "No valid models loaded. Cannot make ensemble predictions.\n",
      "No valid models loaded. Cannot make ensemble predictions.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_21/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_21/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_21/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_21/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_21/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_21/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_21/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No valid models loaded. Cannot make ensemble predictions.\n",
      "No valid models loaded. Cannot make ensemble predictions.\n",
      "No valid models loaded. Cannot make ensemble predictions.\n",
      "No valid models loaded. Cannot make ensemble predictions.\n",
      "No valid models loaded. Cannot make ensemble predictions.\n",
      "No valid models loaded. Cannot make ensemble predictions.\n",
      "No valid models loaded. Cannot make ensemble predictions.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_21/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_21/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_21/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_21/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_21/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_21/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No valid models loaded. Cannot make ensemble predictions.\n",
      "No valid models loaded. Cannot make ensemble predictions.\n",
      "No valid models loaded. Cannot make ensemble predictions.\n",
      "No valid models loaded. Cannot make ensemble predictions.\n",
      "No valid models loaded. Cannot make ensemble predictions.\n",
      "No valid models loaded. Cannot make ensemble predictions.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_21/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_21/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_21/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_21/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_21/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_21/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_21/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_21/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No valid models loaded. Cannot make ensemble predictions.\n",
      "No valid models loaded. Cannot make ensemble predictions.\n",
      "No valid models loaded. Cannot make ensemble predictions.\n",
      "No valid models loaded. Cannot make ensemble predictions.\n",
      "No valid models loaded. Cannot make ensemble predictions.\n",
      "No valid models loaded. Cannot make ensemble predictions.\n",
      "No valid models loaded. Cannot make ensemble predictions.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_21/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_21/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_21/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_21/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_21/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_21/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No valid models loaded. Cannot make ensemble predictions.\n",
      "No valid models loaded. Cannot make ensemble predictions.\n",
      "No valid models loaded. Cannot make ensemble predictions.\n",
      "No valid models loaded. Cannot make ensemble predictions.\n",
      "No valid models loaded. Cannot make ensemble predictions.\n",
      "No valid models loaded. Cannot make ensemble predictions.\n",
      "No valid models loaded. Cannot make ensemble predictions.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_21/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_21/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_21/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_21/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_21/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_21/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_21/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No valid models loaded. Cannot make ensemble predictions.\n",
      "No valid models loaded. Cannot make ensemble predictions.\n",
      "No valid models loaded. Cannot make ensemble predictions.\n",
      "No valid models loaded. Cannot make ensemble predictions.\n",
      "No valid models loaded. Cannot make ensemble predictions.\n",
      "No valid models loaded. Cannot make ensemble predictions.\n",
      "No valid models loaded. Cannot make ensemble predictions.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_21/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_21/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_21/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_21/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_21/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_21/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No valid models loaded. Cannot make ensemble predictions.\n",
      "No valid models loaded. Cannot make ensemble predictions.\n",
      "No valid models loaded. Cannot make ensemble predictions.\n",
      "No valid models loaded. Cannot make ensemble predictions.\n",
      "No valid models loaded. Cannot make ensemble predictions.\n",
      "No valid models loaded. Cannot make ensemble predictions.\n",
      "No valid models loaded. Cannot make ensemble predictions.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_21/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_21/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_21/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_21/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_21/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_21/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_21/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No valid models loaded. Cannot make ensemble predictions.\n",
      "No valid models loaded. Cannot make ensemble predictions.\n",
      "No valid models loaded. Cannot make ensemble predictions.\n",
      "No valid models loaded. Cannot make ensemble predictions.\n",
      "No valid models loaded. Cannot make ensemble predictions.\n",
      "No valid models loaded. Cannot make ensemble predictions.\n",
      "No valid models loaded. Cannot make ensemble predictions.\n",
      "No valid models loaded. Cannot make ensemble predictions.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_21/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_21/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_21/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_21/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_21/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_21/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_21/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No valid models loaded. Cannot make ensemble predictions.\n",
      "No valid models loaded. Cannot make ensemble predictions.\n",
      "No valid models loaded. Cannot make ensemble predictions.\n",
      "No valid models loaded. Cannot make ensemble predictions.\n",
      "No valid models loaded. Cannot make ensemble predictions.\n",
      "No valid models loaded. Cannot make ensemble predictions.\n",
      "No valid models loaded. Cannot make ensemble predictions.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_21/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_21/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_21/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_21/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_21/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_21/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_21/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No valid models loaded. Cannot make ensemble predictions.\n",
      "No valid models loaded. Cannot make ensemble predictions.\n",
      "No valid models loaded. Cannot make ensemble predictions.\n",
      "No valid models loaded. Cannot make ensemble predictions.\n",
      "No valid models loaded. Cannot make ensemble predictions.\n",
      "No valid models loaded. Cannot make ensemble predictions.\n",
      "No valid models loaded. Cannot make ensemble predictions.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_21/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_21/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_21/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_21/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_21/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_21/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_21/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No valid models loaded. Cannot make ensemble predictions.\n",
      "No valid models loaded. Cannot make ensemble predictions.\n",
      "No valid models loaded. Cannot make ensemble predictions.\n",
      "No valid models loaded. Cannot make ensemble predictions.\n",
      "No valid models loaded. Cannot make ensemble predictions.\n",
      "No valid models loaded. Cannot make ensemble predictions.\n",
      "No valid models loaded. Cannot make ensemble predictions.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_21/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_21/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_21/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_21/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_21/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_21/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No valid models loaded. Cannot make ensemble predictions.\n",
      "No valid models loaded. Cannot make ensemble predictions.\n"
     ]
    }
   ],
   "source": [
    "counter = 0\n",
    "for test, revealed_targets, sample_prediction in iter_test:\n",
    "    #df_test_raw = pl.DataFrame(test)\n",
    "\n",
    "    feat = feat_engineering(test)\n",
    "    \n",
    "\n",
    "    #feat = df_test.to_pandas()\n",
    "\n",
    "    list_cols_drop = [\"stock_id\", \"date_id\", \"row_id\"]\n",
    "    feat = feat.drop(list_cols_drop, axis=1)\n",
    "\n",
    "    sample_prediction[\"target\"] = ensemble_predict(model_paths, feat)\n",
    "    env.predict(sample_prediction)\n",
    "    counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7a3bd4bc",
   "metadata": {
    "_cell_guid": "1bca4965-2df1-4a88-bc93-1e6d8fea7ac3",
    "_uuid": "0a5200ae-92df-4b21-bf5c-6e255b90676f",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2023-10-29T17:14:02.769332Z",
     "iopub.status.busy": "2023-10-29T17:14:02.768326Z",
     "iopub.status.idle": "2023-10-29T17:14:02.773599Z",
     "shell.execute_reply": "2023-10-29T17:14:02.772694Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.031948,
     "end_time": "2023-10-29T17:14:02.776086",
     "exception": false,
     "start_time": "2023-10-29T17:14:02.744138",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if TRAIN:\n",
    "    list_cols_drop = [\"stock_id\", \"date_id\", \"row_id\"]\n",
    "    feat = feat.drop(list_cols_drop, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "081e2758",
   "metadata": {
    "papermill": {
     "duration": 0.022488,
     "end_time": "2023-10-29T17:14:02.821201",
     "exception": false,
     "start_time": "2023-10-29T17:14:02.798713",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f32a8aa2",
   "metadata": {
    "papermill": {
     "duration": 0.022397,
     "end_time": "2023-10-29T17:14:02.866505",
     "exception": false,
     "start_time": "2023-10-29T17:14:02.844108",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3231cf24",
   "metadata": {
    "papermill": {
     "duration": 0.022322,
     "end_time": "2023-10-29T17:14:02.911806",
     "exception": false,
     "start_time": "2023-10-29T17:14:02.889484",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9154e674",
   "metadata": {
    "papermill": {
     "duration": 0.022515,
     "end_time": "2023-10-29T17:14:02.956958",
     "exception": false,
     "start_time": "2023-10-29T17:14:02.934443",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd10aaf2",
   "metadata": {
    "_cell_guid": "18344629-3cbc-4244-ac34-c8af6b88b0d8",
    "_uuid": "667a4d83-d235-446e-a72c-cf02718a1a7b",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.022681,
     "end_time": "2023-10-29T17:14:03.002235",
     "exception": false,
     "start_time": "2023-10-29T17:14:02.979554",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "765a7946",
   "metadata": {
    "papermill": {
     "duration": 0.022524,
     "end_time": "2023-10-29T17:14:03.047914",
     "exception": false,
     "start_time": "2023-10-29T17:14:03.025390",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "566fd054",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-29T17:14:03.094733Z",
     "iopub.status.busy": "2023-10-29T17:14:03.094329Z",
     "iopub.status.idle": "2023-10-29T17:14:03.099141Z",
     "shell.execute_reply": "2023-10-29T17:14:03.098059Z"
    },
    "papermill": {
     "duration": 0.03074,
     "end_time": "2023-10-29T17:14:03.101281",
     "exception": false,
     "start_time": "2023-10-29T17:14:03.070541",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#clean_directory_except_one('/kaggle/working/', 'submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d16d760",
   "metadata": {
    "_cell_guid": "3ef4ae54-1bb4-4cc3-a7da-a6691a272d30",
    "_uuid": "495a4e04-554d-4ef1-919d-62bc59f089c4",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.022143,
     "end_time": "2023-10-29T17:14:03.146079",
     "exception": false,
     "start_time": "2023-10-29T17:14:03.123936",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c55cc61d",
   "metadata": {
    "papermill": {
     "duration": 0.022242,
     "end_time": "2023-10-29T17:14:03.190929",
     "exception": false,
     "start_time": "2023-10-29T17:14:03.168687",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7da13fe3",
   "metadata": {
    "_cell_guid": "5498ae4b-62e8-4050-91e7-075fc549380f",
    "_uuid": "ce9653b2-7dff-4c5b-af77-5f0ef88b6e86",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.022763,
     "end_time": "2023-10-29T17:14:03.236519",
     "exception": false,
     "start_time": "2023-10-29T17:14:03.213756",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee72695",
   "metadata": {
    "_cell_guid": "92a8ed18-3a63-464e-9d62-17cea3baed59",
    "_uuid": "0b017968-3b5a-4465-820e-e89659b10afd",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.02234,
     "end_time": "2023-10-29T17:14:03.281905",
     "exception": false,
     "start_time": "2023-10-29T17:14:03.259565",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0714017",
   "metadata": {
    "_cell_guid": "f9d19e8d-0074-4ae2-adfb-4c039b700215",
    "_uuid": "b6096d27-d452-4593-9832-d15bfbd83e1b",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.023396,
     "end_time": "2023-10-29T17:14:03.328390",
     "exception": false,
     "start_time": "2023-10-29T17:14:03.304994",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccbe39c1",
   "metadata": {
    "_cell_guid": "855f8db3-e344-4d4d-b577-36499775f352",
    "_uuid": "fde5edb9-d23d-41ac-92c6-593f299230c2",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.023621,
     "end_time": "2023-10-29T17:14:03.374978",
     "exception": false,
     "start_time": "2023-10-29T17:14:03.351357",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ad7988",
   "metadata": {
    "_cell_guid": "82330d89-f35e-4cae-876c-de852438527d",
    "_uuid": "f90f329d-b882-464c-84fc-74a815cfe1cd",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.022435,
     "end_time": "2023-10-29T17:14:03.421377",
     "exception": false,
     "start_time": "2023-10-29T17:14:03.398942",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53bf86c0",
   "metadata": {
    "_cell_guid": "ed96eca6-9765-4c90-a6bf-3071518ba2f0",
    "_uuid": "ca1d48b9-5537-491b-9de2-57360edd9844",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.02221,
     "end_time": "2023-10-29T17:14:03.466641",
     "exception": false,
     "start_time": "2023-10-29T17:14:03.444431",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a5173c",
   "metadata": {
    "_cell_guid": "06b24b0a-6515-48c6-b4a0-240865cfc1ea",
    "_uuid": "5e9b8536-f812-432a-ba05-c2f0b0f8994f",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.023496,
     "end_time": "2023-10-29T17:14:03.513109",
     "exception": false,
     "start_time": "2023-10-29T17:14:03.489613",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 13.294222,
   "end_time": "2023-10-29T17:14:04.058881",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-10-29T17:13:50.764659",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
