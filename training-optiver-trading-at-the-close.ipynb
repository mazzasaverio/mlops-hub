{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8dd24e2d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-29T17:16:43.958185Z",
     "iopub.status.busy": "2023-10-29T17:16:43.957509Z",
     "iopub.status.idle": "2023-10-29T17:16:44.601997Z",
     "shell.execute_reply": "2023-10-29T17:16:44.601043Z"
    },
    "papermill": {
     "duration": 0.660513,
     "end_time": "2023-10-29T17:16:44.604311",
     "exception": false,
     "start_time": "2023-10-29T17:16:43.943798",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Constants\n",
    "TRAIN = True\n",
    "OVERWRITE = False\n",
    "DEBUG = True\n",
    "DEBUG_SAMPLE = 10000\n",
    "\n",
    "N_TRIALS = 10\n",
    "\n",
    "VERSION_NB = 2\n",
    "\n",
    "state = 42\n",
    "\n",
    "download_kaggle_data = False\n",
    "\n",
    "# External general-purpose modules\n",
    "import os\n",
    "import shutil\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "import glob\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "from dotenv import load_dotenv\n",
    "from joblib import dump\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "# Setting pandas options and warning filters\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "2b9c1eb0",
   "metadata": {
    "_cell_guid": "3bdaa15b-0b9e-4b3b-9c47-58e6b9b18b5e",
    "_uuid": "206043b9-f1d5-4691-b03a-935f2177086d",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2023-10-29T17:16:44.631270Z",
     "iopub.status.busy": "2023-10-29T17:16:44.630505Z",
     "iopub.status.idle": "2023-10-29T17:16:44.646887Z",
     "shell.execute_reply": "2023-10-29T17:16:44.645855Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.032481,
     "end_time": "2023-10-29T17:16:44.649372",
     "exception": false,
     "start_time": "2023-10-29T17:16:44.616891",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/sam/github/mlops-hub/\n",
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Setting up the project directory path\n",
    "path_project_dir = os.getcwd()\n",
    "if path_project_dir not in [\"/kaggle/working\", \"/content\"]:\n",
    "    path_project_dir = os.getenv(\"ROOT_PATH\")\n",
    "\n",
    "print(path_project_dir)\n",
    "\n",
    "# Imports and setup for training\n",
    "if TRAIN:\n",
    "    # Install packages and import logging libraries\n",
    "    if path_project_dir == '/kaggle/working':\n",
    "        !pip install loguru mlflow optuna > /dev/null\n",
    "        from utils import get_data, clean_directory_except_one\n",
    "\n",
    "    from utils import log_feature_importance, create_model, log_training_details\n",
    "    \n",
    "    from loguru import logger\n",
    "    import mlflow\n",
    "    import optuna\n",
    "    from optuna.integration.mlflow import MLflowCallback\n",
    "    from mlflow.tracking import MlflowClient\n",
    "    import zipfile\n",
    "    \n",
    "    from tqdm import tqdm\n",
    "\n",
    "    # Import machine learning libraries\n",
    "    from lightgbm import log_evaluation, early_stopping, LGBMRegressor as LGBMR\n",
    "    from sklearn.model_selection import KFold\n",
    "    from xgboost import XGBRegressor as XGBR\n",
    "\n",
    "    # Set logging\n",
    "    logger.add(\"logs.log\", format=\"{time:YYYY-MM-DD HH:mm} | {level} | {message}\")\n",
    "    optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "    warnings.filterwarnings(\"ignore\", category=optuna.exceptions.ExperimentalWarning)\n",
    "    \n",
    "    # Auto-reload modules\n",
    "    %load_ext autoreload\n",
    "    %autoreload 2\n",
    "\n",
    "    # Initialize MLflow callback\n",
    "    mlflow_callback = MLflowCallback(\n",
    "        tracking_uri=mlflow.get_tracking_uri(), metric_name=\"mae\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "1371d06b",
   "metadata": {
    "_cell_guid": "9765420b-f8ab-46ba-8b4c-e4487eb04b76",
    "_uuid": "c4174134-1441-41f8-adbf-f03ad3f7d7dc",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2023-10-29T17:16:44.675396Z",
     "iopub.status.busy": "2023-10-29T17:16:44.675038Z",
     "iopub.status.idle": "2023-10-29T17:16:44.683847Z",
     "shell.execute_reply": "2023-10-29T17:16:44.682828Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.024352,
     "end_time": "2023-10-29T17:16:44.685842",
     "exception": false,
     "start_time": "2023-10-29T17:16:44.661490",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if path_project_dir == \"/kaggle/working\":\n",
    "    path_data_project_dir = \"/kaggle/input/optiver-trading-at-the-close\"\n",
    "    path_experiments_storage = os.path.join(path_project_dir, \"experiments_storage\")\n",
    "\n",
    "    path_dataset_train_raw = \"/kaggle/input/optiver-trading-at-the-close/train.csv\"\n",
    "    path_dataset_test_raw = (\n",
    "        \"/kaggle/input/optiver-trading-at-the-close/example_test_files/test.csv\"\n",
    "    )\n",
    "\n",
    "    path_dataset_processed = \"/kaggle/working/processed_data\"\n",
    "    path_dataset_train = os.path.join(path_dataset_processed, \"train.csv\")\n",
    "    path_dataset_test = os.path.join(path_dataset_processed, \"test.csv\")\n",
    "\n",
    "else:\n",
    "    name_folder_data_project = \"kaggle_optiver_trading_at_the_close\"\n",
    "\n",
    "    path_data_dir = os.path.join(path_project_dir, \"data\")\n",
    "    path_dataset_train_raw = os.path.join(\n",
    "        path_data_dir, \"kaggle_optiver_trading_at_the_close/raw\", \"train.csv\"\n",
    "    )\n",
    "    path_dataset_processed = os.path.join(\n",
    "        path_data_dir, \"kaggle_optiver_trading_at_the_close/processed\"\n",
    "    )\n",
    "\n",
    "    path_data_project_dir = os.path.join(path_data_dir, name_folder_data_project)\n",
    "\n",
    "    path_config_dir = os.path.join(path_project_dir, \"config\")\n",
    "    path_config_train = os.path.join(path_config_dir, \"train_config.yaml\")\n",
    "\n",
    "    path_experiments_storage = os.path.join(\n",
    "        path_data_project_dir, \"experiments_storage\"\n",
    "    )\n",
    "\n",
    "    if download_kaggle_data:\n",
    "        dataset_name = \"ravi20076/optiver-memoryreduceddatasets\"\n",
    "        kaggle_json_path = os.path.join(path_project_dir, \"kaggle.json\")\n",
    "        get_data(\n",
    "            kaggle_json_path,\n",
    "            path_data_project_dir,\n",
    "            dataset_name=dataset_name,\n",
    "            specific_file=None,\n",
    "        )\n",
    "\n",
    "    file_name_df_train = \"train.csv\"\n",
    "    file_name_df_test = \"test.csv\"\n",
    "\n",
    "    path_dataset_train = os.path.join(path_data_project_dir, file_name_df_train)\n",
    "    path_dataset_test = os.path.join(path_data_project_dir, file_name_df_test)\n",
    "\n",
    "if TRAIN:\n",
    "    mlflow.set_tracking_uri(path_experiments_storage)\n",
    "    client = MlflowClient()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19d96edb",
   "metadata": {
    "_cell_guid": "b8d09bc0-2c22-4c0c-9142-ab698ac4e6ea",
    "_uuid": "858fa67e-742c-461a-9c05-5ec2e4554328",
    "papermill": {
     "duration": 0.01189,
     "end_time": "2023-10-29T17:16:44.709850",
     "exception": false,
     "start_time": "2023-10-29T17:16:44.697960",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Constants and Settings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "02aa491e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-29T17:16:44.735704Z",
     "iopub.status.busy": "2023-10-29T17:16:44.735278Z",
     "iopub.status.idle": "2023-10-29T17:16:44.741584Z",
     "shell.execute_reply": "2023-10-29T17:16:44.740330Z"
    },
    "papermill": {
     "duration": 0.021683,
     "end_time": "2023-10-29T17:16:44.743683",
     "exception": false,
     "start_time": "2023-10-29T17:16:44.722000",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if TRAIN:\n",
    "    if not os.path.exists(path_dataset_processed):\n",
    "        os.makedirs(path_dataset_processed)\n",
    "\n",
    "    if not os.path.exists(path_dataset_train) or OVERWRITE:\n",
    "        df_train_raw = pd.read_csv(path_dataset_train_raw)\n",
    "    else:\n",
    "        df_train_raw = pd.read_csv(path_dataset_train)\n",
    "\n",
    "    if DEBUG:\n",
    "        df_train_raw = df_train_raw[df_train_raw[\"stock_id\"].isin([0, 1, 2, 3, 4, 5])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "3908f69c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-29T17:16:44.770491Z",
     "iopub.status.busy": "2023-10-29T17:16:44.770054Z",
     "iopub.status.idle": "2023-10-29T17:16:44.775300Z",
     "shell.execute_reply": "2023-10-29T17:16:44.774125Z"
    },
    "papermill": {
     "duration": 0.020681,
     "end_time": "2023-10-29T17:16:44.777236",
     "exception": false,
     "start_time": "2023-10-29T17:16:44.756555",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if TRAIN:\n",
    "    # Dropping rows with null targets:-\n",
    "    drop_idx = df_train_raw.loc[df_train_raw[\"target\"].isna(), \"target\"].index.to_list()\n",
    "    df_train_raw = df_train_raw.drop(drop_idx, axis=0)\n",
    "    df_train_raw.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "a152dee1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-29T17:16:44.803330Z",
     "iopub.status.busy": "2023-10-29T17:16:44.802966Z",
     "iopub.status.idle": "2023-10-29T17:16:44.830061Z",
     "shell.execute_reply": "2023-10-29T17:16:44.829231Z"
    },
    "papermill": {
     "duration": 0.042863,
     "end_time": "2023-10-29T17:16:44.832191",
     "exception": false,
     "start_time": "2023-10-29T17:16:44.789328",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def feat_engineering(df_train):\n",
    "    df = pl.DataFrame(df_train)\n",
    "    # 7. Handle Missing Values\n",
    "    df = df.with_columns(\n",
    "        [\n",
    "            pl.col(\"far_price\").fill_null(strategy=\"forward\").alias(\"far_price\"),\n",
    "            pl.col(\"near_price\").fill_null(strategy=\"forward\").alias(\"near_price\"),\n",
    "        ]\n",
    "    )\n",
    "    # Level 1 Features\n",
    "    level_one_features = [\n",
    "        (pl.col(\"imbalance_size\") / pl.col(\"matched_size\")).alias(\n",
    "            \"imbalance_to_matched_size\"\n",
    "        ),\n",
    "        (pl.col(\"imbalance_size\") * pl.col(\"imbalance_buy_sell_flag\")).alias(\n",
    "            \"imbalance_flag_to_size\"\n",
    "        ),\n",
    "        (pl.col(\"ask_price\") - pl.col(\"bid_price\")).alias(\"spread\"),\n",
    "        (pl.col(\"bid_size\") - pl.col(\"ask_size\")).alias(\"bid_ask_imbalance\"),\n",
    "        (pl.col(\"bid_size\") / pl.col(\"ask_size\")).alias(\"liquidity\"),\n",
    "        (pl.col(\"bid_price\") - pl.col(\"wap\")).alias(\"price_diff_bid_to_wap\"),\n",
    "        (pl.col(\"ask_price\") - pl.col(\"wap\")).alias(\"price_diff_ask_to_wap\"),\n",
    "        (pl.col(\"bid_size\") - pl.col(\"wap\")).alias(\"size_diff_bid_to_wap\"),\n",
    "        (pl.col(\"ask_size\") - pl.col(\"wap\")).alias(\"size_diff_ask_to_wap\"),\n",
    "        (pl.col(\"wap\") - pl.col(\"wap\").shift(1).over([\"stock_id\", \"date_id\"])).alias(\n",
    "            \"wap_velocity\"\n",
    "        ),\n",
    "        (\n",
    "            pl.col(\"wap\") / pl.col(\"wap\").shift(5).over([\"stock_id\", \"date_id\"]) - 1\n",
    "        ).alias(\"wap_momentum_5\"),\n",
    "        (\n",
    "            pl.col(\"wap\")\n",
    "            .std()\n",
    "            .over([\"stock_id\", \"date_id\"])\n",
    "            .alias(\"short_term_volatility\")\n",
    "        ),\n",
    "        (\n",
    "            (\n",
    "                pl.col(\"imbalance_size\")\n",
    "                / (pl.col(\"matched_size\") + pl.col(\"imbalance_size\"))\n",
    "            ).alias(\"price_impact\")\n",
    "        ),\n",
    "        (\n",
    "            (pl.col(\"bid_size\") - pl.col(\"ask_size\"))\n",
    "            / (pl.col(\"bid_size\") + pl.col(\"ask_size\"))\n",
    "        ).alias(\"order_imbalance_ratio\"),\n",
    "        (\n",
    "            (pl.col(\"ask_price\") - pl.col(\"bid_price\"))\n",
    "            / (pl.col(\"ask_price\") + pl.col(\"bid_price\"))\n",
    "        ).alias(\"price_skewness\"),\n",
    "        (pl.col(\"seconds_in_bucket\") / 600).alias(\"time_decay\"),\n",
    "    ]\n",
    "\n",
    "    # Level 2 Features\n",
    "    level_two_features = [\n",
    "        (\n",
    "            pl.col(\"wap_velocity\")\n",
    "            - pl.col(\"wap_velocity\").shift(1).over([\"stock_id\", \"date_id\"])\n",
    "        ).alias(\"wap_acceleration\"),\n",
    "        (\n",
    "            pl.col(\"short_term_volatility\").shift(1).over([\"stock_id\", \"date_id\"])\n",
    "            - pl.col(\"short_term_volatility\")\n",
    "        ).alias(\"volatility_rate_of_change\"),\n",
    "        (\n",
    "            (\n",
    "                pl.col(\"liquidity\")\n",
    "                - pl.col(\"liquidity\").shift(1).over([\"stock_id\", \"date_id\"])\n",
    "            )\n",
    "            / pl.col(\"liquidity\").shift(1).over([\"stock_id\", \"date_id\"])\n",
    "        ).alias(\"liquidity_ratio_change\"),\n",
    "        (\n",
    "            (\n",
    "                pl.col(\"order_imbalance_ratio\")\n",
    "                - pl.col(\"order_imbalance_ratio\").shift(1).over([\"stock_id\", \"date_id\"])\n",
    "            )\n",
    "            / pl.col(\"order_imbalance_ratio\").shift(1).over([\"stock_id\", \"date_id\"])\n",
    "        ).alias(\"order_imbalance_over_time\"),\n",
    "        (\n",
    "            (\n",
    "                pl.col(\"price_skewness\")\n",
    "                - pl.col(\"price_skewness\").shift(1).over([\"stock_id\", \"date_id\"])\n",
    "            )\n",
    "            / pl.col(\"price_skewness\").shift(1).over([\"stock_id\", \"date_id\"])\n",
    "        ).alias(\"price_skewness_rate_of_change\"),\n",
    "    ]\n",
    "\n",
    "    # Level 3 Features\n",
    "    level_three_aggregations = [\n",
    "        pl.col(\"wap\").mean().alias(\"avg_wap_by_market\"),\n",
    "        pl.col(\"near_price\").mean().alias(\"avg_near_price_by_market\"),\n",
    "        pl.col(\"matched_size\").mean().alias(\"avg_matched_size_by_market\"),\n",
    "        pl.col(\"imbalance_to_matched_size\")\n",
    "        .mean()\n",
    "        .alias(\"avg_imbalance_to_matched_size_by_market\"),\n",
    "        pl.col(\"spread\").mean().alias(\"avg_spread_by_market\"),\n",
    "        pl.col(\"liquidity\").mean().alias(\"avg_liquidity_by_market\"),\n",
    "        pl.col(\"short_term_volatility\").mean().alias(\"avg_market_volatility\"),\n",
    "        pl.col(\"order_imbalance_ratio\").mean().alias(\"avg_market_imbalance\"),\n",
    "        pl.col(\"liquidity\").mean().alias(\"avg_market_liquidity\"),\n",
    "        pl.col(\"price_impact\").mean().alias(\"avg_market_price_impact\"),\n",
    "        pl.col(\"price_skewness\").mean().alias(\"avg_market_price_skewness\"),\n",
    "    ]\n",
    "\n",
    "    # Adding all features and performing join operation\n",
    "    df = df.with_columns(level_one_features)\n",
    "    df = df.with_columns(level_two_features)\n",
    "    group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
    "        *level_three_aggregations\n",
    "    )\n",
    "    df = df.join(group_by_market, on=[\"date_id\", \"seconds_in_bucket\"], how=\"left\")\n",
    "\n",
    "    polynomial_and_interaction_features = [\n",
    "        (pl.col(\"seconds_in_bucket\") * pl.col(\"near_price\")).alias(\n",
    "            \"seconds_in_bucket_X_near_price\"\n",
    "        ),\n",
    "        (pl.col(\"matched_size\") * pl.col(\"near_price\")).alias(\n",
    "            \"matched_size_X_near_price\"\n",
    "        ),\n",
    "        (pl.col(\"near_price\") ** 2).alias(\"near_price_squared\"),\n",
    "        (pl.col(\"matched_size\") ** 2).alias(\"matched_size_squared\"),\n",
    "        (pl.col(\"seconds_in_bucket\") * pl.col(\"imbalance_flag_to_size\")).alias(\n",
    "            \"seconds_in_bucket_X_imbalance_flag_to_size\"\n",
    "        ),\n",
    "        (pl.col(\"seconds_in_bucket\") ** 2).alias(\"seconds_in_bucket_squared\"),\n",
    "        (pl.col(\"imbalance_flag_to_size\") ** 2).alias(\"imbalance_flag_to_size_squared\"),\n",
    "    ]\n",
    "\n",
    "    # Relative to Market Features\n",
    "    relative_to_market_features = [\n",
    "        (pl.col(\"wap\") / pl.col(\"avg_wap_by_market\")).alias(\"relative_wap_to_market\"),\n",
    "        (pl.col(\"near_price\") / pl.col(\"avg_near_price_by_market\")).alias(\n",
    "            \"relative_near_price_to_market\"\n",
    "        ),\n",
    "        (pl.col(\"matched_size\") / pl.col(\"avg_matched_size_by_market\")).alias(\n",
    "            \"relative_matched_size_to_market\"\n",
    "        ),\n",
    "        (\n",
    "            pl.col(\"imbalance_to_matched_size\")\n",
    "            / pl.col(\"avg_imbalance_to_matched_size_by_market\")\n",
    "        ).alias(\"relative_imbalance_to_matched_size_to_market\"),\n",
    "        (pl.col(\"spread\") / pl.col(\"avg_spread_by_market\")).alias(\n",
    "            \"relative_spread_to_market\"\n",
    "        ),\n",
    "        (pl.col(\"liquidity\") / pl.col(\"avg_liquidity_by_market\")).alias(\n",
    "            \"relative_liquidity_to_market\"\n",
    "        ),\n",
    "    ]\n",
    "\n",
    "    # Combine all Level 4 features and add them to the DataFrame\n",
    "    all_level_four_features = (\n",
    "        polynomial_and_interaction_features + relative_to_market_features\n",
    "    )\n",
    "    df = df.with_columns(all_level_four_features)\n",
    "\n",
    "    for window in [5, 10]:\n",
    "        rolling_group = df.group_by_rolling(\n",
    "            index_column=\"seconds_in_bucket\",\n",
    "            period=f\"{window}i\",  # 'i' denotes index count (integer)\n",
    "            by=[\"stock_id\", \"date_id\"],\n",
    "            closed=\"left\",  # Adjust as needed\n",
    "        )\n",
    "\n",
    "        # Apply to basic and new features\n",
    "        for col in [\n",
    "            \"wap\",\n",
    "            \"imbalance_size\",\n",
    "            \"bid_price\",\n",
    "            \"ask_price\",\n",
    "            \"relative_wap_to_market\",\n",
    "            \"wap_momentum_5\",\n",
    "        ]:\n",
    "            df = df.join(\n",
    "                rolling_group.agg(pl.col(col).mean().alias(f\"{col}_mean_{window}\")),\n",
    "                on=[\"stock_id\", \"date_id\", \"seconds_in_bucket\"],\n",
    "                how=\"left\",\n",
    "            )\n",
    "\n",
    "    low_importance_cols = [\n",
    "        \"wap_mean_5\",\n",
    "        \"imbalance_buy_sell_flag\",\n",
    "        \"imbalance_flag_to_size_squared\",\n",
    "        \"imbalance_size_mean_5\",\n",
    "        \"bid_price_mean_5\",\n",
    "        \"ask_price_mean_5\",\n",
    "        \"wap_momentum_5_mean_5\",\n",
    "        \"relative_wap_to_market_mean_5\",\n",
    "        \"volatility_rate_of_change\"\n",
    "        # Add more columns as needed\n",
    "    ]\n",
    "\n",
    "    existing_cols = df.columns\n",
    "\n",
    "    # Drop columns only if they exist in DataFrame\n",
    "    cols_to_drop = [col for col in low_importance_cols if col in existing_cols]\n",
    "\n",
    "    if cols_to_drop:\n",
    "        engineered_df = df.drop(cols_to_drop)\n",
    "    else:\n",
    "        engineered_df = df.to_pandas()\n",
    "\n",
    "    engineered_df = engineered_df.to_pandas()\n",
    "\n",
    "    return engineered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "29500bf6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-29T17:16:44.858014Z",
     "iopub.status.busy": "2023-10-29T17:16:44.857639Z",
     "iopub.status.idle": "2023-10-29T17:16:44.862362Z",
     "shell.execute_reply": "2023-10-29T17:16:44.861363Z"
    },
    "papermill": {
     "duration": 0.020101,
     "end_time": "2023-10-29T17:16:44.864351",
     "exception": false,
     "start_time": "2023-10-29T17:16:44.844250",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_33538/4088201138.py:107: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_33538/4088201138.py:156: DeprecationWarning: `group_by_rolling` is deprecated. It has been renamed to `rolling`.\n",
      "  rolling_group = df.group_by_rolling(\n"
     ]
    }
   ],
   "source": [
    "if TRAIN:\n",
    "    df_train = feat_engineering(df_train_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "2a642493",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools as itt\n",
    "import numbers\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from abc import abstractmethod\n",
    "from typing import Iterable, Tuple, List\n",
    "\n",
    "\n",
    "def embargo(\n",
    "    cv: BaseTimeSeriesCrossValidator,\n",
    "    train_indices: np.ndarray,\n",
    "    test_indices: np.ndarray,\n",
    "    test_fold_end: int,\n",
    ") -> np.ndarray:\n",
    "    last_test_eval_time = cv.eval_times.iloc[cv.indices[:test_fold_end]].max()\n",
    "    min_train_index = len(\n",
    "        cv.pred_times[cv.pred_times <= last_test_eval_time + cv.embargo_td]\n",
    "    )\n",
    "    if min_train_index < cv.indices.shape[0]:\n",
    "        allowed_indices = np.concatenate(\n",
    "            (cv.indices[:test_fold_end], cv.indices[min_train_index:])\n",
    "        )\n",
    "        train_indices = np.intersect1d(train_indices, allowed_indices)\n",
    "    return train_indices\n",
    "\n",
    "\n",
    "def purge(\n",
    "    cv: BaseTimeSeriesCrossValidator,\n",
    "    train_indices: np.ndarray,\n",
    "    test_fold_start: int,\n",
    "    test_fold_end: int,\n",
    ") -> np.ndarray:\n",
    "    time_test_fold_start = cv.pred_times.iloc[test_fold_start]\n",
    "\n",
    "    # The train indices before the start of the test fold, purged.\n",
    "    train_indices_1 = np.intersect1d(\n",
    "        train_indices, cv.indices[cv.eval_times < time_test_fold_start]\n",
    "    )\n",
    "\n",
    "    # The train indices after the end of the test fold.\n",
    "    train_indices_2 = np.intersect1d(train_indices, cv.indices[test_fold_end:])\n",
    "\n",
    "    return np.concatenate((train_indices_1, train_indices_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e976098e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "689f1eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from loguru import logger\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools as itt\n",
    "\n",
    "\n",
    "def time_series_split(X, n_splits=3, n_test_splits=1, embargo_td=2):\n",
    "    factorized_indices = np.unique(X[\"factorized\"])\n",
    "\n",
    "    # Compute the fold boundaries\n",
    "    fold_bounds = [\n",
    "        (fold[0], fold[-1] + 1) for fold in np.array_split(factorized_indices, n_splits)\n",
    "    ]\n",
    "\n",
    "    # Create the list of all tests test_fold_bounds that will become the test sets\n",
    "    selected_fold_bounds = list(itt.combinations(fold_bounds, n_test_splits))\n",
    "\n",
    "    # Reverse to start the testing from the most recent part of the dataset\n",
    "    selected_fold_bounds.reverse()\n",
    "\n",
    "    for fold_bound_list in selected_fold_bounds:\n",
    "        test_factorized_indices = np.empty(0)\n",
    "        test_fold_bounds = []\n",
    "\n",
    "        for fold_start, fold_end in fold_bound_list:\n",
    "            # Records the boundaries of the current test split\n",
    "            if not test_fold_bounds or fold_start != test_fold_bounds[-1][-1]:\n",
    "                test_fold_bounds.append((fold_start, fold_end))\n",
    "            elif fold_start == test_fold_bounds[-1][-1]:\n",
    "                test_fold_bounds[-1] = (test_fold_bounds[-1][0], fold_end)\n",
    "\n",
    "            test_factorized_indices = np.union1d(\n",
    "                test_factorized_indices, factorized_indices[fold_start:fold_end]\n",
    "            ).astype(int)\n",
    "\n",
    "        # Compute the train set indices\n",
    "        train_indices = np.setdiff1d(factorized_indices, test_factorized_indices)\n",
    "\n",
    "        # Purge and embargo can be added here if needed\n",
    "        # ...\n",
    "\n",
    "        yield train_indices, test_factorized_indices\n",
    "\n",
    "\n",
    "# # Example usage:\n",
    "\n",
    "# for train_indices, test_indices in time_series_split(X):\n",
    "#     logger.info(f\"Train indices: {train_indices}\")\n",
    "#     logger.info(f\"Test indices: {test_indices}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "845bcb46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort DataFrame and reset index\n",
    "col_split = \"date_id\"\n",
    "df_train.sort_values([col_split], inplace=True)\n",
    "df_train.reset_index(drop=True, inplace=True)\n",
    "df_train[\"factorized\"] = pd.factorize(df_train[col_split])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "c87f9e59",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-29T17:16:44.890878Z",
     "iopub.status.busy": "2023-10-29T17:16:44.889968Z",
     "iopub.status.idle": "2023-10-29T17:16:44.895347Z",
     "shell.execute_reply": "2023-10-29T17:16:44.894479Z"
    },
    "papermill": {
     "duration": 0.020588,
     "end_time": "2023-10-29T17:16:44.897368",
     "exception": false,
     "start_time": "2023-10-29T17:16:44.876780",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# if TRAIN:\n",
    "#     list_cols_drop = [\"stock_id\", \"date_id\", \"row_id\", \"time_id\", \"target\"]\n",
    "\n",
    "#     y_train = engineered_df[\"target\"]\n",
    "\n",
    "#     X_train = engineered_df.drop(list_cols_drop, axis=1).copy()\n",
    "\n",
    "#     y_train = y_train.loc[X_train.index].squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "e448d6b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_cols_drop = [\"row_id\", \"time_id\", \"date_id\"]\n",
    "df_train.drop(list_cols_drop, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "e5adb023",
   "metadata": {
    "_cell_guid": "334e81a6-b09b-4a3d-96a1-fe0278deff13",
    "_uuid": "4288a49c-b472-4af2-b2bb-95cf0cd2bd6d",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2023-10-29T17:16:44.923231Z",
     "iopub.status.busy": "2023-10-29T17:16:44.922804Z",
     "iopub.status.idle": "2023-10-29T17:16:44.945318Z",
     "shell.execute_reply": "2023-10-29T17:16:44.944512Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.038424,
     "end_time": "2023-10-29T17:16:44.947628",
     "exception": false,
     "start_time": "2023-10-29T17:16:44.909204",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/10/30 22:48:09 INFO mlflow.tracking.fluent: Experiment with name 'optiver_trading_at_the_close_20231030_224809' does not exist. Creating a new experiment.\n"
     ]
    }
   ],
   "source": [
    "gpu_switch = \"OFF\"\n",
    "n_splits = 4\n",
    "n_test_split = 1\n",
    "embargo_td = 100\n",
    "\n",
    "n_repeats = 1\n",
    "nbrnd_erly_stp = 130\n",
    "\n",
    "cv_mthd = \"KF\"\n",
    "\n",
    "# Cross-Validation Setup\n",
    "if TRAIN:\n",
    "    all_cv = {\"KF\": KFold(n_splits=n_splits, shuffle=True, random_state=state)}\n",
    "    cv = all_cv[cv_mthd]\n",
    "\n",
    "    model_params_dict = {\n",
    "        \"LGBMR\": {\n",
    "            \"static_params\": {\n",
    "                \"device\": \"gpu\" if gpu_switch == \"ON\" else \"cpu\",\n",
    "                \"objective\": \"regression_l1\",\n",
    "                \"boosting_type\": \"gbdt\",\n",
    "                \"random_state\": state,\n",
    "                \"verbose\": -1,\n",
    "                \"verbose_eval\": False,\n",
    "            },\n",
    "            \"dynamic_params\": {\n",
    "                \"n_estimators\": {\n",
    "                    \"type\": \"int\",\n",
    "                    \"low\": 620,\n",
    "                    \"high\": 730,\n",
    "                },\n",
    "                \"learning_rate\": {\n",
    "                    \"type\": \"float\",\n",
    "                    \"low\": 0.05,\n",
    "                    \"high\": 0.07,\n",
    "                },\n",
    "                \"max_depth\": {\"type\": \"int\", \"low\": 14, \"high\": 19},\n",
    "                \"num_leaves\": {\n",
    "                    \"type\": \"int\",\n",
    "                    \"low\": 65,\n",
    "                    \"high\": 85,\n",
    "                },\n",
    "                \"min_child_samples\": {\n",
    "                    \"type\": \"int\",\n",
    "                    \"low\": 71,\n",
    "                    \"high\": 74,\n",
    "                },\n",
    "                \"subsample\": {\n",
    "                    \"type\": \"float\",\n",
    "                    \"low\": 0.63,\n",
    "                    \"high\": 0.71,\n",
    "                },\n",
    "                \"colsample_bytree\": {\n",
    "                    \"type\": \"float\",\n",
    "                    \"low\": 0.74,\n",
    "                    \"high\": 0.77,\n",
    "                },\n",
    "                \"min_split_gain\": {\n",
    "                    \"type\": \"float\",\n",
    "                    \"low\": 0.08,\n",
    "                    \"high\": 0.11,\n",
    "                },\n",
    "                \"reg_alpha\": {\n",
    "                    \"type\": \"float\",\n",
    "                    \"low\": 0.7,\n",
    "                    \"high\": 1.3,\n",
    "                },\n",
    "                \"reg_lambda\": {\n",
    "                    \"type\": \"float\",\n",
    "                    \"low\": 1.9,\n",
    "                    \"high\": 3.0,\n",
    "                },\n",
    "            },\n",
    "        },\n",
    "    }\n",
    "\n",
    "    dict_models = {\"LGBMR\": LGBMR}\n",
    "\n",
    "    log_model = True\n",
    "\n",
    "    experiment_date_str = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    experiment_purpose = \"optiver_trading_at_the_close\"\n",
    "    experiment_name = f\"{experiment_purpose}_{experiment_date_str}\"\n",
    "\n",
    "    mlflow.set_experiment(experiment_name)\n",
    "\n",
    "\n",
    "def objective(trial, X, y):\n",
    "    try:\n",
    "        with mlflow.start_run() as run:\n",
    "            mlflow.log_param(\"cv_mthd\", cv_mthd)\n",
    "            mlflow.set_tag(\"experiment_purpose\", experiment_purpose)\n",
    "            mlflow.set_tag(\"experiment_name\", experiment_name)\n",
    "            mlflow.set_tag(\"version_nb\", VERSION_NB)\n",
    "            for model_name, model_class in dict_models.items():\n",
    "                model = create_model(\n",
    "                    trial,\n",
    "                    dict_models[model_name],\n",
    "                    model_params_dict[model_name][\"static_params\"],\n",
    "                    model_params_dict[model_name][\"dynamic_params\"],\n",
    "                )\n",
    "                mae_list = []\n",
    "\n",
    "                log_training_details(logger, model, trial, model_name)\n",
    "\n",
    "                for fold_n, (train_indices, test_indices) in enumerate(\n",
    "                    cv.split(df_train)\n",
    "                ):\n",
    "                    with mlflow.start_run(\n",
    "                        run_name=f\"fold_{fold_n+1}\", nested=True\n",
    "                    ) as nested_run:\n",
    "                        mask_train = df_train[\"factorized\"].isin(train_indices)\n",
    "                        mask_test = df_train[\"factorized\"].isin(test_indices)\n",
    "\n",
    "                        # Filter based on the 'factorized' field\n",
    "                        y_train = df_train.loc[mask_train, \"target\"].squeeze()\n",
    "                        y_val = df_train.loc[mask_test, \"target\"].squeeze()\n",
    "                        X_train = df_train[mask_train].drop(\"target\", axis=1)\n",
    "                        X_val = df_train[mask_test].drop(\"target\", axis=1)\n",
    "\n",
    "                        mlflow.log_param(\"training_data_rows\", X_train.shape[0])\n",
    "                        mlflow.log_param(\"training_data_columns\", X_train.shape[1])\n",
    "\n",
    "                        model.fit(\n",
    "                            X_train,\n",
    "                            y_train,\n",
    "                            eval_set=[(X_val, y_val)],\n",
    "                            eval_metric=\"mae\",\n",
    "                        )\n",
    "\n",
    "                        log_feature_importance(\n",
    "                            model, X, fold_n, experiment_purpose, experiment_date_str\n",
    "                        )\n",
    "\n",
    "                        fold_mae = model.best_score_[\"valid_0\"][\"l1\"]\n",
    "                        mae_list.append(fold_mae)\n",
    "                        logger.info(f\"{fold_n + 1:<5} {'|':<2} {fold_mae:<20}\")\n",
    "\n",
    "                        mlflow.log_metric(\"mae\", fold_mae)\n",
    "                        mlflow.log_param(\"fold_number\", fold_n + 1)\n",
    "                        mlflow.log_param(\"model_name\", model_name)\n",
    "                        mlflow.log_param(\"log_model\", log_model)\n",
    "\n",
    "                        params_to_log = model.get_params()\n",
    "                        mlflow.log_params(params_to_log)\n",
    "\n",
    "                        if log_model:\n",
    "                            current_time_str = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "                            model_log_name = (\n",
    "                                f\"{model_name}_{trial.number}_{current_time_str}\"\n",
    "                            )\n",
    "\n",
    "                            mlflow.log_param(\"model_log_name\", model_log_name)\n",
    "\n",
    "                            mlflow.sklearn.log_model(model, model_log_name)\n",
    "\n",
    "                            mlflow.log_param(\"run_time\", current_time_str)\n",
    "\n",
    "                        nested_run_id = nested_run.info.run_id\n",
    "                        model_path = f\"{path_experiments_storage}/{run.info.experiment_id}/{nested_run_id}/artifacts/{model_log_name}/model.pkl\"\n",
    "                        mlflow.log_param(\"model_path\", model_path)\n",
    "                avg_mae = sum(mae_list) / len(mae_list)\n",
    "\n",
    "                mlflow.log_param(\"model_name\", model_name)\n",
    "\n",
    "                return avg_mae\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"An exception occurred: {e}\")\n",
    "        return float(\"inf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "07a7f6d3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-29T17:16:44.973435Z",
     "iopub.status.busy": "2023-10-29T17:16:44.973048Z",
     "iopub.status.idle": "2023-10-29T17:16:44.977881Z",
     "shell.execute_reply": "2023-10-29T17:16:44.977047Z"
    },
    "papermill": {
     "duration": 0.01997,
     "end_time": "2023-10-29T17:16:44.979759",
     "exception": false,
     "start_time": "2023-10-29T17:16:44.959789",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-10-30 22:48:10.047\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mutils\u001b[0m:\u001b[36mlog_training_details\u001b[0m:\u001b[36m234\u001b[0m - \u001b[1m\u001b[34mTraining model: LGBMR\u001b[0m\u001b[0m\n",
      "\u001b[32m2023-10-30 22:48:10.050\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mutils\u001b[0m:\u001b[36mlog_training_details\u001b[0m:\u001b[36m238\u001b[0m - \u001b[1m\u001b[32mTrial 0    | n_estimators: 642 | learning_rate: 0.05040777135931739 | max_depth: 16 | num_leaves: 76 | min_child_samples: 74 | subsample: 0.6679322050036327 | colsample_bytree: 0.7553414114386076 | min_split_gain: 0.10908807289335976 | reg_alpha: 0.8166491048700117 | reg_lambda: 2.0402331376302523\u001b[0m\u001b[0m\n",
      "\u001b[32m2023-10-30 22:48:10.052\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mutils\u001b[0m:\u001b[36mlog_training_details\u001b[0m:\u001b[36m246\u001b[0m - \u001b[1mFold  |  MAE                 \u001b[0m\n",
      "\u001b[32m2023-10-30 22:48:10.055\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mutils\u001b[0m:\u001b[36mlog_training_details\u001b[0m:\u001b[36m247\u001b[0m - \u001b[1m----- |  --------------------\u001b[0m\n",
      "\u001b[32m2023-10-30 22:48:10.230\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mobjective\u001b[0m:\u001b[36m173\u001b[0m - \u001b[31m\u001b[1mAn exception occurred: LGBMRegressor.fit() got an unexpected keyword argument 'early_stopping_rounds'\u001b[0m\n",
      "\u001b[32m2023-10-30 22:48:10.290\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mutils\u001b[0m:\u001b[36mlog_training_details\u001b[0m:\u001b[36m234\u001b[0m - \u001b[1m\u001b[34mTraining model: LGBMR\u001b[0m\u001b[0m\n",
      "\u001b[32m2023-10-30 22:48:10.293\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mutils\u001b[0m:\u001b[36mlog_training_details\u001b[0m:\u001b[36m238\u001b[0m - \u001b[1m\u001b[32mTrial 1    | n_estimators: 729 | learning_rate: 0.0524360077734099 | max_depth: 16 | num_leaves: 67 | min_child_samples: 72 | subsample: 0.6529374739233206 | colsample_bytree: 0.7640802141072921 | min_split_gain: 0.08673235208294845 | reg_alpha: 0.7884938649860167 | reg_lambda: 2.7984681501520834\u001b[0m\u001b[0m\n",
      "\u001b[32m2023-10-30 22:48:10.294\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mutils\u001b[0m:\u001b[36mlog_training_details\u001b[0m:\u001b[36m246\u001b[0m - \u001b[1mFold  |  MAE                 \u001b[0m\n",
      "\u001b[32m2023-10-30 22:48:10.297\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mutils\u001b[0m:\u001b[36mlog_training_details\u001b[0m:\u001b[36m247\u001b[0m - \u001b[1m----- |  --------------------\u001b[0m\n",
      "\u001b[32m2023-10-30 22:48:10.427\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mobjective\u001b[0m:\u001b[36m173\u001b[0m - \u001b[31m\u001b[1mAn exception occurred: LGBMRegressor.fit() got an unexpected keyword argument 'early_stopping_rounds'\u001b[0m\n",
      "\u001b[32m2023-10-30 22:48:10.489\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mutils\u001b[0m:\u001b[36mlog_training_details\u001b[0m:\u001b[36m234\u001b[0m - \u001b[1m\u001b[34mTraining model: LGBMR\u001b[0m\u001b[0m\n",
      "\u001b[32m2023-10-30 22:48:10.492\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mutils\u001b[0m:\u001b[36mlog_training_details\u001b[0m:\u001b[36m238\u001b[0m - \u001b[1m\u001b[32mTrial 2    | n_estimators: 658 | learning_rate: 0.05954952189777091 | max_depth: 16 | num_leaves: 80 | min_child_samples: 72 | subsample: 0.6401260186925142 | colsample_bytree: 0.7576182813571793 | min_split_gain: 0.09720377284888898 | reg_alpha: 1.1707412970411057 | reg_lambda: 2.3927766590840167\u001b[0m\u001b[0m\n",
      "\u001b[32m2023-10-30 22:48:10.493\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mutils\u001b[0m:\u001b[36mlog_training_details\u001b[0m:\u001b[36m246\u001b[0m - \u001b[1mFold  |  MAE                 \u001b[0m\n",
      "\u001b[32m2023-10-30 22:48:10.495\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mutils\u001b[0m:\u001b[36mlog_training_details\u001b[0m:\u001b[36m247\u001b[0m - \u001b[1m----- |  --------------------\u001b[0m\n",
      "\u001b[32m2023-10-30 22:48:10.614\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mobjective\u001b[0m:\u001b[36m173\u001b[0m - \u001b[31m\u001b[1mAn exception occurred: LGBMRegressor.fit() got an unexpected keyword argument 'early_stopping_rounds'\u001b[0m\n",
      "\u001b[32m2023-10-30 22:48:10.672\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mutils\u001b[0m:\u001b[36mlog_training_details\u001b[0m:\u001b[36m234\u001b[0m - \u001b[1m\u001b[34mTraining model: LGBMR\u001b[0m\u001b[0m\n",
      "\u001b[32m2023-10-30 22:48:10.674\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mutils\u001b[0m:\u001b[36mlog_training_details\u001b[0m:\u001b[36m238\u001b[0m - \u001b[1m\u001b[32mTrial 3    | n_estimators: 664 | learning_rate: 0.05199789946958847 | max_depth: 15 | num_leaves: 72 | min_child_samples: 73 | subsample: 0.669704948778001 | colsample_bytree: 0.758520419684361 | min_split_gain: 0.08963911484078119 | reg_alpha: 0.8550780578434651 | reg_lambda: 2.196371851811326\u001b[0m\u001b[0m\n",
      "\u001b[32m2023-10-30 22:48:10.676\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mutils\u001b[0m:\u001b[36mlog_training_details\u001b[0m:\u001b[36m246\u001b[0m - \u001b[1mFold  |  MAE                 \u001b[0m\n",
      "\u001b[32m2023-10-30 22:48:10.678\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mutils\u001b[0m:\u001b[36mlog_training_details\u001b[0m:\u001b[36m247\u001b[0m - \u001b[1m----- |  --------------------\u001b[0m\n",
      "\u001b[32m2023-10-30 22:48:10.807\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mobjective\u001b[0m:\u001b[36m173\u001b[0m - \u001b[31m\u001b[1mAn exception occurred: LGBMRegressor.fit() got an unexpected keyword argument 'early_stopping_rounds'\u001b[0m\n",
      "\u001b[32m2023-10-30 22:48:10.865\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mutils\u001b[0m:\u001b[36mlog_training_details\u001b[0m:\u001b[36m234\u001b[0m - \u001b[1m\u001b[34mTraining model: LGBMR\u001b[0m\u001b[0m\n",
      "\u001b[32m2023-10-30 22:48:10.867\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mutils\u001b[0m:\u001b[36mlog_training_details\u001b[0m:\u001b[36m238\u001b[0m - \u001b[1m\u001b[32mTrial 4    | n_estimators: 640 | learning_rate: 0.050399971418665694 | max_depth: 15 | num_leaves: 72 | min_child_samples: 73 | subsample: 0.6332705381207921 | colsample_bytree: 0.744519116068621 | min_split_gain: 0.0987750501945766 | reg_alpha: 1.0373745851823697 | reg_lambda: 2.9731579240424213\u001b[0m\u001b[0m\n",
      "\u001b[32m2023-10-30 22:48:10.869\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mutils\u001b[0m:\u001b[36mlog_training_details\u001b[0m:\u001b[36m246\u001b[0m - \u001b[1mFold  |  MAE                 \u001b[0m\n",
      "\u001b[32m2023-10-30 22:48:10.871\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mutils\u001b[0m:\u001b[36mlog_training_details\u001b[0m:\u001b[36m247\u001b[0m - \u001b[1m----- |  --------------------\u001b[0m\n",
      "\u001b[32m2023-10-30 22:48:10.994\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mobjective\u001b[0m:\u001b[36m173\u001b[0m - \u001b[31m\u001b[1mAn exception occurred: LGBMRegressor.fit() got an unexpected keyword argument 'early_stopping_rounds'\u001b[0m\n",
      "\u001b[32m2023-10-30 22:48:11.050\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mutils\u001b[0m:\u001b[36mlog_training_details\u001b[0m:\u001b[36m234\u001b[0m - \u001b[1m\u001b[34mTraining model: LGBMR\u001b[0m\u001b[0m\n",
      "\u001b[32m2023-10-30 22:48:11.053\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mutils\u001b[0m:\u001b[36mlog_training_details\u001b[0m:\u001b[36m238\u001b[0m - \u001b[1m\u001b[32mTrial 5    | n_estimators: 640 | learning_rate: 0.058460151502495436 | max_depth: 17 | num_leaves: 78 | min_child_samples: 71 | subsample: 0.6510151791932035 | colsample_bytree: 0.7638457810607169 | min_split_gain: 0.09154517902902573 | reg_alpha: 0.9851827309322148 | reg_lambda: 2.8416895039368715\u001b[0m\u001b[0m\n",
      "\u001b[32m2023-10-30 22:48:11.056\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mutils\u001b[0m:\u001b[36mlog_training_details\u001b[0m:\u001b[36m246\u001b[0m - \u001b[1mFold  |  MAE                 \u001b[0m\n",
      "\u001b[32m2023-10-30 22:48:11.058\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mutils\u001b[0m:\u001b[36mlog_training_details\u001b[0m:\u001b[36m247\u001b[0m - \u001b[1m----- |  --------------------\u001b[0m\n",
      "\u001b[32m2023-10-30 22:48:11.185\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mobjective\u001b[0m:\u001b[36m173\u001b[0m - \u001b[31m\u001b[1mAn exception occurred: LGBMRegressor.fit() got an unexpected keyword argument 'early_stopping_rounds'\u001b[0m\n",
      "\u001b[32m2023-10-30 22:48:11.240\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mutils\u001b[0m:\u001b[36mlog_training_details\u001b[0m:\u001b[36m234\u001b[0m - \u001b[1m\u001b[34mTraining model: LGBMR\u001b[0m\u001b[0m\n",
      "\u001b[32m2023-10-30 22:48:11.243\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mutils\u001b[0m:\u001b[36mlog_training_details\u001b[0m:\u001b[36m238\u001b[0m - \u001b[1m\u001b[32mTrial 6    | n_estimators: 730 | learning_rate: 0.054047707403095904 | max_depth: 17 | num_leaves: 82 | min_child_samples: 74 | subsample: 0.6773922137608771 | colsample_bytree: 0.7602671835824791 | min_split_gain: 0.08760463772531538 | reg_alpha: 1.215914671302774 | reg_lambda: 2.6224951621530455\u001b[0m\u001b[0m\n",
      "\u001b[32m2023-10-30 22:48:11.245\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mutils\u001b[0m:\u001b[36mlog_training_details\u001b[0m:\u001b[36m246\u001b[0m - \u001b[1mFold  |  MAE                 \u001b[0m\n",
      "\u001b[32m2023-10-30 22:48:11.247\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mutils\u001b[0m:\u001b[36mlog_training_details\u001b[0m:\u001b[36m247\u001b[0m - \u001b[1m----- |  --------------------\u001b[0m\n",
      "\u001b[32m2023-10-30 22:48:11.366\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mobjective\u001b[0m:\u001b[36m173\u001b[0m - \u001b[31m\u001b[1mAn exception occurred: LGBMRegressor.fit() got an unexpected keyword argument 'early_stopping_rounds'\u001b[0m\n",
      "\u001b[32m2023-10-30 22:48:11.419\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mutils\u001b[0m:\u001b[36mlog_training_details\u001b[0m:\u001b[36m234\u001b[0m - \u001b[1m\u001b[34mTraining model: LGBMR\u001b[0m\u001b[0m\n",
      "\u001b[32m2023-10-30 22:48:11.422\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mutils\u001b[0m:\u001b[36mlog_training_details\u001b[0m:\u001b[36m238\u001b[0m - \u001b[1m\u001b[32mTrial 7    | n_estimators: 668 | learning_rate: 0.05395323522941771 | max_depth: 19 | num_leaves: 70 | min_child_samples: 71 | subsample: 0.6712406553632924 | colsample_bytree: 0.7403841509605182 | min_split_gain: 0.09457236795655304 | reg_alpha: 0.7172100683292321 | reg_lambda: 2.08075521793345\u001b[0m\u001b[0m\n",
      "\u001b[32m2023-10-30 22:48:11.425\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mutils\u001b[0m:\u001b[36mlog_training_details\u001b[0m:\u001b[36m246\u001b[0m - \u001b[1mFold  |  MAE                 \u001b[0m\n",
      "\u001b[32m2023-10-30 22:48:11.427\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mutils\u001b[0m:\u001b[36mlog_training_details\u001b[0m:\u001b[36m247\u001b[0m - \u001b[1m----- |  --------------------\u001b[0m\n",
      "\u001b[32m2023-10-30 22:48:11.542\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mobjective\u001b[0m:\u001b[36m173\u001b[0m - \u001b[31m\u001b[1mAn exception occurred: LGBMRegressor.fit() got an unexpected keyword argument 'early_stopping_rounds'\u001b[0m\n",
      "\u001b[32m2023-10-30 22:48:11.593\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mutils\u001b[0m:\u001b[36mlog_training_details\u001b[0m:\u001b[36m234\u001b[0m - \u001b[1m\u001b[34mTraining model: LGBMR\u001b[0m\u001b[0m\n",
      "\u001b[32m2023-10-30 22:48:11.596\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mutils\u001b[0m:\u001b[36mlog_training_details\u001b[0m:\u001b[36m238\u001b[0m - \u001b[1m\u001b[32mTrial 8    | n_estimators: 631 | learning_rate: 0.06306344160458505 | max_depth: 15 | num_leaves: 67 | min_child_samples: 71 | subsample: 0.6850349492663599 | colsample_bytree: 0.7490959344696757 | min_split_gain: 0.104825711115356 | reg_alpha: 0.7599306471399434 | reg_lambda: 2.5839610406356295\u001b[0m\u001b[0m\n",
      "\u001b[32m2023-10-30 22:48:11.598\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mutils\u001b[0m:\u001b[36mlog_training_details\u001b[0m:\u001b[36m246\u001b[0m - \u001b[1mFold  |  MAE                 \u001b[0m\n",
      "\u001b[32m2023-10-30 22:48:11.600\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mutils\u001b[0m:\u001b[36mlog_training_details\u001b[0m:\u001b[36m247\u001b[0m - \u001b[1m----- |  --------------------\u001b[0m\n",
      "\u001b[32m2023-10-30 22:48:11.717\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mobjective\u001b[0m:\u001b[36m173\u001b[0m - \u001b[31m\u001b[1mAn exception occurred: LGBMRegressor.fit() got an unexpected keyword argument 'early_stopping_rounds'\u001b[0m\n",
      "\u001b[32m2023-10-30 22:48:11.770\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mutils\u001b[0m:\u001b[36mlog_training_details\u001b[0m:\u001b[36m234\u001b[0m - \u001b[1m\u001b[34mTraining model: LGBMR\u001b[0m\u001b[0m\n",
      "\u001b[32m2023-10-30 22:48:11.772\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mutils\u001b[0m:\u001b[36mlog_training_details\u001b[0m:\u001b[36m238\u001b[0m - \u001b[1m\u001b[32mTrial 9    | n_estimators: 682 | learning_rate: 0.06914013873996513 | max_depth: 14 | num_leaves: 67 | min_child_samples: 74 | subsample: 0.7030101310572743 | colsample_bytree: 0.7478234998135029 | min_split_gain: 0.10881803268041977 | reg_alpha: 1.1082652904306287 | reg_lambda: 2.055389926639667\u001b[0m\u001b[0m\n",
      "\u001b[32m2023-10-30 22:48:11.774\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mutils\u001b[0m:\u001b[36mlog_training_details\u001b[0m:\u001b[36m246\u001b[0m - \u001b[1mFold  |  MAE                 \u001b[0m\n",
      "\u001b[32m2023-10-30 22:48:11.775\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mutils\u001b[0m:\u001b[36mlog_training_details\u001b[0m:\u001b[36m247\u001b[0m - \u001b[1m----- |  --------------------\u001b[0m\n",
      "\u001b[32m2023-10-30 22:48:11.887\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mobjective\u001b[0m:\u001b[36m173\u001b[0m - \u001b[31m\u001b[1mAn exception occurred: LGBMRegressor.fit() got an unexpected keyword argument 'early_stopping_rounds'\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Run the Optuna study\n",
    "if TRAIN:\n",
    "    study = optuna.create_study(\n",
    "        direction=\"minimize\",\n",
    "        study_name=\"Your Study Name\",\n",
    "        load_if_exists=True,\n",
    "    )\n",
    "    study.optimize(lambda trial: objective(trial, X_train, y_train), n_trials=N_TRIALS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "196384e6",
   "metadata": {
    "papermill": {
     "duration": 0.068637,
     "end_time": "2023-10-29T17:16:45.060324",
     "exception": false,
     "start_time": "2023-10-29T17:16:44.991687",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f014d024",
   "metadata": {
    "_cell_guid": "0a91a281-846a-4119-9fee-268d2f6e2195",
    "_uuid": "02e23e01-f0f7-426e-b170-55ac4273b88f",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2023-10-29T17:16:45.086686Z",
     "iopub.status.busy": "2023-10-29T17:16:45.086028Z",
     "iopub.status.idle": "2023-10-29T17:16:45.092575Z",
     "shell.execute_reply": "2023-10-29T17:16:45.091806Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.022229,
     "end_time": "2023-10-29T17:16:45.094530",
     "exception": false,
     "start_time": "2023-10-29T17:16:45.072301",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def save_and_version_df(df_new):\n",
    "    # Read existing df_runs CSV files and concatenate them with the new df_runs\n",
    "    existing_files = [\n",
    "        f for f in os.listdir() if f.startswith(\"df_runs_\") and f.endswith(\".csv\")\n",
    "    ]\n",
    "    dfs = [pd.read_csv(f) for f in existing_files]\n",
    "\n",
    "    for old_file, old_df in zip(existing_files, dfs):\n",
    "        print(f\"Removed old file: {old_file}, Shape: {old_df.shape}\")\n",
    "        os.remove(old_file)\n",
    "\n",
    "    dfs.append(df_new)\n",
    "    df_concatenated = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "    # Remove duplicates\n",
    "    df_unique = df_concatenated.drop_duplicates()\n",
    "\n",
    "    # Save the new concatenated and deduplicated df_runs to a new versioned CSV file\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M\")\n",
    "    csv_filename = f\"df_runs_{timestamp}.csv\"\n",
    "    df_unique.to_csv(csv_filename, index=False)\n",
    "\n",
    "    print(f\"DataFrame saved to {csv_filename}, Shape: {df_unique.shape}\")\n",
    "\n",
    "    return df_unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cbfbee6e",
   "metadata": {
    "_cell_guid": "51030a77-aba3-469f-9c08-7963bc8a09d2",
    "_uuid": "6ff7d358-62f0-4ac9-b028-d1c920bb4eaf",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2023-10-29T17:16:45.120649Z",
     "iopub.status.busy": "2023-10-29T17:16:45.119992Z",
     "iopub.status.idle": "2023-10-29T17:16:45.126734Z",
     "shell.execute_reply": "2023-10-29T17:16:45.125940Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.022231,
     "end_time": "2023-10-29T17:16:45.128848",
     "exception": false,
     "start_time": "2023-10-29T17:16:45.106617",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def gather_runs_data():\n",
    "    experiments = client.search_experiments()\n",
    "    all_runs_data = []\n",
    "    for exp in experiments:\n",
    "        experiment_id = exp.experiment_id\n",
    "        run_infos = client.search_runs(experiment_ids=[experiment_id])\n",
    "\n",
    "        for run_info in run_infos:\n",
    "            run_data = {\n",
    "                \"experiment_id\": experiment_id,\n",
    "                \"experiment_name\": exp.name,\n",
    "                \"run_id\": run_info.info.run_id,\n",
    "            }\n",
    "\n",
    "            # Add metrics to run_data\n",
    "            for key, value in run_info.data.metrics.items():\n",
    "                run_data[f\"{key}\"] = value\n",
    "\n",
    "            # Add params to run_data\n",
    "            for key, value in run_info.data.params.items():\n",
    "                run_data[f\"{key}\"] = value\n",
    "\n",
    "            all_runs_data.append(run_data)\n",
    "\n",
    "    df_runs_new = pd.DataFrame(all_runs_data)\n",
    "\n",
    "    df_runs_unique = save_and_version_df(df_runs_new)\n",
    "\n",
    "    df_runs_unique = df_runs_unique[~df_runs_unique[\"model_name\"].isna()]\n",
    "\n",
    "    return df_runs_unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "47d8eb6e",
   "metadata": {
    "_cell_guid": "50c86d61-2198-45c1-8106-1ff027f1f87a",
    "_uuid": "6491f1de-d47e-4d76-8eb4-7cf869d5313a",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2023-10-29T17:16:45.155472Z",
     "iopub.status.busy": "2023-10-29T17:16:45.154828Z",
     "iopub.status.idle": "2023-10-29T17:16:45.159822Z",
     "shell.execute_reply": "2023-10-29T17:16:45.158914Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.020934,
     "end_time": "2023-10-29T17:16:45.161971",
     "exception": false,
     "start_time": "2023-10-29T17:16:45.141037",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "cols_id = [\"experiment_id\", \"run_id\"]\n",
    "cols_info = [\"training_data_rows\", \"training_data_columns\"]\n",
    "cols_metrics = [\"mae\"]\n",
    "cols_param_exp = [\"date_exp\", \"log_model\", \"cv_mthd\", \"fold_number\"]\n",
    "cols_others_info = [\n",
    "    \"experiment_name\",\n",
    "    \"model_path\",\n",
    "    \"device\",\n",
    "    \"n_jobs\",\n",
    "    \"importance_type\",\n",
    "    \"random_state\",\n",
    "    \"model_name\",\n",
    "    \"subsample_freq\",\n",
    "    \"verbose_eval\",\n",
    "    \"class_weight\",\n",
    "    \"model_log_name\",\n",
    "    \"verbose\",\n",
    "    \"silent\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5bc94fb6",
   "metadata": {
    "_cell_guid": "cb16e35c-caf0-4a29-803b-ce578697b66a",
    "_uuid": "d14695ff-57de-4e99-833e-5ef5e6f1038d",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2023-10-29T17:16:45.187952Z",
     "iopub.status.busy": "2023-10-29T17:16:45.187557Z",
     "iopub.status.idle": "2023-10-29T17:16:45.194499Z",
     "shell.execute_reply": "2023-10-29T17:16:45.193207Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.022574,
     "end_time": "2023-10-29T17:16:45.196702",
     "exception": false,
     "start_time": "2023-10-29T17:16:45.174128",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame saved to df_runs_20231030_0743.csv, Shape: (10, 4)\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'model_name'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/training/lib/python3.10/site-packages/pandas/core/indexes/base.py:3790\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3789\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 3790\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_engine\u001b[39m.\u001b[39;49mget_loc(casted_key)\n\u001b[1;32m   3791\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n",
      "File \u001b[0;32mindex.pyx:152\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mindex.pyx:181\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7080\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7088\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'model_name'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m/home/sam/github/mlops-hub/training-optiver-trading-at-the-close.ipynb Cell 16\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/sam/github/mlops-hub/training-optiver-trading-at-the-close.ipynb#X21sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mif\u001b[39;00m TRAIN:\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/sam/github/mlops-hub/training-optiver-trading-at-the-close.ipynb#X21sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     df_runs \u001b[39m=\u001b[39m gather_runs_data()\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/sam/github/mlops-hub/training-optiver-trading-at-the-close.ipynb#X21sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     df_runs \u001b[39m=\u001b[39m df_runs\u001b[39m.\u001b[39mdrop_duplicates()\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/sam/github/mlops-hub/training-optiver-trading-at-the-close.ipynb#X21sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     cols_params \u001b[39m=\u001b[39m df_runs\u001b[39m.\u001b[39mcolumns\u001b[39m.\u001b[39mtolist()\n",
      "\u001b[1;32m/home/sam/github/mlops-hub/training-optiver-trading-at-the-close.ipynb Cell 16\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/sam/github/mlops-hub/training-optiver-trading-at-the-close.ipynb#X21sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m df_runs_new \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame(all_runs_data)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/sam/github/mlops-hub/training-optiver-trading-at-the-close.ipynb#X21sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m df_runs_unique \u001b[39m=\u001b[39m save_and_version_df(df_runs_new)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/sam/github/mlops-hub/training-optiver-trading-at-the-close.ipynb#X21sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m df_runs_unique \u001b[39m=\u001b[39m df_runs_unique[\u001b[39m~\u001b[39mdf_runs_unique[\u001b[39m\"\u001b[39;49m\u001b[39mmodel_name\u001b[39;49m\u001b[39m\"\u001b[39;49m]\u001b[39m.\u001b[39misna()]\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/sam/github/mlops-hub/training-optiver-trading-at-the-close.ipynb#X21sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m \u001b[39mreturn\u001b[39;00m df_runs_unique\n",
      "File \u001b[0;32m~/miniconda3/envs/training/lib/python3.10/site-packages/pandas/core/frame.py:3896\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3894\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns\u001b[39m.\u001b[39mnlevels \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m   3895\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 3896\u001b[0m indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcolumns\u001b[39m.\u001b[39;49mget_loc(key)\n\u001b[1;32m   3897\u001b[0m \u001b[39mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   3898\u001b[0m     indexer \u001b[39m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m~/miniconda3/envs/training/lib/python3.10/site-packages/pandas/core/indexes/base.py:3797\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3792\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(casted_key, \u001b[39mslice\u001b[39m) \u001b[39mor\u001b[39;00m (\n\u001b[1;32m   3793\u001b[0m         \u001b[39misinstance\u001b[39m(casted_key, abc\u001b[39m.\u001b[39mIterable)\n\u001b[1;32m   3794\u001b[0m         \u001b[39mand\u001b[39;00m \u001b[39many\u001b[39m(\u001b[39misinstance\u001b[39m(x, \u001b[39mslice\u001b[39m) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m casted_key)\n\u001b[1;32m   3795\u001b[0m     ):\n\u001b[1;32m   3796\u001b[0m         \u001b[39mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[0;32m-> 3797\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n\u001b[1;32m   3798\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m   3799\u001b[0m     \u001b[39m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3800\u001b[0m     \u001b[39m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3801\u001b[0m     \u001b[39m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3802\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'model_name'"
     ]
    }
   ],
   "source": [
    "if TRAIN:\n",
    "    df_runs = gather_runs_data()\n",
    "    df_runs = df_runs.drop_duplicates()\n",
    "    cols_params = df_runs.columns.tolist()\n",
    "\n",
    "    for col in cols_param_exp + cols_metrics + cols_info + cols_id + cols_others_info:\n",
    "        cols_params.remove(col)\n",
    "\n",
    "    new_col_order = (\n",
    "        cols_param_exp\n",
    "        + cols_metrics\n",
    "        + cols_info\n",
    "        + cols_id\n",
    "        + cols_params\n",
    "        + cols_others_info\n",
    "    )\n",
    "\n",
    "    df_runs = df_runs[~df_runs[\"fold_number\"].isna()]\n",
    "    df_runs = df_runs[new_col_order]\n",
    "\n",
    "    df_runs[\"experiment_time\"] = df_runs[\"experiment_name\"].apply(\n",
    "        lambda x: x.split(\"_\")[-1]\n",
    "    )\n",
    "    df_runs[\"experiment_date\"] = df_runs[\"experiment_name\"].apply(\n",
    "        lambda x: x.split(\"_\")[-2]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b8eb92",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-29T17:16:45.222380Z",
     "iopub.status.busy": "2023-10-29T17:16:45.222030Z",
     "iopub.status.idle": "2023-10-29T17:16:45.226319Z",
     "shell.execute_reply": "2023-10-29T17:16:45.225208Z"
    },
    "papermill": {
     "duration": 0.019686,
     "end_time": "2023-10-29T17:16:45.228434",
     "exception": false,
     "start_time": "2023-10-29T17:16:45.208748",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# df_runs[[\"date_exp\"] + cols_info + cols_metrics + cols_params].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66dc0b38",
   "metadata": {
    "papermill": {
     "duration": 0.011698,
     "end_time": "2023-10-29T17:16:45.252218",
     "exception": false,
     "start_time": "2023-10-29T17:16:45.240520",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cb939cd",
   "metadata": {
    "papermill": {
     "duration": 0.011683,
     "end_time": "2023-10-29T17:16:45.275882",
     "exception": false,
     "start_time": "2023-10-29T17:16:45.264199",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51db42bb",
   "metadata": {
    "papermill": {
     "duration": 0.011765,
     "end_time": "2023-10-29T17:16:45.299514",
     "exception": false,
     "start_time": "2023-10-29T17:16:45.287749",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab3e4a51",
   "metadata": {
    "papermill": {
     "duration": 0.011543,
     "end_time": "2023-10-29T17:16:45.322883",
     "exception": false,
     "start_time": "2023-10-29T17:16:45.311340",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28dacb77",
   "metadata": {
    "papermill": {
     "duration": 0.011605,
     "end_time": "2023-10-29T17:16:45.346309",
     "exception": false,
     "start_time": "2023-10-29T17:16:45.334704",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e153e5",
   "metadata": {
    "papermill": {
     "duration": 0.011661,
     "end_time": "2023-10-29T17:16:45.369942",
     "exception": false,
     "start_time": "2023-10-29T17:16:45.358281",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85dcbaec",
   "metadata": {
    "papermill": {
     "duration": 0.011597,
     "end_time": "2023-10-29T17:16:45.393480",
     "exception": false,
     "start_time": "2023-10-29T17:16:45.381883",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e06150",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-29T17:16:45.419305Z",
     "iopub.status.busy": "2023-10-29T17:16:45.418959Z",
     "iopub.status.idle": "2023-10-29T17:16:45.429525Z",
     "shell.execute_reply": "2023-10-29T17:16:45.428527Z"
    },
    "papermill": {
     "duration": 0.02626,
     "end_time": "2023-10-29T17:16:45.431618",
     "exception": false,
     "start_time": "2023-10-29T17:16:45.405358",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def aggregate_feature_importance(path_to_csvs):\n",
    "    \"\"\"\n",
    "    Aggregates feature importances from multiple CSV files and calculates mean importance.\n",
    "    \"\"\"\n",
    "    all_files = glob.glob(f\"{path_to_csvs}/*.csv\")\n",
    "    list_of_dfs = [pd.read_csv(filename) for filename in all_files]\n",
    "\n",
    "    if not list_of_dfs:\n",
    "        print(\"No CSV files found.\")\n",
    "        return None\n",
    "\n",
    "    # Concatenate all Jdataframes\n",
    "    aggregated_df = pd.concat(list_of_dfs, ignore_index=True)\n",
    "\n",
    "    # Calculate mean importance for each feature\n",
    "    mean_importance = (\n",
    "        aggregated_df.groupby(\"Feature\")[\"Importance\"].mean().reset_index()\n",
    "    )\n",
    "    mean_importance = mean_importance.sort_values(by=\"Importance\", ascending=False)\n",
    "\n",
    "    return mean_importance\n",
    "\n",
    "\n",
    "def analyze_feature_importance(aggregated_df, top_n=100):\n",
    "    \"\"\"\n",
    "    Analyzes aggregated feature importances.\n",
    "    \"\"\"\n",
    "    if aggregated_df is None:\n",
    "        print(\"No aggregated DataFrame provided.\")\n",
    "        return None\n",
    "\n",
    "    top_n_features = aggregated_df.nlargest(top_n, \"Importance\")\n",
    "\n",
    "    return top_n_features\n",
    "\n",
    "\n",
    "def count_top_n_features(path_to_csvs, top_n=100):\n",
    "    \"\"\"\n",
    "    Counts how many times each feature appears in the top N most important features across multiple experiments.\n",
    "\n",
    "    Parameters:\n",
    "    path_to_csvs (str): Path to the folder containing feature importance CSV files.\n",
    "    top_n (int): The number of top features to consider.\n",
    "\n",
    "    Returns:\n",
    "    DataFrame: A DataFrame showing the count of appearances in the top N features for each feature.\n",
    "    \"\"\"\n",
    "\n",
    "    all_files = glob.glob(f\"{path_to_csvs}/*.csv\")\n",
    "    list_of_dfs = [pd.read_csv(filename) for filename in all_files]\n",
    "\n",
    "    if not list_of_dfs:\n",
    "        print(\"No CSV files found.\")\n",
    "        return None\n",
    "\n",
    "    feature_count = {}\n",
    "\n",
    "    # Count how many times each feature appears in the top N features\n",
    "    for df in list_of_dfs:\n",
    "        top_features = df.nlargest(top_n, \"Importance\")[\"Feature\"].tolist()\n",
    "        for feature in top_features:\n",
    "            if feature in feature_count:\n",
    "                feature_count[feature] += 1\n",
    "            else:\n",
    "                feature_count[feature] = 1\n",
    "\n",
    "    # Convert to DataFrame for easier analysis\n",
    "    feature_count_df = pd.DataFrame(\n",
    "        list(feature_count.items()), columns=[\"Feature\", \"Count\"]\n",
    "    )\n",
    "    feature_count_df = feature_count_df.sort_values(by=\"Count\", ascending=False)\n",
    "\n",
    "    return feature_count_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1514c6c7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-29T17:16:45.458204Z",
     "iopub.status.busy": "2023-10-29T17:16:45.457809Z",
     "iopub.status.idle": "2023-10-29T17:16:45.463640Z",
     "shell.execute_reply": "2023-10-29T17:16:45.462546Z"
    },
    "papermill": {
     "duration": 0.021986,
     "end_time": "2023-10-29T17:16:45.465755",
     "exception": false,
     "start_time": "2023-10-29T17:16:45.443769",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Example usageJ\n",
    "if TRAIN:\n",
    "    path_to_csvs = \"/kaggle/working/\"\n",
    "    aggregated_df = aggregate_feature_importance(path_to_csvs)\n",
    "    top_n_features = analyze_feature_importance(aggregated_df)\n",
    "    list_features = [\n",
    "        col for col in list(top_n_features[\"Feature\"]) if col in engineered_df.columns\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38581647",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-29T17:16:45.492313Z",
     "iopub.status.busy": "2023-10-29T17:16:45.491480Z",
     "iopub.status.idle": "2023-10-29T17:16:45.495952Z",
     "shell.execute_reply": "2023-10-29T17:16:45.494959Z"
    },
    "papermill": {
     "duration": 0.019724,
     "end_time": "2023-10-29T17:16:45.497995",
     "exception": false,
     "start_time": "2023-10-29T17:16:45.478271",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# top_n_features[top_n_features[\"Feature\"].isin(list_features)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c4f6cfd",
   "metadata": {
    "_cell_guid": "95e6c8a8-62ed-4412-944a-5b08508702fc",
    "_uuid": "60c6783c-1e42-43ca-ba32-bda313deb585",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2023-10-29T17:16:45.525266Z",
     "iopub.status.busy": "2023-10-29T17:16:45.524910Z",
     "iopub.status.idle": "2023-10-29T17:16:45.532612Z",
     "shell.execute_reply": "2023-10-29T17:16:45.531445Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.02353,
     "end_time": "2023-10-29T17:16:45.534872",
     "exception": false,
     "start_time": "2023-10-29T17:16:45.511342",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def ensemble_predict(model_paths, X_test):\n",
    "    models = []\n",
    "    predictions = []\n",
    "\n",
    "    # Load models based on full artifact paths\n",
    "    for model_path in model_paths:\n",
    "        try:\n",
    "            # If using direct path to pkl\n",
    "            if model_path.endswith(\".pkl\"):\n",
    "                model = joblib.load(model_path)\n",
    "            else:\n",
    "                print(f\"Unsupported model format for {model_path}. Skipping.\")\n",
    "                continue  # Skip this iteration\n",
    "\n",
    "            models.append(model)\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to load model at {model_path}. Error: {e}\")\n",
    "\n",
    "    # Make predictions\n",
    "    for model in models:\n",
    "        try:\n",
    "            pred = model.predict(X_test)\n",
    "            predictions.append(pred)\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to make prediction with model. Error: {e}\")\n",
    "\n",
    "    # Average predictions\n",
    "    if len(predictions) > 0:\n",
    "        ensemble_pred = np.mean(predictions, axis=0)\n",
    "    else:\n",
    "        print(\"No valid models loaded. Cannot make ensemble predictions.\")\n",
    "        ensemble_pred = None\n",
    "\n",
    "    return ensemble_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12416025",
   "metadata": {
    "papermill": {
     "duration": 0.012259,
     "end_time": "2023-10-29T17:16:45.559669",
     "exception": false,
     "start_time": "2023-10-29T17:16:45.547410",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d93ff597",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-29T17:16:45.587028Z",
     "iopub.status.busy": "2023-10-29T17:16:45.586197Z",
     "iopub.status.idle": "2023-10-29T17:16:45.591083Z",
     "shell.execute_reply": "2023-10-29T17:16:45.589955Z"
    },
    "papermill": {
     "duration": 0.021227,
     "end_time": "2023-10-29T17:16:45.593263",
     "exception": false,
     "start_time": "2023-10-29T17:16:45.572036",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# df_runs = df_runs[df_runs['date_exp'] == 134604.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56263b18",
   "metadata": {
    "papermill": {
     "duration": 0.011923,
     "end_time": "2023-10-29T17:16:45.617942",
     "exception": false,
     "start_time": "2023-10-29T17:16:45.606019",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f969548c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-29T17:16:45.644864Z",
     "iopub.status.busy": "2023-10-29T17:16:45.644479Z",
     "iopub.status.idle": "2023-10-29T17:16:45.653112Z",
     "shell.execute_reply": "2023-10-29T17:16:45.651960Z"
    },
    "papermill": {
     "duration": 0.025212,
     "end_time": "2023-10-29T17:16:45.655277",
     "exception": false,
     "start_time": "2023-10-29T17:16:45.630065",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "models_dir = \"model_1\"\n",
    "\n",
    "if TRAIN:\n",
    "    model_paths = list(df_runs[df_runs[\"experiment_time\"] == \"152953\"][\"model_path\"])\n",
    "\n",
    "    if not os.path.exists(models_dir):\n",
    "        os.makedirs(models_dir)\n",
    "\n",
    "    for model_path in model_paths:\n",
    "        print(f\"Checking if model path exists: {model_path}\")\n",
    "\n",
    "        if not os.path.exists(model_path):\n",
    "            print(f\"File does not exist: {model_path}\")\n",
    "            continue  # Skip to the next iteration\n",
    "\n",
    "        specific_part = model_path.split(\"/\")[-2]\n",
    "        dest_path = os.path.join(models_dir, f\"{specific_part}.pkl\")\n",
    "        if not os.path.exists(dest_path):\n",
    "            print(f\"Copying from {model_path} to {dest_path}\")\n",
    "            shutil.copy(model_path, dest_path)\n",
    "        else:\n",
    "            print(f\"File {dest_path} already exists. Skipping copy.\")\n",
    "\n",
    "    zipf = zipfile.ZipFile(\n",
    "        f\"/kaggle/working/{models_dir}.zip\", \"w\", zipfile.ZIP_DEFLATED\n",
    "    )\n",
    "\n",
    "    # Navigate through the folder and add each file to the ZIP\n",
    "    for root, dirs, files in os.walk(f\"/kaggle/working/{models_dir}\"):\n",
    "        for file in files:\n",
    "            zipf.write(\n",
    "                os.path.join(root, file),\n",
    "                os.path.relpath(\n",
    "                    os.path.join(root, file), f\"/kaggle/working/{models_dir}\"\n",
    "                ),\n",
    "            )\n",
    "\n",
    "    zipf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f30e3a11",
   "metadata": {
    "papermill": {
     "duration": 0.012391,
     "end_time": "2023-10-29T17:16:45.680337",
     "exception": false,
     "start_time": "2023-10-29T17:16:45.667946",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d660d0a7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-29T17:16:45.706622Z",
     "iopub.status.busy": "2023-10-29T17:16:45.706220Z",
     "iopub.status.idle": "2023-10-29T17:16:45.716730Z",
     "shell.execute_reply": "2023-10-29T17:16:45.715716Z"
    },
    "papermill": {
     "duration": 0.026356,
     "end_time": "2023-10-29T17:16:45.718974",
     "exception": false,
     "start_time": "2023-10-29T17:16:45.692618",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of file paths: ['/kaggle/input/model-1/LGBMR_2_20231029_163226.pkl', '/kaggle/input/model-1/LGBMR_1_20231029_162552.pkl', '/kaggle/input/model-1/LGBMR_0_20231029_155038.pkl', '/kaggle/input/model-1/LGBMR_1_20231029_161142.pkl', '/kaggle/input/model-1/LGBMR_0_20231029_154343.pkl', '/kaggle/input/model-1/LGBMR_1_20231029_160439.pkl', '/kaggle/input/model-1/LGBMR_2_20231029_165225.pkl', '/kaggle/input/model-1/LGBMR_0_20231029_155735.pkl', '/kaggle/input/model-1/LGBMR_0_20231029_153649.pkl', '/kaggle/input/model-1/LGBMR_2_20231029_163851.pkl', '/kaggle/input/model-1/LGBMR_2_20231029_164537.pkl', '/kaggle/input/model-1/LGBMR_1_20231029_161843.pkl']\n"
     ]
    }
   ],
   "source": [
    "model_paths = []\n",
    "models_dir_input = models_dir.replace(\"_\", \"-\")\n",
    "directory = f\"/kaggle/input/{models_dir_input}\"\n",
    "\n",
    "# Check if the directory exists\n",
    "if os.path.exists(directory):\n",
    "    # Traverse the directory and collect file paths\n",
    "    for filename in os.listdir(directory):\n",
    "        full_path = os.path.join(directory, filename)\n",
    "\n",
    "        # Check if the item is a file (and not a sub-directory)\n",
    "        if os.path.isfile(full_path):\n",
    "            model_paths.append(full_path)\n",
    "else:\n",
    "    print(f\"The directory {directory} does not exist.\")\n",
    "\n",
    "# Print or return the list of file paths\n",
    "print(\"List of file paths:\", model_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c04a0e",
   "metadata": {
    "papermill": {
     "duration": 0.01212,
     "end_time": "2023-10-29T17:16:45.743760",
     "exception": false,
     "start_time": "2023-10-29T17:16:45.731640",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8204c47f",
   "metadata": {
    "_cell_guid": "79aa495a-7580-497c-b913-8941cf7d0c61",
    "_uuid": "b3efe16f-ca61-4000-94d7-568364d0e11b",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2023-10-29T17:16:45.769869Z",
     "iopub.status.busy": "2023-10-29T17:16:45.769449Z",
     "iopub.status.idle": "2023-10-29T17:16:45.773391Z",
     "shell.execute_reply": "2023-10-29T17:16:45.772685Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.019319,
     "end_time": "2023-10-29T17:16:45.775373",
     "exception": false,
     "start_time": "2023-10-29T17:16:45.756054",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Assuming X_test for predict\n",
    "# ensemble_predictions = ensemble_predict(model_paths, df_test, mlflow_client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb50a54",
   "metadata": {
    "_cell_guid": "d4bfb979-2758-4fe9-8947-399b1c3a574c",
    "_uuid": "bef1a199-0988-4743-813e-99d4308fc9bb",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2023-10-29T17:16:45.802639Z",
     "iopub.status.busy": "2023-10-29T17:16:45.802215Z",
     "iopub.status.idle": "2023-10-29T17:16:45.827446Z",
     "shell.execute_reply": "2023-10-29T17:16:45.826566Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.04228,
     "end_time": "2023-10-29T17:16:45.830037",
     "exception": false,
     "start_time": "2023-10-29T17:16:45.787757",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import optiver2023\n",
    "\n",
    "env = optiver2023.make_env()\n",
    "iter_test = env.iter_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df03b72b",
   "metadata": {
    "papermill": {
     "duration": 0.016727,
     "end_time": "2023-10-29T17:16:45.862613",
     "exception": false,
     "start_time": "2023-10-29T17:16:45.845886",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e278f082",
   "metadata": {
    "_cell_guid": "d30f3862-dca2-4242-9db0-ab9750118622",
    "_uuid": "efd9073c-aaef-4135-bf7c-1e892e9aa831",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2023-10-29T17:16:45.896555Z",
     "iopub.status.busy": "2023-10-29T17:16:45.895646Z",
     "iopub.status.idle": "2023-10-29T17:18:31.343150Z",
     "shell.execute_reply": "2023-10-29T17:18:31.341949Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 105.467568,
     "end_time": "2023-10-29T17:18:31.345732",
     "exception": false,
     "start_time": "2023-10-29T17:16:45.878164",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This version of the API is not optimized and should not be used to estimate the runtime of your code on the hidden test set.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_20/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_20/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_20/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_20/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_20/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_20/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_20/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_20/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_20/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_20/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_20/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_20/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_20/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_20/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_20/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_20/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_20/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_20/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_20/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_20/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_20/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_20/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_20/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_20/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_20/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_20/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_20/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_20/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_20/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_20/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_20/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_20/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_20/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_20/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_20/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_20/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_20/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_20/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_20/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_20/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_20/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_20/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_20/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_20/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_20/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_20/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_20/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_20/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_20/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_20/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_20/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_20/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_20/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_20/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_20/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_20/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_20/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_20/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_20/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_20/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_20/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_20/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_20/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_20/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_20/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_20/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_20/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_20/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_20/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_20/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_20/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_20/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_20/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_20/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_20/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_20/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_20/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_20/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_20/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_20/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_20/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_20/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_20/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_20/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_20/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_20/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_20/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_20/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_20/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_20/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_20/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_20/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_20/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_20/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_20/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_20/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_20/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_20/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_20/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_20/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_20/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_20/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_20/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_20/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_20/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_20/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_20/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_20/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_20/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_20/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_20/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_20/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_20/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_20/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_20/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_20/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_20/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_20/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_20/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_20/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_20/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_20/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_20/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_20/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_20/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_20/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_20/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_20/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_20/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_20/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_20/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_20/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_20/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_20/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_20/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_20/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_20/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_20/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_20/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_20/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_20/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_20/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_20/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_20/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_20/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_20/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_20/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_20/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_20/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_20/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_20/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_20/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_20/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_20/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_20/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_20/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_20/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_20/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_20/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_20/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_20/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_20/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_20/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_20/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n",
      "/tmp/ipykernel_20/2963540819.py:108: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  group_by_market = df.groupby([\"date_id\", \"seconds_in_bucket\"]).agg(\n"
     ]
    }
   ],
   "source": [
    "counter = 0\n",
    "for test, revealed_targets, sample_prediction in iter_test:\n",
    "    # df_test_raw = pl.DataFrame(test)\n",
    "\n",
    "    feat = feat_engineering(test)\n",
    "\n",
    "    # feat = df_test.to_pandas()\n",
    "\n",
    "    list_cols_drop = [\"stock_id\", \"date_id\", \"row_id\"]\n",
    "    feat = feat.drop(list_cols_drop, axis=1)\n",
    "\n",
    "    sample_prediction[\"target\"] = ensemble_predict(model_paths, feat)\n",
    "    env.predict(sample_prediction)\n",
    "    counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "137c97f6",
   "metadata": {
    "_cell_guid": "1bca4965-2df1-4a88-bc93-1e6d8fea7ac3",
    "_uuid": "0a5200ae-92df-4b21-bf5c-6e255b90676f",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2023-10-29T17:18:31.393301Z",
     "iopub.status.busy": "2023-10-29T17:18:31.392938Z",
     "iopub.status.idle": "2023-10-29T17:18:31.398076Z",
     "shell.execute_reply": "2023-10-29T17:18:31.396952Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.031034,
     "end_time": "2023-10-29T17:18:31.400229",
     "exception": false,
     "start_time": "2023-10-29T17:18:31.369195",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if TRAIN:\n",
    "    list_cols_drop = [\"stock_id\", \"date_id\", \"row_id\"]\n",
    "    feat = feat.drop(list_cols_drop, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff22ec7e",
   "metadata": {
    "papermill": {
     "duration": 0.022343,
     "end_time": "2023-10-29T17:18:31.445104",
     "exception": false,
     "start_time": "2023-10-29T17:18:31.422761",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50cba56c",
   "metadata": {
    "papermill": {
     "duration": 0.02223,
     "end_time": "2023-10-29T17:18:31.490205",
     "exception": false,
     "start_time": "2023-10-29T17:18:31.467975",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac0aa3ca",
   "metadata": {
    "papermill": {
     "duration": 0.022172,
     "end_time": "2023-10-29T17:18:31.534807",
     "exception": false,
     "start_time": "2023-10-29T17:18:31.512635",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0afeb98c",
   "metadata": {
    "papermill": {
     "duration": 0.022005,
     "end_time": "2023-10-29T17:18:31.579172",
     "exception": false,
     "start_time": "2023-10-29T17:18:31.557167",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "381d3f6a",
   "metadata": {
    "_cell_guid": "18344629-3cbc-4244-ac34-c8af6b88b0d8",
    "_uuid": "667a4d83-d235-446e-a72c-cf02718a1a7b",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.022032,
     "end_time": "2023-10-29T17:18:31.623976",
     "exception": false,
     "start_time": "2023-10-29T17:18:31.601944",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf7460d",
   "metadata": {
    "papermill": {
     "duration": 0.022491,
     "end_time": "2023-10-29T17:18:31.668688",
     "exception": false,
     "start_time": "2023-10-29T17:18:31.646197",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf598930",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-29T17:18:31.715241Z",
     "iopub.status.busy": "2023-10-29T17:18:31.714510Z",
     "iopub.status.idle": "2023-10-29T17:18:31.718993Z",
     "shell.execute_reply": "2023-10-29T17:18:31.717934Z"
    },
    "papermill": {
     "duration": 0.030147,
     "end_time": "2023-10-29T17:18:31.721197",
     "exception": false,
     "start_time": "2023-10-29T17:18:31.691050",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# clean_directory_except_one('/kaggle/working/', 'submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b4a341",
   "metadata": {
    "_cell_guid": "3ef4ae54-1bb4-4cc3-a7da-a6691a272d30",
    "_uuid": "495a4e04-554d-4ef1-919d-62bc59f089c4",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.022039,
     "end_time": "2023-10-29T17:18:31.765571",
     "exception": false,
     "start_time": "2023-10-29T17:18:31.743532",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ae3ce4",
   "metadata": {
    "papermill": {
     "duration": 0.023232,
     "end_time": "2023-10-29T17:18:31.811015",
     "exception": false,
     "start_time": "2023-10-29T17:18:31.787783",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9621a6a3",
   "metadata": {
    "_cell_guid": "5498ae4b-62e8-4050-91e7-075fc549380f",
    "_uuid": "ce9653b2-7dff-4c5b-af77-5f0ef88b6e86",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.022222,
     "end_time": "2023-10-29T17:18:31.856624",
     "exception": false,
     "start_time": "2023-10-29T17:18:31.834402",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "230a1f26",
   "metadata": {
    "_cell_guid": "92a8ed18-3a63-464e-9d62-17cea3baed59",
    "_uuid": "0b017968-3b5a-4465-820e-e89659b10afd",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.022485,
     "end_time": "2023-10-29T17:18:31.901283",
     "exception": false,
     "start_time": "2023-10-29T17:18:31.878798",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7890cffb",
   "metadata": {
    "_cell_guid": "f9d19e8d-0074-4ae2-adfb-4c039b700215",
    "_uuid": "b6096d27-d452-4593-9832-d15bfbd83e1b",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.022606,
     "end_time": "2023-10-29T17:18:31.946090",
     "exception": false,
     "start_time": "2023-10-29T17:18:31.923484",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4772cb6b",
   "metadata": {
    "_cell_guid": "855f8db3-e344-4d4d-b577-36499775f352",
    "_uuid": "fde5edb9-d23d-41ac-92c6-593f299230c2",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.021902,
     "end_time": "2023-10-29T17:18:31.990239",
     "exception": false,
     "start_time": "2023-10-29T17:18:31.968337",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa9b0591",
   "metadata": {
    "_cell_guid": "82330d89-f35e-4cae-876c-de852438527d",
    "_uuid": "f90f329d-b882-464c-84fc-74a815cfe1cd",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.021851,
     "end_time": "2023-10-29T17:18:32.034578",
     "exception": false,
     "start_time": "2023-10-29T17:18:32.012727",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb88fda8",
   "metadata": {
    "_cell_guid": "ed96eca6-9765-4c90-a6bf-3071518ba2f0",
    "_uuid": "ca1d48b9-5537-491b-9de2-57360edd9844",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.022409,
     "end_time": "2023-10-29T17:18:32.079037",
     "exception": false,
     "start_time": "2023-10-29T17:18:32.056628",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e791f0a5",
   "metadata": {
    "_cell_guid": "06b24b0a-6515-48c6-b4a0-240865cfc1ea",
    "_uuid": "5e9b8536-f812-432a-ba05-c2f0b0f8994f",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.021691,
     "end_time": "2023-10-29T17:18:32.123180",
     "exception": false,
     "start_time": "2023-10-29T17:18:32.101489",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 112.139146,
   "end_time": "2023-10-29T17:18:32.866665",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-10-29T17:16:40.727519",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
